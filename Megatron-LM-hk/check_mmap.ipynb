{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check amber mmap token count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Zarr-based strategies will not be registered because of missing packages\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "\n",
    "import nltk\n",
    "import requests\n",
    "\n",
    "from megatron.core.datasets.indexed_dataset import MMapIndexedDataset\n",
    "from megatron.tokenizer.gpt2_tokenization import (\n",
    "    PRETRAINED_MERGES_ARCHIVE_MAP,\n",
    "    PRETRAINED_VOCAB_ARCHIVE_MAP,\n",
    ")\n",
    "from tools.merge_datasets import main as merge_main\n",
    "from tools.preprocess_data import Encoder\n",
    "from tools.preprocess_data import get_args as build_args\n",
    "from tools.preprocess_data import main as build_main\n",
    "\n",
    "__HUGGINGFACE_BERT_BASE_UNCASED_VOCAB = (\n",
    "    \"https://huggingface.co/bert-base-uncased/raw/main/vocab.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_path = \"full-amber-8node_0_token_ids_document\"\n",
    "dataset = MMapIndexedDataset(slice_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2050"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76783616"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def count_ids(dataset):\n",
    "    count = 0\n",
    "    for doc_ids in tqdm(dataset):\n",
    "        count += doc_ids.shape[0]\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 76783616/76783616 [02:22<00:00, 539157.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens:  157406412952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total_cnt = count_ids(dataset)\n",
    "print(\"Total number of tokens: \", total_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1259.251303616"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(157406412952/10**9)*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1256"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "157*8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check slimpajama mmap token count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "\n",
    "import nltk\n",
    "import requests\n",
    "\n",
    "from megatron.core.datasets.indexed_dataset import MMapIndexedDataset\n",
    "from megatron.tokenizer.gpt2_tokenization import (\n",
    "    PRETRAINED_MERGES_ARCHIVE_MAP,\n",
    "    PRETRAINED_VOCAB_ARCHIVE_MAP,\n",
    ")\n",
    "from tools.merge_datasets import main as merge_main\n",
    "from tools.preprocess_data import Encoder\n",
    "from tools.preprocess_data import get_args as build_args\n",
    "from tools.preprocess_data import main as build_main\n",
    "\n",
    "__HUGGINGFACE_BERT_BASE_UNCASED_VOCAB = (\n",
    "    \"https://huggingface.co/bert-base-uncased/raw/main/vocab.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_path = \"full-gpt2-8node_0_text_document\"\n",
    "dataset2 = MMapIndexedDataset(slice_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73799329"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73799329/73799329 [02:17<00:00, 535339.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens:  80730556394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total_cnt = count_ids(dataset2)\n",
    "print(\"Total number of tokens: \", total_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.730556394"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "80730556394/10**9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "645.844451152"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "80.730556394*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slimpajama .bin dtype= int32\n",
      "amber .bin dtype= uint16\n"
     ]
    }
   ],
   "source": [
    "print(\"slimpajama .bin dtype=\", dataset2[0].dtype)\n",
    "print(\"amber .bin dtype=\", dataset[0].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65535"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**16-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check icp mmap disable shuffle\n",
    "after disable shuffle, check if mmap data order is the same as jsonl data order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Zarr-based strategies will not be registered because of missing packages\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "\n",
    "import nltk\n",
    "import requests\n",
    "\n",
    "from megatron.core.datasets.indexed_dataset import MMapIndexedDataset\n",
    "from megatron.tokenizer.gpt2_tokenization import (\n",
    "    PRETRAINED_MERGES_ARCHIVE_MAP,\n",
    "    PRETRAINED_VOCAB_ARCHIVE_MAP,\n",
    ")\n",
    "from tools.merge_datasets import main as merge_main\n",
    "from tools.preprocess_data import Encoder\n",
    "from tools.preprocess_data import get_args as build_args\n",
    "from tools.preprocess_data import main as build_main\n",
    "from tools.preprocess_data import Encoder\n",
    "\n",
    "__HUGGINGFACE_BERT_BASE_UNCASED_VOCAB = (\n",
    "    \"https://huggingface.co/bert-base-uncased/raw/main/vocab.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_detokenizer():\n",
    "    extra_args = [\n",
    "            \"--tokenizer-model\",\n",
    "            \"/workspace/megatron/baichuan.tokenizer.model\",\n",
    "            \"--tokenizer-type\",\n",
    "            \"SentencePieceTokenizer\",\n",
    "            \"--append-eod\",\n",
    "            \"--workers\",\n",
    "            \"1\",\n",
    "            \"--log-interval\",\n",
    "            \"1\",\n",
    "        ]\n",
    "\n",
    "    sys.argv = [sys.argv[0], \"--input\", None, \"--output-prefix\", None,] + extra_args\n",
    "\n",
    "    encoder = Encoder(build_args())\n",
    "    encoder.initializer()\n",
    "    detok = encoder.tokenizer.detokenize\n",
    "    return detok\n",
    "\n",
    "detok = build_detokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# set random seed\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "def load_head(jsonl_path, n=10):\n",
    "    jsonls = []\n",
    "    with open(jsonl_path, \"r\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            _dict = json.loads(line)\n",
    "            _dict[\"jsonl_idx\"] = i\n",
    "            jsonls.append(_dict)\n",
    "            if len(jsonls) == n:\n",
    "                break\n",
    "    return jsonls\n",
    "\n",
    "def load_random_percent(jsonl_path, percent=1.4e-5, n=100):\n",
    "    jsonls = []\n",
    "    with open(jsonl_path, \"r\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if random.random() < percent:\n",
    "                _dict = json.loads(line)\n",
    "                _dict[\"jsonl_idx\"] = i\n",
    "                jsonls.append(_dict)\n",
    "                if len(jsonls) == n:\n",
    "                    break\n",
    "    return jsonls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install fast-edit-distance\n",
    "# !pip install editdistance\n",
    "from editdistance import eval as edit_distance\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def check_match(jsonls, dataset, detok, error_rate_threshold=3):\n",
    "    match_cnt = 0\n",
    "    not_match_samples = []\n",
    "    for _dict in tqdm(jsonls):\n",
    "        jsonl_text = _dict[\"text\"]\n",
    "        jsonl_idx = _dict[\"jsonl_idx\"]\n",
    "        dataset_text = detok(dataset[jsonl_idx].tolist())\n",
    "        dist = edit_distance(jsonl_text, dataset_text)\n",
    "        error_rate = (dist / len(jsonl_text)) * 100\n",
    "        if error_rate > error_rate_threshold:\n",
    "            print(f\"Not match! Error rate: {error_rate}% for jsonl_idx: {jsonl_idx}.\")\n",
    "            not_match_samples.append((_dict, error_rate))\n",
    "            print(f\"jsonl_text: \\n{jsonl_text}\\n\\n\\n\\ndataset_text: \\n{dataset_text}\")\n",
    "        else:\n",
    "            match_cnt += 1\n",
    "    print(f\"Matched {match_cnt} out of {len(jsonls)}\")\n",
    "    return match_cnt, not_match_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 91/100 [00:07<00:01,  8.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not match! Error rate: 3.2710280373831773% for jsonl_idx: 6539480.\n",
      "jsonl_text: \n",
      "Finished the 2013 Buller Gorge Recreational Walk in 03:21:17 in F3544 class!\n",
      "She came 37th in class and 264th overall!\n",
      "No photos of Shelley Dunnings in 2013 , we may still be uploading/processing, check back later.\n",
      "\n",
      "\n",
      "\n",
      "dataset_text: \n",
      "Finished the 2013 Buller Gorge Recreational Walk in 03:21:17 in F3544 class!\n",
      "She came 37th in class and 264th overall!\n",
      "No photos of Shelley Dunnings in 2013 , we may still be uploading/processing, check back later. <EOD> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:09<00:00, 10.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched 99 out of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# jsonl_path = \"/workspace/dataset/test/sample.json\"\n",
    "# jsonl_path = \"/workspace/rawdata/slimpajama/slimpajama/in_context_pretraining/sorting_output/chunk1_sorted.jsonl\"\n",
    "jsonl_path = \"/workspace/rawdata/slimpajama/slimpajama/in_context_pretraining/sorting_output/final_merged_sorted.jsonl\"\n",
    "\n",
    "# mmap_path = \"debug-sample-data-sequential_text_document\"\n",
    "# mmap_path = \"slimpajama-icp-chunk1_sorted-8node_text_document\"\n",
    "mmap_path = \"slimpajama-icp-final_merged_sorted-8node_text_document\"\n",
    "\n",
    "dataset = MMapIndexedDataset(mmap_path)\n",
    "\n",
    "# jsonls = load_head(jsonl_path, n=10)\n",
    "jsonls = load_random_percent(jsonl_path, percent=1.4e-5, n=100)\n",
    "match_cnt, not_match_samples = check_match(jsonls, dataset, detok, error_rate_threshold=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58996336, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset), len(jsonls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
