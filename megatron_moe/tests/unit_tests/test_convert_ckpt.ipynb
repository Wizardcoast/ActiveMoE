{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0874a887-0e55-4790-8a80-8374919fc8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "megatron_root: /cpfs/29ccba8f16c61395/data/user/liushan/projects/demo/Megatron-LM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Zarr-based strategies will not be registered because of missing packages\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pdb\n",
    "import torch\n",
    "import argparse\n",
    "\n",
    "this_filepath = os.path.abspath('')\n",
    "megatron_root = os.path.abspath(\n",
    "    this_filepath + \"../../..\")\n",
    "print(f\"megatron_root: {megatron_root}\")\n",
    "\n",
    "sys.path.insert(0, megatron_root)\n",
    "\n",
    "import copy\n",
    "import subprocess\n",
    "import tempfile\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoModel, AutoModelForCausalLM\n",
    "\n",
    "from megatron.arguments import parse_args\n",
    "from megatron.global_vars import set_args\n",
    "from tests.unit_tests.test_utilities import Utils\n",
    "from megatron.core.tensor_parallel import model_parallel_cuda_manual_seed\n",
    "from megatron.core.parallel_state import _set_global_memory_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4f985a4-31b5-46af-9b06-3d36bd3449e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cpfs/29ccba8f16c61395/data/user/liushan/projects/demo/Megatron-LM/tests/unit_tests\n",
      "config.json\t\t\t  modeling_llama.py\n",
      "configuration_llama.py\t\t  special_tokens_map.json\n",
      "model-00001-of-00003.safetensors  tokenizer.json\n",
      "model-00002-of-00003.safetensors  tokenizer.model\n",
      "model-00003-of-00003.safetensors  tokenizer_config.json\n",
      "model.safetensors.index.json\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "# Download the huggingface model from oss.\n",
    "!rclone copy --transfers 16 --checkers 16 oss://inf-alpha/home/megatron/tests/test_convert_ckpt/models/Llama-2-13b-hf ./models/Llama-2-13b-hf\n",
    "llama_root = \"./models/Llama-2-13b-hf\"\n",
    "!ls {llama_root}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4bf0b13-3d37-4d35-a93f-2390b62644cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cb5a40e16ef163cfb65face3dcbfea8fa624425a\n"
     ]
    }
   ],
   "source": [
    "# Show current git revision of Megatron-LM (expected: cb5a40e16ef163cfb65face3dcbfea8fa624425a)\n",
    "!git rev-parse HEAD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e2203c-2a50-4fd1-8975-d44a2f442e26",
   "metadata": {},
   "source": [
    "## Convert Llama-2-13b-hf to Megatron checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f57f9ec-b2ec-46cd-9f76-1d4ff740aa73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-14 08:55:40,316] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "# import unicorn\n",
    "sys.path.append(os.path.join(megatron_root, \"tools\", \"unicorn\"))\n",
    "import unicorn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1eaac82-9261-4e30-9a66-da5305a4a24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(megatron_path='/cpfs/29ccba8f16c61395/data/user/liushan/projects/demo/Megatron-LM', convert_checkpoint_from_megatron_to_transformers=False, load_path='./models/Llama-2-13b-hf', save_path='/cpfs/29ccba8f16c61395/data/user/liushan/projects/demo/Megatron-LM/tests/unit_tests/models/llama-megatron-t2p2', model_name='llama2-13b', template_name='llama', print_checkpoint_structure=True, tokenizer_name=None, max_shard_size='10GB', target_tensor_model_parallel_size=2, target_pipeline_model_parallel_size=2, target_data_parallel_size=1, target_params_dtype='fp16', make_vocab_size_divisible_by=128, extra_num_vocabs=0, use_distributed_optimizer=False, iteration=0)\n"
     ]
    }
   ],
   "source": [
    "def _parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser = unicorn.add_checkpointing_args(parser)\n",
    "    parser = unicorn.add_transformers_checkpoint_args(parser)\n",
    "    parser = unicorn.add_megatron_checkpoint_args(parser)\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "sys.argv = ['script.py',\n",
    "            '--megatron-path', megatron_root,\n",
    "            '--load-path', llama_root,\n",
    "            '--save-path', os.path.join(this_filepath, \"models\", \"llama-megatron-t2p2\"),\n",
    "            '--model-name', 'llama2-13b',\n",
    "            '--template-name', 'llama',\n",
    "            '--print-checkpoint-structure',\n",
    "            '--target_tensor_model_parallel_size', '2',\n",
    "            '--target_pipeline_model_parallel_size', '2',\n",
    "            '--target_params_dtype', 'fp16']\n",
    "\n",
    "args = _parse_args()\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "078dbcb6-8cef-426c-bb87-120cd35e56f7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Loading ./models/Llama-2-13b-hf/model-00001-of-00003.safetensors ...\n",
      "=> Loading ./models/Llama-2-13b-hf/model-00003-of-00003.safetensors ...\n",
      "=> Loading ./models/Llama-2-13b-hf/model-00002-of-00003.safetensors ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cp: not writing through dangling symlink '/cpfs/29ccba8f16c61395/data/user/liushan/projects/demo/Megatron-LM/tests/unit_tests/models/llama-megatron-t2p2/config.json'\n",
      "cp: not writing through dangling symlink '/cpfs/29ccba8f16c61395/data/user/liushan/projects/demo/Megatron-LM/tests/unit_tests/models/llama-megatron-t2p2/model.safetensors.index.json'\n",
      "cp: not writing through dangling symlink '/cpfs/29ccba8f16c61395/data/user/liushan/projects/demo/Megatron-LM/tests/unit_tests/models/llama-megatron-t2p2/special_tokens_map.json'\n",
      "cp: not writing through dangling symlink '/cpfs/29ccba8f16c61395/data/user/liushan/projects/demo/Megatron-LM/tests/unit_tests/models/llama-megatron-t2p2/tokenizer.json'\n",
      "cp: not writing through dangling symlink '/cpfs/29ccba8f16c61395/data/user/liushan/projects/demo/Megatron-LM/tests/unit_tests/models/llama-megatron-t2p2/tokenizer_config.json'\n",
      "cp: cannot stat './models/Llama-2-13b-hf/*.tiktoken': No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Converting ...\n",
      "=> converting embedding layer ...\n",
      "=> Converting transformer blocks ...\n",
      "Checkpoint structure of model state dict shard belonging to TP rank 0 and PP rank 0:\n",
      "# model                                           \n",
      "..# language_model                                \n",
      "....# embedding                                   \n",
      "......# word_embeddings                           \n",
      "........# weight                                   : torch.Size([16000, 5120])\n",
      "....# output_layer                                \n",
      "......# weight                                     : torch.Size([16000, 5120])\n",
      "....# encoder                                     \n",
      "......# layers.0.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.0.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.0.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.0.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.0.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.0.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.1.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.1.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.1.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.1.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.1.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.1.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.2.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.2.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.2.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.2.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.2.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.2.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.3.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.3.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.3.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.3.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.3.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.3.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.4.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.4.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.4.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.4.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.4.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.4.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.5.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.5.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.5.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.5.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.5.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.5.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.6.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.6.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.6.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.6.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.6.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.6.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.7.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.7.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.7.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.7.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.7.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.7.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.8.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.8.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.8.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.8.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.8.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.8.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.9.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.9.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.9.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.9.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.9.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.9.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.10.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.10.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.10.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.10.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.10.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.10.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.11.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.11.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.11.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.11.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.11.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.11.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.12.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.12.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.12.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.12.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.12.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.12.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.13.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.13.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.13.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.13.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.13.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.13.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.14.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.14.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.14.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.14.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.14.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.14.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.15.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.15.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.15.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.15.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.15.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.15.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.16.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.16.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.16.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.16.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.16.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.16.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.17.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.17.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.17.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.17.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.17.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.17.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.18.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.18.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.18.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.18.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.18.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.18.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.19.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.19.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.19.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.19.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.19.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.19.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "# checkpoint_version                               : 3.0\n",
      "# args                                             : namespace(orig_vocab_size=32000, hidden_size=5120, ffn_hidden_size=13824, num_layers=40, num_attention_heads=40, kv_channels=128, norm_epsilon=1e-05, init_method_std=0.02, seq_length=4096, untie_embeddings_and_output_weights=True, padded_vocab_size=32000, position_embedding_type='rope', normalization='RMSNorm', swiglu=True, max_position_embeddings=4096, add_bias_linear=False, tensor_model_parallel_size=2, pipeline_model_parallel_size=2, data_parallel_size=1, make_vocab_size_divisible_by=128, rank=0, tokenizer_type='NullTokenizer', float16=True, params_dtype=torch.float16)\n",
      "# iteration                                        : 0\n",
      "Checkpoint structure of model state dict shard belonging to TP rank 1 and PP rank 0:\n",
      "# model                                           \n",
      "..# language_model                                \n",
      "....# embedding                                   \n",
      "......# word_embeddings                           \n",
      "........# weight                                   : torch.Size([16000, 5120])\n",
      "....# output_layer                                \n",
      "......# weight                                     : torch.Size([16000, 5120])\n",
      "....# encoder                                     \n",
      "......# layers.0.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.0.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.0.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.0.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.0.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.0.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.1.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.1.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.1.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.1.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.1.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.1.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.2.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.2.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.2.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.2.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.2.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.2.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.3.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.3.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.3.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.3.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.3.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.3.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.4.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.4.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.4.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.4.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.4.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.4.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.5.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.5.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.5.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.5.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.5.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.5.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.6.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.6.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.6.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.6.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.6.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.6.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.7.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.7.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.7.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.7.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.7.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.7.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.8.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.8.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.8.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.8.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.8.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.8.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.9.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.9.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.9.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.9.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.9.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.9.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.10.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.10.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.10.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.10.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.10.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.10.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.11.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.11.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.11.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.11.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.11.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.11.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.12.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.12.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.12.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.12.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.12.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.12.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.13.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.13.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.13.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.13.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.13.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.13.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.14.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.14.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.14.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.14.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.14.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.14.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.15.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.15.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.15.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.15.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.15.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.15.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.16.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.16.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.16.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.16.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.16.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.16.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.17.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.17.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.17.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.17.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.17.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.17.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.18.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.18.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.18.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.18.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.18.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.18.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.19.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.19.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.19.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.19.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.19.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.19.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "# checkpoint_version                               : 3.0\n",
      "# args                                             : namespace(orig_vocab_size=32000, hidden_size=5120, ffn_hidden_size=13824, num_layers=40, num_attention_heads=40, kv_channels=128, norm_epsilon=1e-05, init_method_std=0.02, seq_length=4096, untie_embeddings_and_output_weights=True, padded_vocab_size=32000, position_embedding_type='rope', normalization='RMSNorm', swiglu=True, max_position_embeddings=4096, add_bias_linear=False, tensor_model_parallel_size=2, pipeline_model_parallel_size=2, data_parallel_size=1, make_vocab_size_divisible_by=128, rank=0, tokenizer_type='NullTokenizer', float16=True, params_dtype=torch.float16)\n",
      "# iteration                                        : 0\n",
      "Checkpoint structure of model state dict shard belonging to TP rank 0 and PP rank 1:\n",
      "# model                                           \n",
      "..# language_model                                \n",
      "....# encoder                                     \n",
      "......# layers.0.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.0.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.0.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.0.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.0.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.0.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.1.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.1.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.1.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.1.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.1.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.1.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.2.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.2.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.2.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.2.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.2.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.2.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.3.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.3.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.3.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.3.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.3.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.3.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.4.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.4.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.4.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.4.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.4.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.4.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.5.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.5.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.5.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.5.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.5.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.5.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.6.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.6.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.6.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.6.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.6.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.6.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.7.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.7.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.7.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.7.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.7.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.7.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.8.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.8.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.8.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.8.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.8.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.8.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.9.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.9.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.9.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.9.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.9.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.9.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.10.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.10.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.10.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.10.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.10.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.10.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.11.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.11.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.11.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.11.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.11.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.11.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.12.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.12.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.12.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.12.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.12.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.12.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.13.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.13.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.13.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.13.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.13.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.13.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.14.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.14.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.14.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.14.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.14.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.14.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.15.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.15.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.15.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.15.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.15.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.15.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.16.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.16.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.16.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.16.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.16.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.16.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.17.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.17.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.17.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.17.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.17.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.17.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.18.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.18.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.18.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.18.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.18.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.18.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.19.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.19.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.19.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.19.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.19.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.19.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# final_norm.weight                          : torch.Size([5120])\n",
      "....# output_layer                                \n",
      "......# weight                                     : torch.Size([16000, 5120])\n",
      "..# word_embeddings_for_head                      \n",
      "....# weight                                       : torch.Size([16000, 5120])\n",
      "# checkpoint_version                               : 3.0\n",
      "# args                                             : namespace(orig_vocab_size=32000, hidden_size=5120, ffn_hidden_size=13824, num_layers=40, num_attention_heads=40, kv_channels=128, norm_epsilon=1e-05, init_method_std=0.02, seq_length=4096, untie_embeddings_and_output_weights=True, padded_vocab_size=32000, position_embedding_type='rope', normalization='RMSNorm', swiglu=True, max_position_embeddings=4096, add_bias_linear=False, tensor_model_parallel_size=2, pipeline_model_parallel_size=2, data_parallel_size=1, make_vocab_size_divisible_by=128, rank=0, tokenizer_type='NullTokenizer', float16=True, params_dtype=torch.float16)\n",
      "# iteration                                        : 0\n",
      "Checkpoint structure of model state dict shard belonging to TP rank 1 and PP rank 1:\n",
      "# model                                           \n",
      "..# language_model                                \n",
      "....# encoder                                     \n",
      "......# layers.0.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.0.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.0.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.0.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.0.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.0.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.1.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.1.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.1.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.1.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.1.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.1.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.2.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.2.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.2.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.2.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.2.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.2.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.3.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.3.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.3.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.3.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.3.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.3.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.4.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.4.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.4.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.4.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.4.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.4.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.5.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.5.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.5.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.5.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.5.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.5.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.6.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.6.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.6.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.6.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.6.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.6.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.7.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.7.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.7.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.7.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.7.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.7.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.8.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.8.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.8.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.8.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.8.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.8.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.9.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.9.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.9.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.9.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.9.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.9.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.10.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.10.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.10.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.10.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.10.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.10.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.11.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.11.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.11.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.11.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.11.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.11.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.12.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.12.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.12.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.12.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.12.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.12.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.13.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.13.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.13.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.13.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.13.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.13.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.14.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.14.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.14.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.14.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.14.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.14.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.15.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.15.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.15.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.15.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.15.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.15.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.16.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.16.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.16.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.16.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.16.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.16.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.17.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.17.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.17.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.17.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.17.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.17.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.18.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.18.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.18.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.18.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.18.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.18.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.19.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.19.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.19.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.19.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.19.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.19.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# final_norm.weight                          : torch.Size([5120])\n",
      "....# output_layer                                \n",
      "......# weight                                     : torch.Size([16000, 5120])\n",
      "..# word_embeddings_for_head                      \n",
      "....# weight                                       : torch.Size([16000, 5120])\n",
      "# checkpoint_version                               : 3.0\n",
      "# args                                             : namespace(orig_vocab_size=32000, hidden_size=5120, ffn_hidden_size=13824, num_layers=40, num_attention_heads=40, kv_channels=128, norm_epsilon=1e-05, init_method_std=0.02, seq_length=4096, untie_embeddings_and_output_weights=True, padded_vocab_size=32000, position_embedding_type='rope', normalization='RMSNorm', swiglu=True, max_position_embeddings=4096, add_bias_linear=False, tensor_model_parallel_size=2, pipeline_model_parallel_size=2, data_parallel_size=1, make_vocab_size_divisible_by=128, rank=0, tokenizer_type='NullTokenizer', float16=True, params_dtype=torch.float16)\n",
      "# iteration                                        : 0\n"
     ]
    }
   ],
   "source": [
    "# make tp=2&pp=2 megatron checkpoint\n",
    "unicorn.convert_checkpoint_from_transformers_to_megatron(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f0726d3-92a1-42e1-addf-a34f3ef1e9f3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Loading ./models/Llama-2-13b-hf/model-00001-of-00003.safetensors ...\n",
      "=> Loading ./models/Llama-2-13b-hf/model-00003-of-00003.safetensors ...\n",
      "=> Loading ./models/Llama-2-13b-hf/model-00002-of-00003.safetensors ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cp: not writing through dangling symlink '/cpfs/29ccba8f16c61395/data/user/liushan/projects/demo/Megatron-LM/tests/unit_tests/models/llama-megatron-t1p1/config.json'\n",
      "cp: not writing through dangling symlink '/cpfs/29ccba8f16c61395/data/user/liushan/projects/demo/Megatron-LM/tests/unit_tests/models/llama-megatron-t1p1/model.safetensors.index.json'\n",
      "cp: not writing through dangling symlink '/cpfs/29ccba8f16c61395/data/user/liushan/projects/demo/Megatron-LM/tests/unit_tests/models/llama-megatron-t1p1/special_tokens_map.json'\n",
      "cp: not writing through dangling symlink '/cpfs/29ccba8f16c61395/data/user/liushan/projects/demo/Megatron-LM/tests/unit_tests/models/llama-megatron-t1p1/tokenizer.json'\n",
      "cp: not writing through dangling symlink '/cpfs/29ccba8f16c61395/data/user/liushan/projects/demo/Megatron-LM/tests/unit_tests/models/llama-megatron-t1p1/tokenizer_config.json'\n",
      "cp: cannot stat './models/Llama-2-13b-hf/*.tiktoken': No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Converting ...\n",
      "=> converting embedding layer ...\n",
      "=> Converting transformer blocks ...\n",
      "Checkpoint structure of model state dict shard belonging to TP rank 0 and PP rank 0:\n",
      "# model                                           \n",
      "..# language_model                                \n",
      "....# embedding                                   \n",
      "......# word_embeddings                           \n",
      "........# weight                                   : torch.Size([32000, 5120])\n",
      "....# output_layer                                \n",
      "......# weight                                     : torch.Size([32000, 5120])\n",
      "....# encoder                                     \n",
      "......# layers.0.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.0.self_attention.query_key_value.weight : torch.Size([15360, 5120])\n",
      "......# layers.0.self_attention.dense.weight       : torch.Size([5120, 5120])\n",
      "......# layers.0.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.0.mlp.dense_h_to_4h.weight          : torch.Size([27648, 5120])\n",
      "......# layers.0.mlp.dense_4h_to_h.weight          : torch.Size([5120, 13824])\n",
      "......# layers.1.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.1.self_attention.query_key_value.weight : torch.Size([15360, 5120])\n",
      "......# layers.1.self_attention.dense.weight       : torch.Size([5120, 5120])\n",
      "......# layers.1.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.1.mlp.dense_h_to_4h.weight          : torch.Size([27648, 5120])\n",
      "......# layers.1.mlp.dense_4h_to_h.weight          : torch.Size([5120, 13824])\n",
      "......# layers.2.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.2.self_attention.query_key_value.weight : torch.Size([15360, 5120])\n",
      "......# layers.2.self_attention.dense.weight       : torch.Size([5120, 5120])\n",
      "......# layers.2.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.2.mlp.dense_h_to_4h.weight          : torch.Size([27648, 5120])\n",
      "......# layers.2.mlp.dense_4h_to_h.weight          : torch.Size([5120, 13824])\n",
      "......# layers.3.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.3.self_attention.query_key_value.weight : torch.Size([15360, 5120])\n",
      "......# layers.3.self_attention.dense.weight       : torch.Size([5120, 5120])\n",
      "......# layers.3.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.3.mlp.dense_h_to_4h.weight          : torch.Size([27648, 5120])\n",
      "......# layers.3.mlp.dense_4h_to_h.weight          : torch.Size([5120, 13824])\n",
      "......# layers.4.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.4.self_attention.query_key_value.weight : torch.Size([15360, 5120])\n",
      "......# layers.4.self_attention.dense.weight       : torch.Size([5120, 5120])\n",
      "......# layers.4.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.4.mlp.dense_h_to_4h.weight          : torch.Size([27648, 5120])\n",
      "......# layers.4.mlp.dense_4h_to_h.weight          : torch.Size([5120, 13824])\n",
      "......# layers.5.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.5.self_attention.query_key_value.weight : torch.Size([15360, 5120])\n",
      "......# layers.5.self_attention.dense.weight       : torch.Size([5120, 5120])\n",
      "......# layers.5.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.5.mlp.dense_h_to_4h.weight          : torch.Size([27648, 5120])\n",
      "......# layers.5.mlp.dense_4h_to_h.weight          : torch.Size([5120, 13824])\n",
      "......# layers.6.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.6.self_attention.query_key_value.weight : torch.Size([15360, 5120])\n",
      "......# layers.6.self_attention.dense.weight       : torch.Size([5120, 5120])\n",
      "......# layers.6.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.6.mlp.dense_h_to_4h.weight          : torch.Size([27648, 5120])\n",
      "......# layers.6.mlp.dense_4h_to_h.weight          : torch.Size([5120, 13824])\n",
      "......# layers.7.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.7.self_attention.query_key_value.weight : torch.Size([15360, 5120])\n",
      "......# layers.7.self_attention.dense.weight       : torch.Size([5120, 5120])\n",
      "......# layers.7.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.7.mlp.dense_h_to_4h.weight          : torch.Size([27648, 5120])\n",
      "......# layers.7.mlp.dense_4h_to_h.weight          : torch.Size([5120, 13824])\n",
      "......# layers.8.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.8.self_attention.query_key_value.weight : torch.Size([15360, 5120])\n",
      "......# layers.8.self_attention.dense.weight       : torch.Size([5120, 5120])\n",
      "......# layers.8.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.8.mlp.dense_h_to_4h.weight          : torch.Size([27648, 5120])\n",
      "......# layers.8.mlp.dense_4h_to_h.weight          : torch.Size([5120, 13824])\n",
      "......# layers.9.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.9.self_attention.query_key_value.weight : torch.Size([15360, 5120])\n",
      "......# layers.9.self_attention.dense.weight       : torch.Size([5120, 5120])\n",
      "......# layers.9.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.9.mlp.dense_h_to_4h.weight          : torch.Size([27648, 5120])\n",
      "......# layers.9.mlp.dense_4h_to_h.weight          : torch.Size([5120, 13824])\n",
      "......# layers.10.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.10.self_attention.query_key_value.weight : torch.Size([15360, 5120])\n",
      "......# layers.10.self_attention.dense.weight      : torch.Size([5120, 5120])\n",
      "......# layers.10.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.10.mlp.dense_h_to_4h.weight         : torch.Size([27648, 5120])\n",
      "......# layers.10.mlp.dense_4h_to_h.weight         : torch.Size([5120, 13824])\n",
      "......# layers.11.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.11.self_attention.query_key_value.weight : torch.Size([15360, 5120])\n",
      "......# layers.11.self_attention.dense.weight      : torch.Size([5120, 5120])\n",
      "......# layers.11.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.11.mlp.dense_h_to_4h.weight         : torch.Size([27648, 5120])\n",
      "......# layers.11.mlp.dense_4h_to_h.weight         : torch.Size([5120, 13824])\n",
      "......# layers.12.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.12.self_attention.query_key_value.weight : torch.Size([15360, 5120])\n",
      "......# layers.12.self_attention.dense.weight      : torch.Size([5120, 5120])\n",
      "......# layers.12.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.12.mlp.dense_h_to_4h.weight         : torch.Size([27648, 5120])\n",
      "......# layers.12.mlp.dense_4h_to_h.weight         : torch.Size([5120, 13824])\n",
      "......# layers.13.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.13.self_attention.query_key_value.weight : torch.Size([15360, 5120])\n",
      "......# layers.13.self_attention.dense.weight      : torch.Size([5120, 5120])\n",
      "......# layers.13.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.13.mlp.dense_h_to_4h.weight         : torch.Size([27648, 5120])\n",
      "......# layers.13.mlp.dense_4h_to_h.weight         : torch.Size([5120, 13824])\n",
      "......# layers.14.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.14.self_attention.query_key_value.weight : torch.Size([15360, 5120])\n",
      "......# layers.14.self_attention.dense.weight      : torch.Size([5120, 5120])\n",
      "......# layers.14.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.14.mlp.dense_h_to_4h.weight         : torch.Size([27648, 5120])\n",
      "......# layers.14.mlp.dense_4h_to_h.weight         : torch.Size([5120, 13824])\n",
      "......# layers.15.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.15.self_attention.query_key_value.weight : torch.Size([15360, 5120])\n",
      "......# layers.15.self_attention.dense.weight      : torch.Size([5120, 5120])\n",
      "......# layers.15.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.15.mlp.dense_h_to_4h.weight         : torch.Size([27648, 5120])\n",
      "......# layers.15.mlp.dense_4h_to_h.weight         : torch.Size([5120, 13824])\n",
      "......# layers.16.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.16.self_attention.query_key_value.weight : torch.Size([15360, 5120])\n",
      "......# layers.16.self_attention.dense.weight      : torch.Size([5120, 5120])\n",
      "......# layers.16.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.16.mlp.dense_h_to_4h.weight         : torch.Size([27648, 5120])\n",
      "......# layers.16.mlp.dense_4h_to_h.weight         : torch.Size([5120, 13824])\n",
      "......# layers.17.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.17.self_attention.query_key_value.weight : torch.Size([15360, 5120])\n",
      "......# layers.17.self_attention.dense.weight      : torch.Size([5120, 5120])\n",
      "......# layers.17.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.17.mlp.dense_h_to_4h.weight         : torch.Size([27648, 5120])\n",
      "......# layers.17.mlp.dense_4h_to_h.weight         : torch.Size([5120, 13824])\n",
      "......# layers.18.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.18.self_attention.query_key_value.weight : torch.Size([15360, 5120])\n",
      "......# layers.18.self_attention.dense.weight      : torch.Size([5120, 5120])\n",
      "......# layers.18.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.18.mlp.dense_h_to_4h.weight         : torch.Size([27648, 5120])\n",
      "......# layers.18.mlp.dense_4h_to_h.weight         : torch.Size([5120, 13824])\n",
      "......# layers.19.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.19.self_attention.query_key_value.weight : torch.Size([15360, 5120])\n",
      "......# layers.19.self_attention.dense.weight      : torch.Size([5120, 5120])\n",
      "......# layers.19.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.19.mlp.dense_h_to_4h.weight         : torch.Size([27648, 5120])\n",
      "......# layers.19.mlp.dense_4h_to_h.weight         : torch.Size([5120, 13824])\n",
      "......# layers.20.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.20.self_attention.query_key_value.weight : torch.Size([15360, 5120])\n",
      "......# layers.20.self_attention.dense.weight      : torch.Size([5120, 5120])\n",
      "......# layers.20.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.20.mlp.dense_h_to_4h.weight         : torch.Size([27648, 5120])\n",
      "......# layers.20.mlp.dense_4h_to_h.weight         : torch.Size([5120, 13824])\n",
      "......# layers.21.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.21.self_attention.query_key_value.weight : torch.Size([15360, 5120])\n",
      "......# layers.21.self_attention.dense.weight      : torch.Size([5120, 5120])\n",
      "......# layers.21.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.21.mlp.dense_h_to_4h.weight         : torch.Size([27648, 5120])\n",
      "......# layers.21.mlp.dense_4h_to_h.weight         : torch.Size([5120, 13824])\n",
      "......# layers.22.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.22.self_attention.query_key_value.weight : torch.Size([15360, 5120])\n",
      "......# layers.22.self_attention.dense.weight      : torch.Size([5120, 5120])\n",
      "......# layers.22.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.22.mlp.dense_h_to_4h.weight         : torch.Size([27648, 5120])\n",
      "......# layers.22.mlp.dense_4h_to_h.weight         : torch.Size([5120, 13824])\n",
      "......# layers.23.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.23.self_attention.query_key_value.weight : torch.Size([15360, 5120])\n",
      "......# layers.23.self_attention.dense.weight      : torch.Size([5120, 5120])\n",
      "......# layers.23.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.23.mlp.dense_h_to_4h.weight         : torch.Size([27648, 5120])\n",
      "......# layers.23.mlp.dense_4h_to_h.weight         : torch.Size([5120, 13824])\n",
      "......# layers.24.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.24.self_attention.query_key_value.weight : torch.Size([15360, 5120])\n",
      "......# layers.24.self_attention.dense.weight      : torch.Size([5120, 5120])\n",
      "......# layers.24.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.24.mlp.dense_h_to_4h.weight         : torch.Size([27648, 5120])\n",
      "......# layers.24.mlp.dense_4h_to_h.weight         : torch.Size([5120, 13824])\n",
      "......# layers.25.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.25.self_attention.query_key_value.weight : torch.Size([15360, 5120])\n",
      "......# layers.25.self_attention.dense.weight      : torch.Size([5120, 5120])\n",
      "......# layers.25.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.25.mlp.dense_h_to_4h.weight         : torch.Size([27648, 5120])\n",
      "......# layers.25.mlp.dense_4h_to_h.weight         : torch.Size([5120, 13824])\n",
      "......# layers.26.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.26.self_attention.query_key_value.weight : torch.Size([15360, 5120])\n",
      "......# layers.26.self_attention.dense.weight      : torch.Size([5120, 5120])\n",
      "......# layers.26.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.26.mlp.dense_h_to_4h.weight         : torch.Size([27648, 5120])\n",
      "......# layers.26.mlp.dense_4h_to_h.weight         : torch.Size([5120, 13824])\n",
      "......# layers.27.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.27.self_attention.query_key_value.weight : torch.Size([15360, 5120])\n",
      "......# layers.27.self_attention.dense.weight      : torch.Size([5120, 5120])\n",
      "......# layers.27.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.27.mlp.dense_h_to_4h.weight         : torch.Size([27648, 5120])\n",
      "......# layers.27.mlp.dense_4h_to_h.weight         : torch.Size([5120, 13824])\n",
      "......# layers.28.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.28.self_attention.query_key_value.weight : torch.Size([15360, 5120])\n",
      "......# layers.28.self_attention.dense.weight      : torch.Size([5120, 5120])\n",
      "......# layers.28.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.28.mlp.dense_h_to_4h.weight         : torch.Size([27648, 5120])\n",
      "......# layers.28.mlp.dense_4h_to_h.weight         : torch.Size([5120, 13824])\n",
      "......# layers.29.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.29.self_attention.query_key_value.weight : torch.Size([15360, 5120])\n",
      "......# layers.29.self_attention.dense.weight      : torch.Size([5120, 5120])\n",
      "......# layers.29.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.29.mlp.dense_h_to_4h.weight         : torch.Size([27648, 5120])\n",
      "......# layers.29.mlp.dense_4h_to_h.weight         : torch.Size([5120, 13824])\n",
      "......# layers.30.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.30.self_attention.query_key_value.weight : torch.Size([15360, 5120])\n",
      "......# layers.30.self_attention.dense.weight      : torch.Size([5120, 5120])\n",
      "......# layers.30.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.30.mlp.dense_h_to_4h.weight         : torch.Size([27648, 5120])\n",
      "......# layers.30.mlp.dense_4h_to_h.weight         : torch.Size([5120, 13824])\n",
      "......# layers.31.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.31.self_attention.query_key_value.weight : torch.Size([15360, 5120])\n",
      "......# layers.31.self_attention.dense.weight      : torch.Size([5120, 5120])\n",
      "......# layers.31.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.31.mlp.dense_h_to_4h.weight         : torch.Size([27648, 5120])\n",
      "......# layers.31.mlp.dense_4h_to_h.weight         : torch.Size([5120, 13824])\n",
      "......# layers.32.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.32.self_attention.query_key_value.weight : torch.Size([15360, 5120])\n",
      "......# layers.32.self_attention.dense.weight      : torch.Size([5120, 5120])\n",
      "......# layers.32.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.32.mlp.dense_h_to_4h.weight         : torch.Size([27648, 5120])\n",
      "......# layers.32.mlp.dense_4h_to_h.weight         : torch.Size([5120, 13824])\n",
      "......# layers.33.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.33.self_attention.query_key_value.weight : torch.Size([15360, 5120])\n",
      "......# layers.33.self_attention.dense.weight      : torch.Size([5120, 5120])\n",
      "......# layers.33.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.33.mlp.dense_h_to_4h.weight         : torch.Size([27648, 5120])\n",
      "......# layers.33.mlp.dense_4h_to_h.weight         : torch.Size([5120, 13824])\n",
      "......# layers.34.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.34.self_attention.query_key_value.weight : torch.Size([15360, 5120])\n",
      "......# layers.34.self_attention.dense.weight      : torch.Size([5120, 5120])\n",
      "......# layers.34.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.34.mlp.dense_h_to_4h.weight         : torch.Size([27648, 5120])\n",
      "......# layers.34.mlp.dense_4h_to_h.weight         : torch.Size([5120, 13824])\n",
      "......# layers.35.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.35.self_attention.query_key_value.weight : torch.Size([15360, 5120])\n",
      "......# layers.35.self_attention.dense.weight      : torch.Size([5120, 5120])\n",
      "......# layers.35.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.35.mlp.dense_h_to_4h.weight         : torch.Size([27648, 5120])\n",
      "......# layers.35.mlp.dense_4h_to_h.weight         : torch.Size([5120, 13824])\n",
      "......# layers.36.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.36.self_attention.query_key_value.weight : torch.Size([15360, 5120])\n",
      "......# layers.36.self_attention.dense.weight      : torch.Size([5120, 5120])\n",
      "......# layers.36.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.36.mlp.dense_h_to_4h.weight         : torch.Size([27648, 5120])\n",
      "......# layers.36.mlp.dense_4h_to_h.weight         : torch.Size([5120, 13824])\n",
      "......# layers.37.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.37.self_attention.query_key_value.weight : torch.Size([15360, 5120])\n",
      "......# layers.37.self_attention.dense.weight      : torch.Size([5120, 5120])\n",
      "......# layers.37.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.37.mlp.dense_h_to_4h.weight         : torch.Size([27648, 5120])\n",
      "......# layers.37.mlp.dense_4h_to_h.weight         : torch.Size([5120, 13824])\n",
      "......# layers.38.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.38.self_attention.query_key_value.weight : torch.Size([15360, 5120])\n",
      "......# layers.38.self_attention.dense.weight      : torch.Size([5120, 5120])\n",
      "......# layers.38.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.38.mlp.dense_h_to_4h.weight         : torch.Size([27648, 5120])\n",
      "......# layers.38.mlp.dense_4h_to_h.weight         : torch.Size([5120, 13824])\n",
      "......# layers.39.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.39.self_attention.query_key_value.weight : torch.Size([15360, 5120])\n",
      "......# layers.39.self_attention.dense.weight      : torch.Size([5120, 5120])\n",
      "......# layers.39.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.39.mlp.dense_h_to_4h.weight         : torch.Size([27648, 5120])\n",
      "......# layers.39.mlp.dense_4h_to_h.weight         : torch.Size([5120, 13824])\n",
      "......# final_norm.weight                          : torch.Size([5120])\n",
      "..# word_embeddings_for_head                      \n",
      "....# weight                                       : torch.Size([32000, 5120])\n",
      "# checkpoint_version                               : 3.0\n",
      "# args                                             : namespace(orig_vocab_size=32000, hidden_size=5120, ffn_hidden_size=13824, num_layers=40, num_attention_heads=40, kv_channels=128, norm_epsilon=1e-05, init_method_std=0.02, seq_length=4096, untie_embeddings_and_output_weights=True, padded_vocab_size=32000, position_embedding_type='rope', normalization='RMSNorm', swiglu=True, max_position_embeddings=4096, add_bias_linear=False, tensor_model_parallel_size=1, pipeline_model_parallel_size=1, data_parallel_size=1, make_vocab_size_divisible_by=128, rank=0, tokenizer_type='NullTokenizer', float16=True, params_dtype=torch.float16)\n",
      "# iteration                                        : 0\n"
     ]
    }
   ],
   "source": [
    "# make tp=1&pp=1 megatron checkpoint\n",
    "args.save_path = os.path.join(this_filepath, \"models\", \"llama-megatron-t1p1\")\n",
    "args.target_tensor_model_parallel_size = 1\n",
    "args.target_pipeline_model_parallel_size = 1\n",
    "\n",
    "unicorn.convert_checkpoint_from_transformers_to_megatron(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c172e84-71e7-4675-9adf-dd6da5e78aed",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded loader_megatron as the loader.\n",
      "Loaded saver_megatron as the saver.\n",
      "Starting saver...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Zarr-based strategies will not be registered because of missing packages\n",
      "Zarr-based strategies will not be registered because of missing packages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting loader...\n",
      "Setting num_layers to 40 from checkpoint\n",
      "Setting hidden_size to 5120 from checkpoint\n",
      "Setting ffn_hidden_size to 13824 from checkpoint\n",
      "Setting seq_length to 4096 from checkpoint\n",
      "Setting num_attention_heads to 40 from checkpoint\n",
      "Checkpoint did not provide arguments num_query_groups\n",
      "Setting kv_channels to 128 from checkpoint\n",
      "Setting max_position_embeddings to 4096 from checkpoint\n",
      "Setting position_embedding_type to rope from checkpoint\n",
      "Checkpoint did not provide arguments rotary_percent\n",
      "Setting add_bias_linear to False from checkpoint\n",
      "Setting swiglu to True from checkpoint\n",
      "Setting untie_embeddings_and_output_weights to True from checkpoint\n",
      "Checkpoint did not provide arguments apply_layernorm_1p\n",
      "Setting normalization to RMSNorm from checkpoint\n",
      "Setting tokenizer_type to NullTokenizer from checkpoint\n",
      "Setting padded_vocab_size to 32000 from checkpoint\n",
      "Setting tensor_model_parallel_size to 2 from checkpoint\n",
      "Setting pipeline_model_parallel_size to 2 from checkpoint\n",
      "Checkpoint did not provide arguments virtual_pipeline_model_parallel_size\n",
      "Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage\n",
      "using world size: 4, data-parallel-size: 1, tensor-model-parallel size: 2, pipeline-model-parallel size: 2 \n",
      "setting global batch size to 1\n",
      "WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  add_bias_linear ................................. False\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_layernorm_1p .............................. False\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  async_tensor_model_parallel_allreduce ........... False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  barrier_with_L1_time ............................ True\n",
      "  bert_binary_head ................................ True\n",
      "  bert_embedder_type .............................. megatron\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_attn_linear ................................ False\n",
      "  bias_dropout_fusion ............................. False\n",
      "  bias_gelu_fusion ................................ False\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  bucket_size ..................................... 40000000\n",
      "  check_for_nan_in_loss_and_grad .................. True\n",
      "  classes_fraction ................................ 1.0\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  continue_on_missing_checkpoint .................. False\n",
      "  data_cache_path ................................. None\n",
      "  data_parallel_random_init ....................... False\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... None\n",
      "  data_per_class_fraction ......................... 1.0\n",
      "  data_sharding ................................... True\n",
      "  dataloader_type ................................. single\n",
      "  decoder_num_layers .............................. None\n",
      "  decoder_seq_length .............................. None\n",
      "  delay_grad_reduce ............................... True\n",
      "  dino_bottleneck_size ............................ 256\n",
      "  dino_freeze_last_layer .......................... 1\n",
      "  dino_head_hidden_size ........................... 2048\n",
      "  dino_local_crops_number ......................... 10\n",
      "  dino_local_img_size ............................. 96\n",
      "  dino_norm_last_layer ............................ False\n",
      "  dino_teacher_temp ............................... 0.07\n",
      "  dino_warmup_teacher_temp ........................ 0.04\n",
      "  dino_warmup_teacher_temp_epochs ................. 30\n",
      "  disable_scaled_init_method ...................... False\n",
      "  distribute_saved_activations .................... False\n",
      "  distributed_backend ............................. nccl\n",
      "  distributed_timeout_minutes ..................... 10\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  encoder_num_layers .............................. 40\n",
      "  encoder_seq_length .............................. 4096\n",
      "  end_weight_decay ................................ 0.01\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 100\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... None\n",
      "  exit_signal_handler ............................. False\n",
      "  expert_model_parallel_size ...................... 1\n",
      "  expert_parallel ................................. False\n",
      "  ffn_hidden_size ................................. 13824\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  fp8 ............................................. None\n",
      "  fp8_amax_compute_algo ........................... most_recent\n",
      "  fp8_amax_history_len ............................ 1\n",
      "  fp8_interval .................................... 1\n",
      "  fp8_margin ...................................... 0\n",
      "  fp8_wgrad ....................................... True\n",
      "  global_batch_size ............................... 1\n",
      "  gradient_accumulation_fusion .................... True\n",
      "  head_lr_mult .................................... 1.0\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 5120\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_h ........................................... 224\n",
      "  img_w ........................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  inference_batch_times_seqlen_threshold .......... 512\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  iter_per_epoch .................................. 1250\n",
      "  iteration ....................................... 0\n",
      "  kv_channels ..................................... 128\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ /cpfs/29ccba8f16c61395/data/user/liushan/projects/demo/Megatron-LM/tests/unit_tests/models/llama-megatron-t2p2\n",
      "  local_rank ...................................... None\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 100\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  log_world_size_to_tensorboard ................... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. None\n",
      "  lr_decay_iters .................................. None\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. linear\n",
      "  lr_warmup_fraction .............................. None\n",
      "  lr_warmup_init .................................. 0.0\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_factor ..................................... 1.0\n",
      "  mask_prob ....................................... 0.15\n",
      "  mask_type ....................................... random\n",
      "  masked_softmax_fusion ........................... False\n",
      "  max_position_embeddings ......................... 4096\n",
      "  max_tokens_to_oom ............................... 12000\n",
      "  mem_efficient_ln ................................ True\n",
      "  merge_file ...................................... None\n",
      "  micro_batch_size ................................ 1\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 0.0\n",
      "  mmap_warmup ..................................... False\n",
      "  model_spec ...................................... None\n",
      "  no_load_lr_scheduler ............................ None\n",
      "  no_load_optim ................................... True\n",
      "  no_load_rng ..................................... True\n",
      "  no_persist_layer_norm ........................... False\n",
      "  no_save_optim ................................... True\n",
      "  no_save_rng ..................................... True\n",
      "  no_shuffle_dataset .............................. None\n",
      "  norm_epsilon .................................... 1e-05\n",
      "  normalization ................................... RMSNorm\n",
      "  num_attention_heads ............................. 40\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_experts ..................................... None\n",
      "  num_layers ...................................... 40\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_query_groups ................................ None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  output_bert_embeddings .......................... False\n",
      "  overlap_grad_reduce ............................. False\n",
      "  overlap_p2p_comm ................................ False\n",
      "  override_opt_param_scheduler .................... False\n",
      "  padded_vocab_size ............................... 32000\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  perform_initialization .......................... False\n",
      "  pipeline_model_parallel_size .................... 2\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  position_embedding_type ......................... rope\n",
      "  profile ......................................... False\n",
      "  profile_ranks ................................... [0]\n",
      "  profile_step_end ................................ 12\n",
      "  profile_step_start .............................. 10\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  recompute_granularity ........................... None\n",
      "  recompute_method ................................ None\n",
      "  recompute_num_layers ............................ None\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  reset_sample_and_iteration_stat ................. None\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  retro_add_retriever ............................. False\n",
      "  retro_cyclic_train_iters ........................ None\n",
      "  retro_encoder_attention_dropout ................. 0.1\n",
      "  retro_encoder_hidden_dropout .................... 0.1\n",
      "  retro_encoder_layers ............................ 2\n",
      "  retro_num_neighbors ............................. 2\n",
      "  retro_num_retrieved_chunks ...................... 2\n",
      "  retro_return_doc_ids ............................ False\n",
      "  retro_workdir ................................... None\n",
      "  rotary_percent .................................. 1.0\n",
      "  rotary_seq_len_interpolation_factor ............. None\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ None\n",
      "  save_interval ................................... None\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 4096\n",
      "  sequence_parallel ............................... False\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  skip_train ...................................... False\n",
      "  split ........................................... 969, 30, 1\n",
      "  squared_relu .................................... False\n",
      "  standalone_embedding_stage ...................... False\n",
      "  start_weight_decay .............................. 0.01\n",
      "  swiglu .......................................... True\n",
      "  swiglu_make_ffn_hidden_size_divisible_by ........ 256\n",
      "  swin_backbone_type .............................. tiny\n",
      "  tensor_model_parallel_size ...................... 2\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  test_data_path .................................. None\n",
      "  timing_log_level ................................ 0\n",
      "  timing_log_option ............................... minmax\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_model ................................. None\n",
      "  tokenizer_name_or_path .......................... None\n",
      "  tokenizer_type .................................. NullTokenizer\n",
      "  tp_shared_dataloader ............................ True\n",
      "  train_data_path ................................. None\n",
      "  train_iters ..................................... None\n",
      "  train_samples ................................... None\n",
      "  transformer_impl ................................ local\n",
      "  transformer_pipeline_model_parallel_size ........ 2\n",
      "  untie_embeddings_and_output_weights ............. True\n",
      "  use_checkpoint_args ............................. False\n",
      "  use_checkpoint_opt_param_scheduler .............. False\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_distributed_optimizer ....................... False\n",
      "  use_flash_attn .................................. False\n",
      "  use_one_sent_docs ............................... False\n",
      "  use_ring_exchange_p2p ........................... False\n",
      "  valid_data_path ................................. None\n",
      "  variable_seq_lengths ............................ False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vision_backbone_type ............................ vit\n",
      "  vision_pretraining .............................. False\n",
      "  vision_pretraining_type ......................... classify\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... None\n",
      "  vocab_size ...................................... None\n",
      "  weight_decay .................................... 0.01\n",
      "  weight_decay_incr_style ......................... constant\n",
      "  world_size ...................................... 4\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "building GPT model ...\n",
      " loading release checkpoint from /cpfs/29ccba8f16c61395/data/user/liushan/projects/demo/Megatron-LM/tests/unit_tests/models/llama-megatron-t2p2\n",
      " checkpoint version 3.0\n",
      "  successfully loaded checkpoint from /cpfs/29ccba8f16c61395/data/user/liushan/projects/demo/Megatron-LM/tests/unit_tests/models/llama-megatron-t2p2 at iteration 0\n",
      "building GPT model ...\n",
      " loading release checkpoint from /cpfs/29ccba8f16c61395/data/user/liushan/projects/demo/Megatron-LM/tests/unit_tests/models/llama-megatron-t2p2\n",
      " checkpoint version 3.0\n",
      "  successfully loaded checkpoint from /cpfs/29ccba8f16c61395/data/user/liushan/projects/demo/Megatron-LM/tests/unit_tests/models/llama-megatron-t2p2 at iteration 0\n",
      "Checkpoint had argument orig_vocab_size but new arguments does not have this.\n",
      "Overwriting default ffn_hidden_size value None with value from checkpoint 13824.\n",
      "Overwriting default kv_channels value None with value from checkpoint 128.\n",
      "Checkpoint had argument padded_vocab_size but new arguments does not have this.\n",
      "Overwriting default normalization value LayerNorm with value from checkpoint RMSNorm.\n",
      "Overwriting default swiglu value False with value from checkpoint True.\n",
      "Checkpoint had argument data_parallel_size but new arguments does not have this.\n",
      "Checkpoint had argument float16 but new arguments does not have this.\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "setting global batch size to 1\n",
      "WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication\n",
      "using torch.float32 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  add_bias_linear ................................. False\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_layernorm_1p .............................. False\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  async_tensor_model_parallel_allreduce ........... False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  barrier_with_L1_time ............................ True\n",
      "  bert_binary_head ................................ True\n",
      "  bert_embedder_type .............................. megatron\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_attn_linear ................................ False\n",
      "  bias_dropout_fusion ............................. False\n",
      "  bias_gelu_fusion ................................ False\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  bucket_size ..................................... 40000000\n",
      "  check_for_nan_in_loss_and_grad .................. True\n",
      "  classes_fraction ................................ 1.0\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  continue_on_missing_checkpoint .................. False\n",
      "  data_cache_path ................................. None\n",
      "  data_parallel_random_init ....................... False\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... None\n",
      "  data_per_class_fraction ......................... 1.0\n",
      "  data_sharding ................................... True\n",
      "  dataloader_type ................................. single\n",
      "  decoder_num_layers .............................. None\n",
      "  decoder_seq_length .............................. None\n",
      "  delay_grad_reduce ............................... True\n",
      "  dino_bottleneck_size ............................ 256\n",
      "  dino_freeze_last_layer .......................... 1\n",
      "  dino_head_hidden_size ........................... 2048\n",
      "  dino_local_crops_number ......................... 10\n",
      "  dino_local_img_size ............................. 96\n",
      "  dino_norm_last_layer ............................ False\n",
      "  dino_teacher_temp ............................... 0.07\n",
      "  dino_warmup_teacher_temp ........................ 0.04\n",
      "  dino_warmup_teacher_temp_epochs ................. 30\n",
      "  disable_scaled_init_method ...................... False\n",
      "  distribute_saved_activations .................... False\n",
      "  distributed_backend ............................. nccl\n",
      "  distributed_timeout_minutes ..................... 10\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  encoder_num_layers .............................. 40\n",
      "  encoder_seq_length .............................. 4096\n",
      "  end_weight_decay ................................ 0.01\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 100\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... None\n",
      "  exit_signal_handler ............................. False\n",
      "  expert_model_parallel_size ...................... 1\n",
      "  expert_parallel ................................. False\n",
      "  ffn_hidden_size ................................. 13824\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ False\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  fp8 ............................................. None\n",
      "  fp8_amax_compute_algo ........................... most_recent\n",
      "  fp8_amax_history_len ............................ 1\n",
      "  fp8_interval .................................... 1\n",
      "  fp8_margin ...................................... 0\n",
      "  fp8_wgrad ....................................... True\n",
      "  global_batch_size ............................... 1\n",
      "  gradient_accumulation_fusion .................... True\n",
      "  head_lr_mult .................................... 1.0\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 5120\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_h ........................................... 224\n",
      "  img_w ........................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  inference_batch_times_seqlen_threshold .......... 512\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  iter_per_epoch .................................. 1250\n",
      "  kv_channels ..................................... 128\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ None\n",
      "  local_rank ...................................... None\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 100\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  log_world_size_to_tensorboard ................... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. None\n",
      "  lr_decay_iters .................................. None\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. linear\n",
      "  lr_warmup_fraction .............................. None\n",
      "  lr_warmup_init .................................. 0.0\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_factor ..................................... 1.0\n",
      "  mask_prob ....................................... 0.15\n",
      "  mask_type ....................................... random\n",
      "  masked_softmax_fusion ........................... False\n",
      "  max_position_embeddings ......................... 4096\n",
      "  max_tokens_to_oom ............................... 12000\n",
      "  mem_efficient_ln ................................ True\n",
      "  merge_file ...................................... None\n",
      "  micro_batch_size ................................ 1\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 0.0\n",
      "  mmap_warmup ..................................... False\n",
      "  model_spec ...................................... None\n",
      "  no_load_lr_scheduler ............................ None\n",
      "  no_load_optim ................................... True\n",
      "  no_load_rng ..................................... True\n",
      "  no_persist_layer_norm ........................... False\n",
      "  no_save_optim ................................... True\n",
      "  no_save_rng ..................................... True\n",
      "  no_shuffle_dataset .............................. None\n",
      "  norm_epsilon .................................... 1e-05\n",
      "  normalization ................................... RMSNorm\n",
      "  num_attention_heads ............................. 40\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_experts ..................................... None\n",
      "  num_layers ...................................... 40\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_query_groups ................................ None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  output_bert_embeddings .......................... False\n",
      "  overlap_grad_reduce ............................. False\n",
      "  overlap_p2p_comm ................................ False\n",
      "  override_opt_param_scheduler .................... False\n",
      "  params_dtype .................................... torch.float32\n",
      "  patch_dim ....................................... 16\n",
      "  perform_initialization .......................... False\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  position_embedding_type ......................... rope\n",
      "  profile ......................................... False\n",
      "  profile_ranks ................................... [0]\n",
      "  profile_step_end ................................ 12\n",
      "  profile_step_start .............................. 10\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  recompute_granularity ........................... None\n",
      "  recompute_method ................................ None\n",
      "  recompute_num_layers ............................ None\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  reset_sample_and_iteration_stat ................. None\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  retro_add_retriever ............................. False\n",
      "  retro_cyclic_train_iters ........................ None\n",
      "  retro_encoder_attention_dropout ................. 0.1\n",
      "  retro_encoder_hidden_dropout .................... 0.1\n",
      "  retro_encoder_layers ............................ 2\n",
      "  retro_num_neighbors ............................. 2\n",
      "  retro_num_retrieved_chunks ...................... 2\n",
      "  retro_return_doc_ids ............................ False\n",
      "  retro_workdir ................................... None\n",
      "  rotary_percent .................................. 1.0\n",
      "  rotary_seq_len_interpolation_factor ............. None\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ /cpfs/29ccba8f16c61395/data/user/liushan/projects/demo/Megatron-LM/tests/unit_tests/models/megatron-util-t1p1\n",
      "  save_interval ................................... 1\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 4096\n",
      "  sequence_parallel ............................... False\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  skip_train ...................................... False\n",
      "  split ........................................... 969, 30, 1\n",
      "  squared_relu .................................... False\n",
      "  standalone_embedding_stage ...................... False\n",
      "  start_weight_decay .............................. 0.01\n",
      "  swiglu .......................................... True\n",
      "  swiglu_make_ffn_hidden_size_divisible_by ........ 256\n",
      "  swin_backbone_type .............................. tiny\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  test_data_path .................................. None\n",
      "  timing_log_level ................................ 0\n",
      "  timing_log_option ............................... minmax\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_model ................................. None\n",
      "  tokenizer_name_or_path .......................... None\n",
      "  tokenizer_type .................................. NullTokenizer\n",
      "  tp_shared_dataloader ............................ True\n",
      "  train_data_path ................................. None\n",
      "  train_iters ..................................... None\n",
      "  train_samples ................................... None\n",
      "  transformer_impl ................................ local\n",
      "  transformer_pipeline_model_parallel_size ........ 1\n",
      "  untie_embeddings_and_output_weights ............. True\n",
      "  use_checkpoint_args ............................. False\n",
      "  use_checkpoint_opt_param_scheduler .............. False\n",
      "  use_cpu_initialization .......................... True\n",
      "  use_distributed_optimizer ....................... False\n",
      "  use_flash_attn .................................. False\n",
      "  use_one_sent_docs ............................... False\n",
      "  use_ring_exchange_p2p ........................... False\n",
      "  valid_data_path ................................. None\n",
      "  variable_seq_lengths ............................ False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vision_backbone_type ............................ vit\n",
      "  vision_pretraining .............................. False\n",
      "  vision_pretraining_type ......................... classify\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... None\n",
      "  vocab_size ...................................... None\n",
      "  weight_decay .................................... 0.01\n",
      "  weight_decay_incr_style ......................... constant\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "Setting consumed_train_samples to 0 and consumed_valid_samples to 0\n",
      "received embeddings\n",
      "Original vocab size not specified, leaving embedding table as-is. If you've changed the tensor parallel size this could cause problems.\n",
      "building GPT model ...\n",
      "sending embeddings\n",
      "sending transformer layer 0\n",
      "sending transformer layer 1\n",
      "sending transformer layer 2\n",
      "sending transformer layer 3\n",
      "sending transformer layer 4\n",
      "sending transformer layer 5\n",
      "sending transformer layer 6\n",
      "sending transformer layer 7\n",
      "sending transformer layer 8\n",
      "sending transformer layer 9\n",
      "sending transformer layer 10\n",
      "sending transformer layer 11\n",
      "sending transformer layer 12\n",
      "sending transformer layer 13\n",
      "sending transformer layer 14\n",
      "sending transformer layer 15\n",
      "sending transformer layer 16\n",
      "sending transformer layer 17\n",
      "sending transformer layer 18\n",
      "sending transformer layer 19\n",
      "building GPT model ...\n",
      " loading release checkpoint from /cpfs/29ccba8f16c61395/data/user/liushan/projects/demo/Megatron-LM/tests/unit_tests/models/llama-megatron-t2p2\n",
      " checkpoint version 3.0\n",
      "  successfully loaded checkpoint from /cpfs/29ccba8f16c61395/data/user/liushan/projects/demo/Megatron-LM/tests/unit_tests/models/llama-megatron-t2p2 at iteration 0\n",
      "building GPT model ...\n",
      " loading release checkpoint from /cpfs/29ccba8f16c61395/data/user/liushan/projects/demo/Megatron-LM/tests/unit_tests/models/llama-megatron-t2p2\n",
      " checkpoint version 3.0\n",
      "  successfully loaded checkpoint from /cpfs/29ccba8f16c61395/data/user/liushan/projects/demo/Megatron-LM/tests/unit_tests/models/llama-megatron-t2p2 at iteration 0\n",
      "received transformer layer 0\n",
      "received transformer layer 1\n",
      "received transformer layer 2\n",
      "received transformer layer 3\n",
      "received transformer layer 4\n",
      "received transformer layer 5\n",
      "received transformer layer 6\n",
      "received transformer layer 7\n",
      "received transformer layer 8\n",
      "received transformer layer 9\n",
      "received transformer layer 10\n",
      "received transformer layer 11\n",
      "received transformer layer 12\n",
      "received transformer layer 13\n",
      "received transformer layer 14\n",
      "received transformer layer 15\n",
      "received transformer layer 16\n",
      "received transformer layer 17\n",
      "received transformer layer 18\n",
      "received transformer layer 19\n",
      "received transformer layer 20\n",
      "received transformer layer 21\n",
      "received transformer layer 22\n",
      "received transformer layer 23\n",
      "received transformer layer 24\n",
      "received transformer layer 25\n",
      "received transformer layer 26\n",
      "received transformer layer 27\n",
      "received transformer layer 28\n",
      "received transformer layer 29\n",
      "received transformer layer 30\n",
      "received transformer layer 31\n",
      "received transformer layer 32\n",
      "received transformer layer 33\n",
      "received transformer layer 34\n",
      "received transformer layer 35\n",
      "received transformer layer 36\n",
      "received transformer layer 37\n",
      "received transformer layer 38\n",
      "received transformer layer 39\n",
      "received final norm\n",
      "received output layer\n",
      "saving checkpoint at iteration       0 to /cpfs/29ccba8f16c61395/data/user/liushan/projects/demo/Megatron-LM/tests/unit_tests/models/megatron-util-t1p1\n",
      "  successfully saved checkpoint at iteration       0 to /cpfs/29ccba8f16c61395/data/user/liushan/projects/demo/Megatron-LM/tests/unit_tests/models/megatron-util-t1p1\n",
      "Done!\n",
      "sending transformer layer 20\n",
      "sending transformer layer 21\n",
      "sending transformer layer 22\n",
      "sending transformer layer 23\n",
      "sending transformer layer 24\n",
      "sending transformer layer 25\n",
      "sending transformer layer 26\n",
      "sending transformer layer 27\n",
      "sending transformer layer 28\n",
      "sending transformer layer 29\n",
      "sending transformer layer 30\n",
      "sending transformer layer 31\n",
      "sending transformer layer 32\n",
      "sending transformer layer 33\n",
      "sending transformer layer 34\n",
      "sending transformer layer 35\n",
      "sending transformer layer 36\n",
      "sending transformer layer 37\n",
      "sending transformer layer 38\n",
      "sending transformer layer 39\n",
      "sending final norm\n",
      "sending output layer\n",
      "Waiting for saver to complete...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['python', 'tools/checkpoint/util.py', '--model-type', 'GPT', '--load-dir', '/cpfs/29ccba8f16c61395/data/user/liushan/projects/demo/Megatron-LM/tests/unit_tests/models/llama-megatron-t2p2', '--save-dir', '/cpfs/29ccba8f16c61395/data/user/liushan/projects/demo/Megatron-LM/tests/unit_tests/models/megatron-util-t1p1', '--target-tensor-parallel-size', '1', '--target-pipeline-parallel-size', '1'], returncode=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use tools/checkpoint in Megatron-LM to convert llama-megatron-t2p2 to megatron-util-t1p1\n",
    "cmds = [\n",
    "    \"python\",\n",
    "    \"tools/checkpoint/util.py\",\n",
    "    \"--model-type\", \"GPT\",\n",
    "    \"--load-dir\", os.path.join(this_filepath, \"models\", \"llama-megatron-t2p2\"),\n",
    "    \"--save-dir\", os.path.join(this_filepath, \"models\", \"megatron-util-t1p1\"),\n",
    "    \"--target-tensor-parallel-size\", \"1\",\n",
    "    \"--target-pipeline-parallel-size\", \"1\"\n",
    "]\n",
    "\n",
    "subprocess.run(cmds, cwd=megatron_root, env={\"PYTHONPATH\":megatron_root})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16812807-1180-4201-9bab-a8cdfe6674ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the converted checkpoints\n",
    "# src: converted by unicorn\n",
    "# dst: converted by Megatron tools/checkpoint/util\n",
    "src_ckpt = os.path.join(this_filepath, \"models\", \"llama-megatron-t1p1\", \"release\", \"mp_rank_00\", \"model_optim_rng.pt\")\n",
    "dst_ckpt = os.path.join(this_filepath, \"models\", \"megatron-util-t1p1\", \"iter_0000000\", \"mp_rank_00\", \"model_optim_rng.pt\")\n",
    "\n",
    "unicorn_state_dict = torch.load(src_ckpt, map_location=\"cpu\")\n",
    "util_state_dict = torch.load(dst_ckpt, map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8de92946-9213-4a9c-b28c-9f6a0ad2968c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wte diff: 0\n",
      "lm_head diff: 0\n",
      "layers.0.input_norm.weight diff: 0\n",
      "layers.0.self_attention.query_key_value.weight diff: 0\n",
      "layers.0.self_attention.dense.weight diff: 0\n",
      "layers.0.post_attention_norm.weight diff: 0\n",
      "layers.0.mlp.dense_h_to_4h.weight diff: 0\n",
      "layers.0.mlp.dense_4h_to_h.weight diff: 0\n",
      "layers.1.input_norm.weight diff: 0\n",
      "layers.1.self_attention.query_key_value.weight diff: 0\n",
      "layers.1.self_attention.dense.weight diff: 0\n",
      "layers.1.post_attention_norm.weight diff: 0\n",
      "layers.1.mlp.dense_h_to_4h.weight diff: 0\n",
      "layers.1.mlp.dense_4h_to_h.weight diff: 0\n",
      "layers.2.input_norm.weight diff: 0\n",
      "layers.2.self_attention.query_key_value.weight diff: 0\n",
      "layers.2.self_attention.dense.weight diff: 0\n",
      "layers.2.post_attention_norm.weight diff: 0\n",
      "layers.2.mlp.dense_h_to_4h.weight diff: 0\n",
      "layers.2.mlp.dense_4h_to_h.weight diff: 0\n",
      "layers.3.input_norm.weight diff: 0\n",
      "layers.3.self_attention.query_key_value.weight diff: 0\n",
      "layers.3.self_attention.dense.weight diff: 0\n",
      "layers.3.post_attention_norm.weight diff: 0\n",
      "layers.3.mlp.dense_h_to_4h.weight diff: 0\n",
      "layers.3.mlp.dense_4h_to_h.weight diff: 0\n",
      "layers.4.input_norm.weight diff: 0\n",
      "layers.4.self_attention.query_key_value.weight diff: 0\n",
      "layers.4.self_attention.dense.weight diff: 0\n",
      "layers.4.post_attention_norm.weight diff: 0\n",
      "layers.4.mlp.dense_h_to_4h.weight diff: 0\n",
      "layers.4.mlp.dense_4h_to_h.weight diff: 0\n",
      "layers.5.input_norm.weight diff: 0\n",
      "layers.5.self_attention.query_key_value.weight diff: 0\n",
      "layers.5.self_attention.dense.weight diff: 0\n",
      "layers.5.post_attention_norm.weight diff: 0\n",
      "layers.5.mlp.dense_h_to_4h.weight diff: 0\n",
      "layers.5.mlp.dense_4h_to_h.weight diff: 0\n",
      "layers.6.input_norm.weight diff: 0\n",
      "layers.6.self_attention.query_key_value.weight diff: 0\n",
      "layers.6.self_attention.dense.weight diff: 0\n",
      "layers.6.post_attention_norm.weight diff: 0\n",
      "layers.6.mlp.dense_h_to_4h.weight diff: 0\n",
      "layers.6.mlp.dense_4h_to_h.weight diff: 0\n",
      "layers.7.input_norm.weight diff: 0\n",
      "layers.7.self_attention.query_key_value.weight diff: 0\n",
      "layers.7.self_attention.dense.weight diff: 0\n",
      "layers.7.post_attention_norm.weight diff: 0\n",
      "layers.7.mlp.dense_h_to_4h.weight diff: 0\n",
      "layers.7.mlp.dense_4h_to_h.weight diff: 0\n",
      "layers.8.input_norm.weight diff: 0\n",
      "layers.8.self_attention.query_key_value.weight diff: 0\n",
      "layers.8.self_attention.dense.weight diff: 0\n",
      "layers.8.post_attention_norm.weight diff: 0\n",
      "layers.8.mlp.dense_h_to_4h.weight diff: 0\n",
      "layers.8.mlp.dense_4h_to_h.weight diff: 0\n",
      "layers.9.input_norm.weight diff: 0\n",
      "layers.9.self_attention.query_key_value.weight diff: 0\n",
      "layers.9.self_attention.dense.weight diff: 0\n",
      "layers.9.post_attention_norm.weight diff: 0\n",
      "layers.9.mlp.dense_h_to_4h.weight diff: 0\n",
      "layers.9.mlp.dense_4h_to_h.weight diff: 0\n",
      "layers.10.input_norm.weight diff: 0\n",
      "layers.10.self_attention.query_key_value.weight diff: 0\n",
      "layers.10.self_attention.dense.weight diff: 0\n",
      "layers.10.post_attention_norm.weight diff: 0\n",
      "layers.10.mlp.dense_h_to_4h.weight diff: 0\n",
      "layers.10.mlp.dense_4h_to_h.weight diff: 0\n",
      "layers.11.input_norm.weight diff: 0\n",
      "layers.11.self_attention.query_key_value.weight diff: 0\n",
      "layers.11.self_attention.dense.weight diff: 0\n",
      "layers.11.post_attention_norm.weight diff: 0\n",
      "layers.11.mlp.dense_h_to_4h.weight diff: 0\n",
      "layers.11.mlp.dense_4h_to_h.weight diff: 0\n",
      "layers.12.input_norm.weight diff: 0\n",
      "layers.12.self_attention.query_key_value.weight diff: 0\n",
      "layers.12.self_attention.dense.weight diff: 0\n",
      "layers.12.post_attention_norm.weight diff: 0\n",
      "layers.12.mlp.dense_h_to_4h.weight diff: 0\n",
      "layers.12.mlp.dense_4h_to_h.weight diff: 0\n",
      "layers.13.input_norm.weight diff: 0\n",
      "layers.13.self_attention.query_key_value.weight diff: 0\n",
      "layers.13.self_attention.dense.weight diff: 0\n",
      "layers.13.post_attention_norm.weight diff: 0\n",
      "layers.13.mlp.dense_h_to_4h.weight diff: 0\n",
      "layers.13.mlp.dense_4h_to_h.weight diff: 0\n",
      "layers.14.input_norm.weight diff: 0\n",
      "layers.14.self_attention.query_key_value.weight diff: 0\n",
      "layers.14.self_attention.dense.weight diff: 0\n",
      "layers.14.post_attention_norm.weight diff: 0\n",
      "layers.14.mlp.dense_h_to_4h.weight diff: 0\n",
      "layers.14.mlp.dense_4h_to_h.weight diff: 0\n",
      "layers.15.input_norm.weight diff: 0\n",
      "layers.15.self_attention.query_key_value.weight diff: 0\n",
      "layers.15.self_attention.dense.weight diff: 0\n",
      "layers.15.post_attention_norm.weight diff: 0\n",
      "layers.15.mlp.dense_h_to_4h.weight diff: 0\n",
      "layers.15.mlp.dense_4h_to_h.weight diff: 0\n",
      "layers.16.input_norm.weight diff: 0\n",
      "layers.16.self_attention.query_key_value.weight diff: 0\n",
      "layers.16.self_attention.dense.weight diff: 0\n",
      "layers.16.post_attention_norm.weight diff: 0\n",
      "layers.16.mlp.dense_h_to_4h.weight diff: 0\n",
      "layers.16.mlp.dense_4h_to_h.weight diff: 0\n",
      "layers.17.input_norm.weight diff: 0\n",
      "layers.17.self_attention.query_key_value.weight diff: 0\n",
      "layers.17.self_attention.dense.weight diff: 0\n",
      "layers.17.post_attention_norm.weight diff: 0\n",
      "layers.17.mlp.dense_h_to_4h.weight diff: 0\n",
      "layers.17.mlp.dense_4h_to_h.weight diff: 0\n",
      "layers.18.input_norm.weight diff: 0\n",
      "layers.18.self_attention.query_key_value.weight diff: 0\n",
      "layers.18.self_attention.dense.weight diff: 0\n",
      "layers.18.post_attention_norm.weight diff: 0\n",
      "layers.18.mlp.dense_h_to_4h.weight diff: 0\n",
      "layers.18.mlp.dense_4h_to_h.weight diff: 0\n",
      "layers.19.input_norm.weight diff: 0\n",
      "layers.19.self_attention.query_key_value.weight diff: 0\n",
      "layers.19.self_attention.dense.weight diff: 0\n",
      "layers.19.post_attention_norm.weight diff: 0\n",
      "layers.19.mlp.dense_h_to_4h.weight diff: 0\n",
      "layers.19.mlp.dense_4h_to_h.weight diff: 0\n",
      "layers.20.input_norm.weight diff: 0\n",
      "layers.20.self_attention.query_key_value.weight diff: 0\n",
      "layers.20.self_attention.dense.weight diff: 0\n",
      "layers.20.post_attention_norm.weight diff: 0\n",
      "layers.20.mlp.dense_h_to_4h.weight diff: 0\n",
      "layers.20.mlp.dense_4h_to_h.weight diff: 0\n",
      "layers.21.input_norm.weight diff: 0\n",
      "layers.21.self_attention.query_key_value.weight diff: 0\n",
      "layers.21.self_attention.dense.weight diff: 0\n",
      "layers.21.post_attention_norm.weight diff: 0\n",
      "layers.21.mlp.dense_h_to_4h.weight diff: 0\n",
      "layers.21.mlp.dense_4h_to_h.weight diff: 0\n",
      "layers.22.input_norm.weight diff: 0\n",
      "layers.22.self_attention.query_key_value.weight diff: 0\n",
      "layers.22.self_attention.dense.weight diff: 0\n",
      "layers.22.post_attention_norm.weight diff: 0\n",
      "layers.22.mlp.dense_h_to_4h.weight diff: 0\n",
      "layers.22.mlp.dense_4h_to_h.weight diff: 0\n",
      "layers.23.input_norm.weight diff: 0\n",
      "layers.23.self_attention.query_key_value.weight diff: 0\n",
      "layers.23.self_attention.dense.weight diff: 0\n",
      "layers.23.post_attention_norm.weight diff: 0\n",
      "layers.23.mlp.dense_h_to_4h.weight diff: 0\n",
      "layers.23.mlp.dense_4h_to_h.weight diff: 0\n",
      "layers.24.input_norm.weight diff: 0\n",
      "layers.24.self_attention.query_key_value.weight diff: 0\n",
      "layers.24.self_attention.dense.weight diff: 0\n",
      "layers.24.post_attention_norm.weight diff: 0\n",
      "layers.24.mlp.dense_h_to_4h.weight diff: 0\n",
      "layers.24.mlp.dense_4h_to_h.weight diff: 0\n",
      "layers.25.input_norm.weight diff: 0\n",
      "layers.25.self_attention.query_key_value.weight diff: 0\n",
      "layers.25.self_attention.dense.weight diff: 0\n",
      "layers.25.post_attention_norm.weight diff: 0\n",
      "layers.25.mlp.dense_h_to_4h.weight diff: 0\n",
      "layers.25.mlp.dense_4h_to_h.weight diff: 0\n",
      "layers.26.input_norm.weight diff: 0\n",
      "layers.26.self_attention.query_key_value.weight diff: 0\n",
      "layers.26.self_attention.dense.weight diff: 0\n",
      "layers.26.post_attention_norm.weight diff: 0\n",
      "layers.26.mlp.dense_h_to_4h.weight diff: 0\n",
      "layers.26.mlp.dense_4h_to_h.weight diff: 0\n",
      "layers.27.input_norm.weight diff: 0\n",
      "layers.27.self_attention.query_key_value.weight diff: 0\n",
      "layers.27.self_attention.dense.weight diff: 0\n",
      "layers.27.post_attention_norm.weight diff: 0\n",
      "layers.27.mlp.dense_h_to_4h.weight diff: 0\n",
      "layers.27.mlp.dense_4h_to_h.weight diff: 0\n",
      "layers.28.input_norm.weight diff: 0\n",
      "layers.28.self_attention.query_key_value.weight diff: 0\n",
      "layers.28.self_attention.dense.weight diff: 0\n",
      "layers.28.post_attention_norm.weight diff: 0\n",
      "layers.28.mlp.dense_h_to_4h.weight diff: 0\n",
      "layers.28.mlp.dense_4h_to_h.weight diff: 0\n",
      "layers.29.input_norm.weight diff: 0\n",
      "layers.29.self_attention.query_key_value.weight diff: 0\n",
      "layers.29.self_attention.dense.weight diff: 0\n",
      "layers.29.post_attention_norm.weight diff: 0\n",
      "layers.29.mlp.dense_h_to_4h.weight diff: 0\n",
      "layers.29.mlp.dense_4h_to_h.weight diff: 0\n",
      "layers.30.input_norm.weight diff: 0\n",
      "layers.30.self_attention.query_key_value.weight diff: 0\n",
      "layers.30.self_attention.dense.weight diff: 0\n",
      "layers.30.post_attention_norm.weight diff: 0\n",
      "layers.30.mlp.dense_h_to_4h.weight diff: 0\n",
      "layers.30.mlp.dense_4h_to_h.weight diff: 0\n",
      "layers.31.input_norm.weight diff: 0\n",
      "layers.31.self_attention.query_key_value.weight diff: 0\n",
      "layers.31.self_attention.dense.weight diff: 0\n",
      "layers.31.post_attention_norm.weight diff: 0\n",
      "layers.31.mlp.dense_h_to_4h.weight diff: 0\n",
      "layers.31.mlp.dense_4h_to_h.weight diff: 0\n",
      "layers.32.input_norm.weight diff: 0\n",
      "layers.32.self_attention.query_key_value.weight diff: 0\n",
      "layers.32.self_attention.dense.weight diff: 0\n",
      "layers.32.post_attention_norm.weight diff: 0\n",
      "layers.32.mlp.dense_h_to_4h.weight diff: 0\n",
      "layers.32.mlp.dense_4h_to_h.weight diff: 0\n",
      "layers.33.input_norm.weight diff: 0\n",
      "layers.33.self_attention.query_key_value.weight diff: 0\n",
      "layers.33.self_attention.dense.weight diff: 0\n",
      "layers.33.post_attention_norm.weight diff: 0\n",
      "layers.33.mlp.dense_h_to_4h.weight diff: 0\n",
      "layers.33.mlp.dense_4h_to_h.weight diff: 0\n",
      "layers.34.input_norm.weight diff: 0\n",
      "layers.34.self_attention.query_key_value.weight diff: 0\n",
      "layers.34.self_attention.dense.weight diff: 0\n",
      "layers.34.post_attention_norm.weight diff: 0\n",
      "layers.34.mlp.dense_h_to_4h.weight diff: 0\n",
      "layers.34.mlp.dense_4h_to_h.weight diff: 0\n",
      "layers.35.input_norm.weight diff: 0\n",
      "layers.35.self_attention.query_key_value.weight diff: 0\n",
      "layers.35.self_attention.dense.weight diff: 0\n",
      "layers.35.post_attention_norm.weight diff: 0\n",
      "layers.35.mlp.dense_h_to_4h.weight diff: 0\n",
      "layers.35.mlp.dense_4h_to_h.weight diff: 0\n",
      "layers.36.input_norm.weight diff: 0\n",
      "layers.36.self_attention.query_key_value.weight diff: 0\n",
      "layers.36.self_attention.dense.weight diff: 0\n",
      "layers.36.post_attention_norm.weight diff: 0\n",
      "layers.36.mlp.dense_h_to_4h.weight diff: 0\n",
      "layers.36.mlp.dense_4h_to_h.weight diff: 0\n",
      "layers.37.input_norm.weight diff: 0\n",
      "layers.37.self_attention.query_key_value.weight diff: 0\n",
      "layers.37.self_attention.dense.weight diff: 0\n",
      "layers.37.post_attention_norm.weight diff: 0\n",
      "layers.37.mlp.dense_h_to_4h.weight diff: 0\n",
      "layers.37.mlp.dense_4h_to_h.weight diff: 0\n",
      "layers.38.input_norm.weight diff: 0\n",
      "layers.38.self_attention.query_key_value.weight diff: 0\n",
      "layers.38.self_attention.dense.weight diff: 0\n",
      "layers.38.post_attention_norm.weight diff: 0\n",
      "layers.38.mlp.dense_h_to_4h.weight diff: 0\n",
      "layers.38.mlp.dense_4h_to_h.weight diff: 0\n",
      "layers.39.input_norm.weight diff: 0\n",
      "layers.39.self_attention.query_key_value.weight diff: 0\n",
      "layers.39.self_attention.dense.weight diff: 0\n",
      "layers.39.post_attention_norm.weight diff: 0\n",
      "layers.39.mlp.dense_h_to_4h.weight diff: 0\n",
      "layers.39.mlp.dense_4h_to_h.weight diff: 0\n",
      "final_norm.weight diff: 0\n",
      "ALL diffs: {}\n"
     ]
    }
   ],
   "source": [
    "unicorn_model = unicorn_state_dict['model']['language_model']\n",
    "util_model = util_state_dict['model']['language_model']\n",
    "\n",
    "# Compare word_embedding\n",
    "unicorn_wte = unicorn_model['embedding']['word_embeddings']['weight']\n",
    "util_wte = util_model['embedding']['word_embeddings']['weight']\n",
    "wte_diff = (unicorn_wte != util_wte).sum()\n",
    "print(f\"wte diff: {wte_diff}\")\n",
    "\n",
    "# Compare lm_head\n",
    "unicorn_lm_head = unicorn_model['output_layer']['weight']\n",
    "util_lm_head = util_model['output_layer']['weight']\n",
    "lm_head_diff = (unicorn_lm_head != util_lm_head).sum()\n",
    "print(f\"lm_head diff: {lm_head_diff}\")\n",
    "\n",
    "# Compare transformer block\n",
    "diff_records = {}\n",
    "for k in util_model['encoder'].keys():\n",
    "    unicorn_v = unicorn_model['encoder'][k]\n",
    "    util_v = util_model['encoder'][k]\n",
    "    diff_v = (unicorn_v != util_v).sum()\n",
    "    print(f\"{k} diff: {diff_v}\")\n",
    "    if diff_v > 0:\n",
    "        diff_records[k] = diff_v\n",
    "print(f\"ALL diffs: {diff_records}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1522b4f-56fe-4b63-8b3b-f1f6faab455a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test forward precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ba75d9-e177-45f0-bfed-bd0229481d93",
   "metadata": {},
   "source": [
    "### 对齐记录\n",
    "\n",
    "(0) 直接跑不对齐的版本，输出基本也OK，top-10的ranking都对\n",
    "\n",
    "(1) 发现第一个TransformerBlock的输出就有差异，开始排查每一个子模块的输出结果，发现在QK^T的计算时使用的矩阵乘法算法有差异：\n",
    "* 在Megatron的实现中使用了`torch.baddbmm`，而HuggingFace的版本使用了`torch.matmul`。仿照Megatron的实现方式对HF代码作修改后，一个TransformerBlock的计算结果可以完全对齐。\n",
    "* 下面也有一个module说明计算结果的差异。\n",
    "\n",
    "(2) 发现最终logits的输出结果仍然不一致，开始回溯每一个TransformerBlock的计算结果，发现在第3个TransformerBlock就有少量结果无法对齐，逐渐扩散；再细致排查每一个子模块的输出结果，发现第3个TransformerBlock输入侧经过RMSNorm(input_normalization)的输出有一个element有差异。于是先尝试在Megatron代码中把FusedRMSNorm实现改为采用pytorch OP实现的朴素版本，发现差异消失；可以从输入到输出一直保持一致。\n",
    "* 保持Megatron的所有实现不变，把HF的实现改为FusedMixedRMSNorm，也能一致\n",
    "* FusedMixedRMSNorm for huggingface generating in multi-devices has bugs, currently worked on one device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9677249-5f0f-4498-9476-b07ee3865bc3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting num_layers to 40 from checkpoint\n",
      "Setting hidden_size to 5120 from checkpoint\n",
      "Setting ffn_hidden_size to 13824 from checkpoint\n",
      "Setting num_attention_heads to 40 from checkpoint\n",
      "Checkpoint did not provide arguments num_query_groups\n",
      "Setting kv_channels to 128 from checkpoint\n",
      "Setting max_position_embeddings to 4096 from checkpoint\n",
      "Setting position_embedding_type to rope from checkpoint\n",
      "Checkpoint did not provide arguments rotary_percent\n",
      "Setting add_bias_linear to False from checkpoint\n",
      "Setting swiglu to True from checkpoint\n",
      "Setting untie_embeddings_and_output_weights to True from checkpoint\n",
      "Checkpoint did not provide arguments apply_layernorm_1p\n",
      "Setting normalization to RMSNorm from checkpoint\n",
      "Setting padded_vocab_size to 32000 from checkpoint\n",
      "Setting tensor_model_parallel_size to 1 from checkpoint\n",
      "Setting pipeline_model_parallel_size to 1 from checkpoint\n",
      "Checkpoint did not provide arguments virtual_pipeline_model_parallel_size\n",
      "Checkpoint did not provide arguments num_layers_per_virtual_pipeline_stage\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "setting global batch size to 1\n",
      "WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication\n",
      "using torch.float16 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  add_bias_linear ................................. False\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_layernorm_1p .............................. False\n",
      "  apply_query_key_layer_scaling ................... False\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  async_tensor_model_parallel_allreduce ........... False\n",
      "  attention_dropout ............................... 0.0\n",
      "  attention_softmax_in_fp32 ....................... True\n",
      "  barrier_with_L1_time ............................ True\n",
      "  bert_binary_head ................................ True\n",
      "  bert_embedder_type .............................. megatron\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_attn_linear ................................ False\n",
      "  bias_dropout_fusion ............................. False\n",
      "  bias_gelu_fusion ................................ False\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  bucket_size ..................................... 40000000\n",
      "  check_for_nan_in_loss_and_grad .................. True\n",
      "  classes_fraction ................................ 1.0\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  continue_on_missing_checkpoint .................. False\n",
      "  data_cache_path ................................. None\n",
      "  data_parallel_random_init ....................... False\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... None\n",
      "  data_per_class_fraction ......................... 1.0\n",
      "  data_sharding ................................... True\n",
      "  dataloader_type ................................. single\n",
      "  decoder_num_layers .............................. None\n",
      "  decoder_seq_length .............................. None\n",
      "  delay_grad_reduce ............................... True\n",
      "  dino_bottleneck_size ............................ 256\n",
      "  dino_freeze_last_layer .......................... 1\n",
      "  dino_head_hidden_size ........................... 2048\n",
      "  dino_local_crops_number ......................... 10\n",
      "  dino_local_img_size ............................. 96\n",
      "  dino_norm_last_layer ............................ False\n",
      "  dino_teacher_temp ............................... 0.07\n",
      "  dino_warmup_teacher_temp ........................ 0.04\n",
      "  dino_warmup_teacher_temp_epochs ................. 30\n",
      "  disable_scaled_init_method ...................... False\n",
      "  distribute_saved_activations .................... False\n",
      "  distributed_backend ............................. nccl\n",
      "  distributed_timeout_minutes ..................... 10\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  encoder_num_layers .............................. 40\n",
      "  encoder_seq_length .............................. 3\n",
      "  end_weight_decay ................................ 0.01\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 100\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... None\n",
      "  exit_signal_handler ............................. False\n",
      "  expert_model_parallel_size ...................... 1\n",
      "  expert_parallel ................................. False\n",
      "  ffn_hidden_size ................................. 13824\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ True\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  fp8 ............................................. None\n",
      "  fp8_amax_compute_algo ........................... most_recent\n",
      "  fp8_amax_history_len ............................ 1\n",
      "  fp8_interval .................................... 1\n",
      "  fp8_margin ...................................... 0\n",
      "  fp8_wgrad ....................................... True\n",
      "  global_batch_size ............................... 1\n",
      "  gradient_accumulation_fusion .................... True\n",
      "  head_lr_mult .................................... 1.0\n",
      "  hidden_dropout .................................. 0.0\n",
      "  hidden_size ..................................... 5120\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_h ........................................... 224\n",
      "  img_w ........................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  inference_batch_times_seqlen_threshold .......... 512\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  iter_per_epoch .................................. 1250\n",
      "  iteration ....................................... 0\n",
      "  kv_channels ..................................... 128\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ /cpfs/29ccba8f16c61395/data/user/liushan/projects/demo/Megatron-LM/tests/unit_tests/models/llama-megatron-t1p1\n",
      "  local_rank ...................................... None\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 100\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  log_world_size_to_tensorboard ................... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. None\n",
      "  lr_decay_iters .................................. None\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. linear\n",
      "  lr_warmup_fraction .............................. None\n",
      "  lr_warmup_init .................................. 0.0\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_factor ..................................... 1.0\n",
      "  mask_prob ....................................... 0.15\n",
      "  mask_type ....................................... random\n",
      "  masked_softmax_fusion ........................... False\n",
      "  max_position_embeddings ......................... 4096\n",
      "  max_tokens_to_oom ............................... 12000\n",
      "  mem_efficient_ln ................................ True\n",
      "  merge_file ...................................... None\n",
      "  micro_batch_size ................................ 1\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 0.0\n",
      "  mmap_warmup ..................................... False\n",
      "  model_spec ...................................... None\n",
      "  model_type ...................................... ModelType.encoder_or_decoder\n",
      "  no_load_lr_scheduler ............................ True\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... True\n",
      "  no_persist_layer_norm ........................... False\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  no_shuffle_dataset .............................. None\n",
      "  norm_epsilon .................................... 1e-05\n",
      "  normalization ................................... RMSNorm\n",
      "  num_attention_heads ............................. 40\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_experts ..................................... None\n",
      "  num_layers ...................................... 40\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_query_groups ................................ None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  output_bert_embeddings .......................... False\n",
      "  overlap_grad_reduce ............................. False\n",
      "  overlap_p2p_comm ................................ False\n",
      "  override_opt_param_scheduler .................... False\n",
      "  padded_vocab_size ............................... 32000\n",
      "  params_dtype .................................... torch.float16\n",
      "  patch_dim ....................................... 16\n",
      "  perform_initialization .......................... False\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  position_embedding_type ......................... rope\n",
      "  profile ......................................... False\n",
      "  profile_ranks ................................... [0]\n",
      "  profile_step_end ................................ 12\n",
      "  profile_step_start .............................. 10\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  recompute_granularity ........................... None\n",
      "  recompute_method ................................ None\n",
      "  recompute_num_layers ............................ None\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  reset_sample_and_iteration_stat ................. None\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  retro_add_retriever ............................. False\n",
      "  retro_cyclic_train_iters ........................ None\n",
      "  retro_encoder_attention_dropout ................. 0.1\n",
      "  retro_encoder_hidden_dropout .................... 0.1\n",
      "  retro_encoder_layers ............................ 2\n",
      "  retro_num_neighbors ............................. 2\n",
      "  retro_num_retrieved_chunks ...................... 2\n",
      "  retro_return_doc_ids ............................ False\n",
      "  retro_workdir ................................... None\n",
      "  rotary_percent .................................. 1.0\n",
      "  rotary_seq_len_interpolation_factor ............. None\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ None\n",
      "  save_interval ................................... None\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 3\n",
      "  sequence_parallel ............................... False\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  skip_train ...................................... False\n",
      "  split ........................................... 969, 30, 1\n",
      "  squared_relu .................................... False\n",
      "  standalone_embedding_stage ...................... False\n",
      "  start_weight_decay .............................. 0.01\n",
      "  swiglu .......................................... True\n",
      "  swiglu_make_ffn_hidden_size_divisible_by ........ 256\n",
      "  swin_backbone_type .............................. tiny\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  test_data_path .................................. None\n",
      "  timing_log_level ................................ 0\n",
      "  timing_log_option ............................... minmax\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_model ................................. None\n",
      "  tokenizer_name_or_path .......................... None\n",
      "  tokenizer_type .................................. NullTokenizer\n",
      "  tp_shared_dataloader ............................ True\n",
      "  train_data_path ................................. None\n",
      "  train_iters ..................................... 10\n",
      "  train_samples ................................... None\n",
      "  transformer_impl ................................ local\n",
      "  transformer_pipeline_model_parallel_size ........ 1\n",
      "  untie_embeddings_and_output_weights ............. True\n",
      "  use_checkpoint_args ............................. False\n",
      "  use_checkpoint_opt_param_scheduler .............. False\n",
      "  use_cpu_initialization .......................... None\n",
      "  use_distributed_optimizer ....................... False\n",
      "  use_flash_attn .................................. False\n",
      "  use_one_sent_docs ............................... False\n",
      "  use_ring_exchange_p2p ........................... False\n",
      "  valid_data_path ................................. None\n",
      "  variable_seq_lengths ............................ False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vision_backbone_type ............................ vit\n",
      "  vision_pretraining .............................. False\n",
      "  vision_pretraining_type ......................... classify\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... None\n",
      "  vocab_size ...................................... 31999\n",
      "  weight_decay .................................... 0.01\n",
      "  weight_decay_incr_style ......................... constant\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building NullTokenizer tokenizer ...\n",
      "building GPT model ...\n",
      " loading release checkpoint from /cpfs/29ccba8f16c61395/data/user/liushan/projects/demo/Megatron-LM/tests/unit_tests/models/llama-megatron-t1p1\n",
      " checkpoint version 3.0\n",
      "  successfully loaded checkpoint from /cpfs/29ccba8f16c61395/data/user/liushan/projects/demo/Megatron-LM/tests/unit_tests/models/llama-megatron-t1p1 at iteration 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Float16Module(\n",
       "  (module): GPTModel(\n",
       "    (language_model): TransformerLanguageModel(\n",
       "      (embedding): Embedding(\n",
       "        (word_embeddings): VocabParallelEmbedding()\n",
       "        (embedding_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (rotary_pos_emb): RotaryEmbedding()\n",
       "      (encoder): ParallelTransformer(\n",
       "        (layers): ModuleList(\n",
       "          (0-39): 40 x ParallelTransformerLayer(\n",
       "            (input_norm): MixedFusedRMSNorm()\n",
       "            (self_attention): ParallelAttention(\n",
       "              (query_key_value): ColumnParallelLinear()\n",
       "              (core_attention): CoreAttention(\n",
       "                (scale_mask_softmax): FusedScaleMaskSoftmax()\n",
       "                (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (dense): RowParallelLinear()\n",
       "            )\n",
       "            (post_attention_norm): MixedFusedRMSNorm()\n",
       "            (mlp): ParallelMLP(\n",
       "              (dense_h_to_4h): ColumnParallelLinear()\n",
       "              (dense_4h_to_h): RowParallelLinear()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_norm): MixedFusedRMSNorm()\n",
       "      )\n",
       "      (output_layer): ColumnParallelLinear()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since models/llama-megatron-t1p1 == megatron-util-t1p1,\n",
    "# we use models/llama-megatron-t1p1 to initialize megatron model.\n",
    "from megatron.arguments import parse_args\n",
    "\n",
    "inputs_mock = [13, 22, 35]\n",
    "position_mock = list(range(len(inputs_mock)))\n",
    "device = 'cuda'\n",
    "\n",
    "sys.argv = [\n",
    "    \"script.py\",\n",
    "    \"--load\", os.path.join(this_filepath, \"models\", \"llama-megatron-t1p1\"),\n",
    "    \"--micro-batch-size\", \"1\",\n",
    "    \"--no-initialization\",\n",
    "    \"--no-async-tensor-model-parallel-allreduce\",\n",
    "    \"--train-iters\", \"10\",\n",
    "    \"--no-load-lr-scheduler\",\n",
    "    \"--no-load-rng\",\n",
    "    \"--position-embedding-type\", \"rope\",\n",
    "    \"--fp16\",\n",
    "    \"--attention-softmax-in-fp32\",\n",
    "    \"--no-bias-gelu-fusion\",\n",
    "    \"--no-bias-dropout-fusion\",\n",
    "    \"--no-query-key-layer-scaling\",\n",
    "    \"--no-masked-softmax-fusion\",\n",
    "    \"--seq-length\", str(len(inputs_mock)),\n",
    "    \"--attention-dropout\", \"0.0\",\n",
    "    \"--hidden-dropout\", \"0.0\",\n",
    "    \"--tokenizer-type\", \"NullTokenizer\",\n",
    "    \"--vocab-size\", str(32000 - 1),\n",
    "]\n",
    "margs = parse_args()\n",
    "\n",
    "megatron_model, _, _ = Utils.gpt_load_checkpoint(margs, load_scheduler=False)\n",
    "\n",
    "_set_global_memory_buffer()\n",
    "model_parallel_cuda_manual_seed(42)\n",
    "\n",
    "megatron_model.to(device)\n",
    "megatron_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "caf956b9-e600-4c72-9a9f-d22bd3d2715f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> encoder_input: tensor([[[-3.3112e-03, -5.1117e-04, -1.2665e-03,  ..., -6.7902e-04,\n",
      "           7.7248e-05, -8.2397e-04]],\n",
      "\n",
      "        [[-1.0376e-02,  4.6997e-03,  3.4912e-02,  ...,  1.0315e-02,\n",
      "          -1.2817e-02, -1.1902e-02]],\n",
      "\n",
      "        [[-2.3842e-06, -6.2585e-06,  6.6757e-06,  ..., -9.1791e-06,\n",
      "          -3.3379e-06, -1.3113e-06]]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<TransposeBackward0>), norm: 1.0498046875\n",
      "=> Layer-0 hidden_states: tensor([[[-0.0010,  0.0044,  0.0078,  ...,  0.0193,  0.0158,  0.0353]],\n",
      "\n",
      "        [[-0.0307, -0.0312,  0.0394,  ..., -0.0051, -0.0293, -0.0594]],\n",
      "\n",
      "        [[ 0.0998,  0.0155,  0.0452,  ...,  0.0493,  0.0040,  0.0309]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>), norm: 53.34375\n",
      "=> Layer-1 hidden_states: tensor([[[-0.0136,  0.0066,  0.0076,  ...,  0.0031,  0.0281,  0.0287]],\n",
      "\n",
      "        [[-0.0829, -0.0149,  0.0231,  ..., -0.0282, -0.0173, -0.0608]],\n",
      "\n",
      "        [[ 0.0880,  0.0363,  0.0485,  ...,  0.0143,  0.0123,  0.0161]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>), norm: 86.0625\n",
      "=> Layer-2 hidden_states: tensor([[[-0.0116,  0.0279,  0.0206,  ...,  0.0133,  0.0001,  0.0441]],\n",
      "\n",
      "        [[-0.0760, -0.0115,  0.0724,  ..., -0.0380, -0.0143, -0.0739]],\n",
      "\n",
      "        [[ 0.0735,  0.0697,  0.0503,  ...,  0.0310, -0.0193,  0.0465]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>), norm: 85.625\n",
      "=> Layer-3 hidden_states: tensor([[[ 0.2534,  0.1616,  0.1797,  ..., -0.0370,  0.4521, -0.2864]],\n",
      "\n",
      "        [[-0.0408, -0.0609,  0.1205,  ..., -0.0791, -0.0075, -0.0311]],\n",
      "\n",
      "        [[ 0.2949,  0.0739, -0.0535,  ..., -0.0373, -0.0328, -0.0049]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>), norm: 1466.0\n",
      "=> Layer-4 hidden_states: tensor([[[ 0.2477,  0.1405,  0.1838,  ..., -0.0384,  0.4890, -0.2898]],\n",
      "\n",
      "        [[-0.0036, -0.1068,  0.0654,  ..., -0.0767,  0.0751, -0.0099]],\n",
      "\n",
      "        [[ 0.3025, -0.0076, -0.0720,  ...,  0.0624, -0.0138, -0.0669]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>), norm: 1467.0\n",
      "=> Layer-5 hidden_states: tensor([[[ 0.2639,  0.1403,  0.2008,  ..., -0.0975,  0.5488, -0.3044]],\n",
      "\n",
      "        [[-0.0510, -0.1606,  0.0351,  ..., -0.1249,  0.1207,  0.0290]],\n",
      "\n",
      "        [[ 0.3186, -0.0526,  0.0225,  ..., -0.2761,  0.2123, -0.0452]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>), norm: 1468.0\n",
      "=> Layer-6 hidden_states: tensor([[[ 0.3188,  0.1255,  0.1394,  ..., -0.2202,  0.6211, -0.3174]],\n",
      "\n",
      "        [[-0.0192, -0.0486,  0.0311,  ..., -0.0365,  0.1648, -0.1088]],\n",
      "\n",
      "        [[ 0.4001, -0.1116,  0.1785,  ..., -0.5669,  0.1038, -0.1089]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>), norm: 1470.0\n",
      "=> Layer-7 hidden_states: tensor([[[ 0.1066, -0.4133,  0.1328,  ..., -0.0848,  0.4285, -0.3513]],\n",
      "\n",
      "        [[-0.1769, -0.1277,  0.0770,  ..., -0.0576,  0.0363,  0.1914]],\n",
      "\n",
      "        [[-0.0105, -0.7637, -0.0719,  ..., -0.3530, -0.2954, -0.2295]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>), norm: 1554.0\n",
      "=> Layer-8 hidden_states: tensor([[[ 0.0664, -0.4138,  0.1230,  ..., -0.0760,  0.4775, -0.3628]],\n",
      "\n",
      "        [[-0.2338, -0.1570, -0.0809,  ..., -0.1920,  0.0120,  0.2578]],\n",
      "\n",
      "        [[-0.1479, -0.7124, -0.2452,  ..., -0.2474, -0.2959, -0.4189]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>), norm: 1554.0\n",
      "=> Layer-9 hidden_states: tensor([[[ 6.3599e-02, -4.0405e-01,  1.0846e-01,  ..., -9.8389e-02,\n",
      "           5.2148e-01, -3.5449e-01]],\n",
      "\n",
      "        [[-2.4280e-01, -6.1035e-04, -1.8005e-01,  ..., -1.9495e-01,\n",
      "          -9.5825e-02,  3.1177e-01]],\n",
      "\n",
      "        [[-3.2593e-02, -6.1426e-01, -3.4497e-01,  ..., -6.2164e-02,\n",
      "           8.5388e-02, -2.5537e-01]]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<AddBackward0>), norm: 1555.0\n",
      "=> Layer-10 hidden_states: tensor([[[ 5.0690e-02, -3.7744e-01,  6.4209e-02,  ..., -4.6356e-02,\n",
      "           5.8496e-01, -3.9697e-01]],\n",
      "\n",
      "        [[-2.4353e-01, -1.1621e-01, -2.2888e-01,  ..., -1.7847e-01,\n",
      "          -2.3352e-01,  3.0420e-01]],\n",
      "\n",
      "        [[-2.6270e-01, -3.6182e-01, -2.8101e-01,  ..., -2.7466e-04,\n",
      "           4.5239e-01, -3.3252e-01]]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<AddBackward0>), norm: 1555.0\n",
      "=> Layer-11 hidden_states: tensor([[[ 0.0142, -0.4092,  0.0839,  ..., -0.0582,  0.6138, -0.4204]],\n",
      "\n",
      "        [[-0.1087, -0.2051, -0.2302,  ..., -0.0939, -0.2477,  0.1696]],\n",
      "\n",
      "        [[-0.2717, -0.2185, -0.5264,  ..., -0.3821,  0.5815, -0.2178]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>), norm: 1555.0\n",
      "=> Layer-12 hidden_states: tensor([[[ 0.0294, -0.4114,  0.0811,  ..., -0.0363,  0.6255, -0.4238]],\n",
      "\n",
      "        [[-0.2035, -0.1412,  0.0735,  ...,  0.0497, -0.3086,  0.1240]],\n",
      "\n",
      "        [[-0.1790,  0.1179, -0.7720,  ..., -0.5439,  0.5464, -0.0984]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>), norm: 1557.0\n",
      "=> Layer-13 hidden_states: tensor([[[ 0.0619, -0.4495,  0.0605,  ..., -0.0270,  0.5771, -0.4368]],\n",
      "\n",
      "        [[-0.0129, -0.0542, -0.0867,  ...,  0.0627, -0.2561, -0.0389]],\n",
      "\n",
      "        [[-0.3499,  0.4153, -0.9722,  ..., -0.6216,  0.6782, -0.2559]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>), norm: 1558.0\n",
      "=> Layer-14 hidden_states: tensor([[[ 0.0624, -0.3984,  0.0092,  ..., -0.0271,  0.5273, -0.4509]],\n",
      "\n",
      "        [[-0.0923, -0.2317, -0.2188,  ...,  0.0263, -0.0164,  0.0457]],\n",
      "\n",
      "        [[-0.6689,  0.5664, -0.9966,  ..., -0.6665,  0.7148, -0.5396]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>), norm: 1558.0\n",
      "=> Layer-15 hidden_states: tensor([[[-0.0149, -0.3506,  0.0228,  ..., -0.1061,  0.5850, -0.3687]],\n",
      "\n",
      "        [[-0.0376, -0.2505, -0.0959,  ...,  0.0431,  0.1337,  0.0779]],\n",
      "\n",
      "        [[-0.8057,  1.3799, -1.0840,  ..., -0.5303,  0.7559, -0.8154]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>), norm: 1559.0\n",
      "=> Layer-16 hidden_states: tensor([[[ 0.0184, -0.3455, -0.0515,  ..., -0.1219,  0.6021, -0.4456]],\n",
      "\n",
      "        [[-0.3616, -0.3152,  0.0278,  ..., -0.0121,  0.3545,  0.2153]],\n",
      "\n",
      "        [[-1.1133,  1.6562, -1.1240,  ..., -0.8735,  0.9751, -0.6948]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>), norm: 1560.0\n",
      "=> Layer-17 hidden_states: tensor([[[ 0.1819, -0.2749, -0.2396,  ..., -0.1981,  0.4624, -0.4736]],\n",
      "\n",
      "        [[-0.2566, -0.0320, -0.2175,  ..., -0.1804,  0.2844, -0.0255]],\n",
      "\n",
      "        [[-0.8052,  1.9658, -1.3633,  ..., -1.2871,  1.0918, -1.0928]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>), norm: 1561.0\n",
      "=> Layer-18 hidden_states: tensor([[[ 0.1141, -0.2996, -0.2925,  ..., -0.2046,  0.4580, -0.4668]],\n",
      "\n",
      "        [[-0.3967, -0.4602, -0.2180,  ..., -0.0048,  0.6758,  0.1462]],\n",
      "\n",
      "        [[-0.5908,  2.0723, -0.9160,  ..., -1.7178,  1.4004, -0.9883]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>), norm: 1562.0\n",
      "=> Layer-19 hidden_states: tensor([[[ 0.0684, -0.2524, -0.4053,  ..., -0.2410,  0.4937, -0.4895]],\n",
      "\n",
      "        [[-0.2257, -0.1250,  0.0638,  ...,  0.0061,  1.0039,  0.5259]],\n",
      "\n",
      "        [[-0.9150,  1.9180, -0.9238,  ..., -2.1016,  1.8271, -0.8721]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>), norm: 1562.0\n",
      "=> Layer-20 hidden_states: tensor([[[ 0.0736, -0.2054, -0.4771,  ..., -0.3389,  0.5469, -0.4431]],\n",
      "\n",
      "        [[-0.4653, -0.1765,  0.4448,  ..., -0.0771,  1.4102,  0.8311]],\n",
      "\n",
      "        [[-0.9985,  1.5000, -1.0996,  ..., -2.3223,  2.1328, -0.5430]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>), norm: 1563.0\n",
      "=> Layer-21 hidden_states: tensor([[[ 0.1175, -0.0459, -0.4741,  ..., -0.2151,  0.3032, -0.4094]],\n",
      "\n",
      "        [[-0.6802, -0.4482,  0.4839,  ..., -0.1604,  1.1230,  1.2324]],\n",
      "\n",
      "        [[-1.1191,  1.8008, -1.1055,  ..., -2.7910,  1.8418,  0.0707]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>), norm: 1566.0\n",
      "=> Layer-22 hidden_states: tensor([[[ 0.0823,  0.0414, -0.4421,  ..., -0.2411,  0.2302, -0.3923]],\n",
      "\n",
      "        [[-0.5205, -0.4250,  0.6079,  ..., -0.3130,  1.3076,  1.4297]],\n",
      "\n",
      "        [[-1.2295,  1.7158, -0.8896,  ..., -3.0898,  1.3945,  0.4644]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>), norm: 1569.0\n",
      "=> Layer-23 hidden_states: tensor([[[ 0.1151,  0.0778, -0.4497,  ..., -0.2783,  0.2969, -0.3442]],\n",
      "\n",
      "        [[-0.4292,  0.0042,  0.6660,  ..., -0.5029,  1.6348,  1.2285]],\n",
      "\n",
      "        [[-1.4609,  1.9258, -0.9395,  ..., -3.2051,  1.3174,  0.6968]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>), norm: 1570.0\n",
      "=> Layer-24 hidden_states: tensor([[[ 0.1748,  0.1095, -0.4663,  ..., -0.3071,  0.3789, -0.3657]],\n",
      "\n",
      "        [[-0.5518, -0.3201,  0.4595,  ..., -0.4844,  1.8896,  2.1348]],\n",
      "\n",
      "        [[-1.4131,  1.8789, -0.6270,  ..., -3.4121,  1.5371,  1.3486]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>), norm: 1570.0\n",
      "=> Layer-25 hidden_states: tensor([[[-0.0509, -0.1649, -0.4453,  ..., -0.2268,  0.2583, -0.4648]],\n",
      "\n",
      "        [[-1.0908, -0.2123,  0.2896,  ..., -0.1055,  2.1094,  2.3398]],\n",
      "\n",
      "        [[-1.7510,  1.3691, -0.7388,  ..., -3.3848,  1.6455,  1.2236]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>), norm: 1570.0\n",
      "=> Layer-26 hidden_states: tensor([[[-0.0502, -0.1536, -0.4917,  ..., -0.2622,  0.2949, -0.4578]],\n",
      "\n",
      "        [[-1.0049, -0.3948,  0.3999,  ...,  0.2737,  2.5508,  2.0996]],\n",
      "\n",
      "        [[-1.8506,  0.9375, -0.4722,  ..., -2.8691,  2.3281,  1.5098]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>), norm: 1571.0\n",
      "=> Layer-27 hidden_states: tensor([[[-0.0959, -0.1696, -0.5332,  ..., -0.3018,  0.3093, -0.4653]],\n",
      "\n",
      "        [[-0.8584, -0.0968,  0.2686,  ...,  0.3130,  2.6895,  2.1289]],\n",
      "\n",
      "        [[-1.8691,  1.0166, -0.1299,  ..., -2.8652,  2.1953,  1.6016]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>), norm: 1571.0\n",
      "=> Layer-28 hidden_states: tensor([[[-0.1153, -0.1332, -0.5439,  ..., -0.2722,  0.3013, -0.4849]],\n",
      "\n",
      "        [[-0.1479,  0.1040,  0.0205,  ...,  0.4197,  2.7188,  1.9277]],\n",
      "\n",
      "        [[-2.0156,  1.0596, -0.2142,  ..., -3.0176,  1.8232,  1.4199]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>), norm: 1571.0\n",
      "=> Layer-29 hidden_states: tensor([[[-0.1117, -0.0797, -0.5732,  ..., -0.2632,  0.2876, -0.5498]],\n",
      "\n",
      "        [[-0.1206,  0.3992,  0.3838,  ...,  0.1068,  2.0020,  2.2305]],\n",
      "\n",
      "        [[-1.8975,  1.3047, -0.5967,  ..., -3.1914,  1.8301,  1.7012]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>), norm: 1572.0\n",
      "=> Layer-30 hidden_states: tensor([[[-0.1372, -0.0850, -0.6318,  ..., -0.2573,  0.3379, -0.6157]],\n",
      "\n",
      "        [[-0.1995,  0.2277,  0.3247,  ...,  0.1155,  2.0879,  2.8262]],\n",
      "\n",
      "        [[-1.6680,  0.9517, -0.7036,  ..., -2.5254,  2.1738,  1.3652]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>), norm: 1572.0\n",
      "=> Layer-31 hidden_states: tensor([[[-0.1639, -0.1470, -0.6982,  ..., -0.2284,  0.3718, -0.7432]],\n",
      "\n",
      "        [[-0.2340,  0.4653,  0.6084,  ..., -0.0206,  2.5000,  3.0000]],\n",
      "\n",
      "        [[-1.5859,  0.7559, -0.5615,  ..., -2.2500,  2.0820,  1.8711]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>), norm: 1572.0\n",
      "=> Layer-32 hidden_states: tensor([[[-0.2115, -0.2025, -0.7739,  ..., -0.2046,  0.3176, -0.8696]],\n",
      "\n",
      "        [[-0.8843,  0.6372,  0.7139,  ..., -0.0254,  2.4961,  2.8770]],\n",
      "\n",
      "        [[-1.7773,  0.6968, -0.6982,  ..., -2.0195,  1.8223,  1.7725]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>), norm: 1572.0\n",
      "=> Layer-33 hidden_states: tensor([[[-0.3513, -0.2219, -0.7358,  ..., -0.2311,  0.3813, -0.9214]],\n",
      "\n",
      "        [[-0.4697,  0.3088,  0.2935,  ...,  0.2118,  2.2871,  2.6621]],\n",
      "\n",
      "        [[-1.8916,  0.8516, -0.8491,  ..., -1.4473,  2.1484,  1.6289]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>), norm: 1571.0\n",
      "=> Layer-34 hidden_states: tensor([[[-0.4482, -0.3127, -0.8057,  ..., -0.1896,  0.4507, -0.9487]],\n",
      "\n",
      "        [[-0.9463,  0.4604,  0.7910,  ...,  0.7754,  2.2930,  2.6953]],\n",
      "\n",
      "        [[-2.5215,  0.1902, -0.6025,  ..., -1.0928,  2.0938,  1.6904]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>), norm: 1571.0\n",
      "=> Layer-35 hidden_states: tensor([[[-0.4270, -0.3154, -0.8589,  ..., -0.2764,  0.5322, -1.0078]],\n",
      "\n",
      "        [[-0.5142,  0.7681,  0.5601,  ...,  0.4668,  2.8418,  2.5195]],\n",
      "\n",
      "        [[-2.4941,  0.3345, -0.7397,  ..., -1.3525,  2.1230,  1.7305]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>), norm: 1570.0\n",
      "=> Layer-36 hidden_states: tensor([[[-0.5488, -0.3396, -0.8428,  ..., -0.3821,  0.8486, -1.0723]],\n",
      "\n",
      "        [[-0.5508,  0.4075,  0.6328,  ...,  0.7607,  2.3789,  2.2168]],\n",
      "\n",
      "        [[-2.1211,  0.0457, -0.2319,  ..., -0.8574,  2.3828,  1.3350]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>), norm: 1570.0\n",
      "=> Layer-37 hidden_states: tensor([[[-0.8486, -0.1123, -0.5972,  ..., -0.6250,  1.4668, -0.8789]],\n",
      "\n",
      "        [[-0.7471,  0.2494, -0.0811,  ...,  0.6655,  2.0820,  1.4238]],\n",
      "\n",
      "        [[-2.5605,  0.7490, -0.6597,  ..., -0.7905,  1.1719,  0.8745]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>), norm: 1547.0\n",
      "=> Layer-38 hidden_states: tensor([[[-0.7881, -0.2820,  0.1538,  ..., -0.3171,  1.1396,  0.0889]],\n",
      "\n",
      "        [[-0.9380,  0.5391,  0.6123,  ...,  0.8281,  1.8135,  1.6660]],\n",
      "\n",
      "        [[-3.2422,  1.9121,  0.9258,  ..., -1.6924,  2.1875,  1.7803]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>), norm: 1057.0\n",
      "=> Layer-39 hidden_states: tensor([[[ 4.1250,  0.9678,  1.9297,  ..., -0.9268, -0.7290, -0.1493]],\n",
      "\n",
      "        [[-2.2227, -0.0767,  0.1482,  ...,  0.3870,  0.9121,  1.4277]],\n",
      "\n",
      "        [[-2.8828,  1.8418,  0.2791,  ..., -1.3818,  1.3467,  1.6904]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>), norm: 607.5\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.tensor([inputs_mock]).to(device)\n",
    "position_ids = torch.tensor([position_mock]).to(device)\n",
    "\n",
    "seq_length = len(inputs_mock)\n",
    "att_mask_batch = 1\n",
    "attention_mask = torch.tril(\n",
    "    torch.ones((att_mask_batch, seq_length, seq_length), device=device)).view(\n",
    "        att_mask_batch, 1, seq_length, seq_length)\n",
    "attention_mask = (attention_mask < 0.5)\n",
    "\n",
    "megatron_logits = megatron_model.forward(input_ids, position_ids, attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b026114-8335-4794-b1b0-acb15fc469ed",
   "metadata": {},
   "source": [
    "#### 直接用官方HuggingFace的LLaMA实现 (transformers==4.32.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ca961c3-665d-42d4-ade5-e28931e6dc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> You are using the LlamaForCausalLM with some modifications ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:12<00:00,  4.24s/it]\n"
     ]
    }
   ],
   "source": [
    "# Run the modeling_llama directly without modification\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(llama_root,\n",
    "                                                torch_dtype=torch.float16,\n",
    "                                                device_map=\"cuda:0\",\n",
    "                                                trust_remote_code=True).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3900d2e4-a067-47f2-9d4b-cd9c34385976",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([inputs_mock]).to(\"cuda\")\n",
    "attention_mask = torch.ones_like(input_ids)\n",
    "hf_logits = hf_model(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "995c711e-dfab-4c02-9bab-795e31fdf45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "megatron_logits: tensor([[[-4.8516, -6.6953,  5.3477,  ..., -2.1328, -3.3652, -2.7891],\n",
      "         [-4.8477, -7.8086,  2.3770,  ..., -2.4609, -2.5469, -3.4395],\n",
      "         [-1.3828, -6.7852,  0.2759,  ..., -2.1406, -1.5371, -1.1377]]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "hf_logits: tensor([[[-4.8555, -6.6953,  5.3477,  ..., -2.1328, -3.3652, -2.7891],\n",
      "         [-4.8477, -7.8125,  2.3750,  ..., -2.4570, -2.5469, -3.4355],\n",
      "         [-1.3828, -6.8008,  0.2744,  ..., -2.1406, -1.5381, -1.1387]]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "final digits >> diff: 62666\n"
     ]
    }
   ],
   "source": [
    "print(\"megatron_logits:\", megatron_logits)\n",
    "print(\"hf_logits:\", hf_logits.logits)\n",
    "diff = (megatron_logits != hf_logits.logits).sum()\n",
    "\n",
    "print(f\"final digits >> diff: {diff.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "924c3912-39e1-4f68-a2d9-18f2a45553c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top-10 diff: 0\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "hf_topk = hf_logits[0][0].topk(k, dim=-1)[-1]\n",
    "megatron_topk = megatron_logits[0].topk(k, dim=-1)[-1]\n",
    "print(f\"top-{k} diff:\", (hf_topk != megatron_topk).sum().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfbf488-ba62-4c2e-9ca0-61b87a745ca6",
   "metadata": {},
   "source": [
    "#### 在<llama_root>/config.json中开启与Megatron对齐的选项，再初始化新模型\n",
    "-  use_mlm_rmsnorm: true\n",
    "-  use_mlm_qkdot: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b8ab555f-19b9-43d8-8012-a6fa10cb899a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> You are using the LlamaForCausalLM with some modifications ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:13<00:00,  4.40s/it]\n"
     ]
    }
   ],
   "source": [
    "# To edit {llama_root}/config.json: \n",
    "# -> use_mlm_rmsnorm: true\n",
    "# -> use_mlm_qkdot: true\n",
    "\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(llama_root,\n",
    "                                                torch_dtype=torch.float16,\n",
    "                                                device_map=\"cuda:0\",\n",
    "                                                trust_remote_code=True).to(\"cuda\")\n",
    "input_ids = torch.tensor([inputs_mock]).to(\"cuda\")\n",
    "attention_mask = torch.ones_like(input_ids)\n",
    "hf_logits = hf_model(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da24c700-6bac-497a-a191-b0bc2f17f99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "megatron_logits: tensor([[[-4.8516, -6.6953,  5.3477,  ..., -2.1328, -3.3652, -2.7891],\n",
      "         [-4.8477, -7.8086,  2.3770,  ..., -2.4609, -2.5469, -3.4395],\n",
      "         [-1.3828, -6.7852,  0.2759,  ..., -2.1406, -1.5371, -1.1377]]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "hf_logits: tensor([[[-4.8516, -6.6953,  5.3477,  ..., -2.1328, -3.3652, -2.7891],\n",
      "         [-4.8477, -7.8086,  2.3770,  ..., -2.4609, -2.5469, -3.4395],\n",
      "         [-1.3828, -6.7852,  0.2759,  ..., -2.1406, -1.5371, -1.1377]]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "final digits >> diff: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"megatron_logits:\", megatron_logits)\n",
    "print(\"hf_logits:\", hf_logits.logits)\n",
    "diff = (megatron_logits != hf_logits.logits).sum()\n",
    "\n",
    "print(f\"final digits >> diff: {diff.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bd219b-c4ab-4e70-9e3c-d588c073123d",
   "metadata": {},
   "source": [
    "#### torch.baddbmm in Megatron-LM vs. torch.matmul in Huggingface llama model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "193e3caf-1251-4246-b67c-bf6832068325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 40, 3, 128]) torch.Size([1, 40, 3, 128])\n",
      "torch.Size([3, 1, 40, 128]) torch.Size([3, 1, 40, 128])\n",
      "mlm_q vs. hf_q: 0\n",
      "mlm_k vs. hf_k: 0\n"
     ]
    }
   ],
   "source": [
    "# load inputs\n",
    "import math\n",
    "# ! rclone copy oss://inf-alpha/home/megatron/tests/test_convert_ckpt/tensors ./tensors\n",
    "save_root = f\"./tensors\"\n",
    "hf_q_rp = torch.load(os.path.join(save_root, \"hf_q_rope.pt\"))   # [b, np, sq, hn]\n",
    "hf_k_rp = torch.load(os.path.join(save_root, \"hf_k_rope.pt\"))   # [b, np, sk, hn]\n",
    "mlm_q_rp = torch.load(os.path.join(save_root, \"mlm_q_rope.pt\")) # [sq, b, np, hn]\n",
    "mlm_k_rp = torch.load(os.path.join(save_root, \"mlm_k_rope.pt\")) # [sk, b, np, hn]\n",
    "\n",
    "print(hf_q_rp.shape, hf_k_rp.shape)\n",
    "print(mlm_q_rp.shape, mlm_k_rp.shape)\n",
    "\n",
    "print(\"mlm_q vs. hf_q:\", (mlm_q_rp.transpose(0, 1).transpose(1, 2) != hf_q_rp).sum().item())\n",
    "print(\"mlm_k vs. hf_k:\", (mlm_k_rp.transpose(0, 1).transpose(1, 2) != hf_k_rp).sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "841e20a3-e50a-4a38-a141-013dd5c23a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 40, 3, 3])\n",
      "tensor([[-0.1736,  0.1776,  0.1196],\n",
      "        [ 1.6396, -2.5645,  1.4336],\n",
      "        [ 0.1628, -0.7998,  1.3223]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# HF attention_weights\n",
    "query_states = hf_q_rp \n",
    "key_states = hf_k_rp   \n",
    "head_dim = query_states.size(-1)\n",
    "\n",
    "attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(head_dim)\n",
    "\n",
    "print(attn_weights.shape)\n",
    "print(attn_weights[0][0])  # masks havn't added yet\n",
    "\n",
    "hf_results = attn_weights[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6f4e99ba-970e-4f04-b1c1-1cebaed22eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 3, 3])\n",
      "tensor([[-0.1736,  0.1777,  0.1196],\n",
      "        [ 1.6396, -2.5645,  1.4346],\n",
      "        [ 0.1627, -0.7998,  1.3223]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Megatron attention_weights\n",
    "# [b, np, sq, sk]\n",
    "output_size = (1,   # query_layer.size(1),\n",
    "               40,  # query_layer.size(2),\n",
    "               3,   # query_layer.size(0),\n",
    "               3,)  # key_layer.size(0))\n",
    "\n",
    "query_layer = mlm_q_rp\n",
    "key_layer = mlm_k_rp\n",
    "\n",
    "# [sq, b, np, hn] -> [sq, b * np, hn]\n",
    "query_layer = query_layer.reshape(output_size[2],\n",
    "                                  output_size[0] * output_size[1], -1)\n",
    "# [sk, b, np, hn] -> [sk, b * np, hn]\n",
    "key_layer = key_layer.view(output_size[3],\n",
    "                           output_size[0] * output_size[1], -1)\n",
    "\n",
    "matmul_input_buffer = torch.empty(\n",
    "    (output_size[0]*output_size[1], output_size[2], output_size[3]),\n",
    "    dtype=query_layer.dtype,\n",
    "    device=\"cuda\")\n",
    "\n",
    "norm_factor = math.sqrt(query_layer.size(-1))\n",
    "matmul_result = torch.baddbmm(\n",
    "            matmul_input_buffer,\n",
    "            query_layer.transpose(0, 1),   # [b * np, sq, hn]\n",
    "            key_layer.transpose(0, 1).transpose(1, 2),  # [b * np, hn, sk]\n",
    "            beta=0.0, alpha=(1.0/norm_factor))\n",
    "\n",
    "print(matmul_result.shape)\n",
    "print(matmul_result[0])\n",
    "mlm_results = matmul_result[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8f489e67-2d9e-4f14-87ab-096946907c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1736,  0.1777,  0.1196],\n",
      "        [ 1.6396, -2.5645,  1.4346],\n",
      "        [ 0.1627, -0.7998,  1.3223]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# HF -> Megatron alignment\n",
    "# Please check the difference results in https://pytorch.org/docs/stable/notes/numerical_accuracy.html#batched-computations-or-slice-computations\n",
    "# `torch.matmul` is difference to `torch.baddbmm`\n",
    "#\n",
    "query_states = hf_q_rp\n",
    "key_states = hf_k_rp\n",
    "\n",
    "output_size = (query_states.shape[0],\n",
    "               query_states.shape[1],\n",
    "               query_states.shape[2],\n",
    "               key_states.shape[2],)\n",
    "\n",
    "query_states = query_states.view(output_size[0] * output_size[1], output_size[2], -1)\n",
    "key_states = key_states.view(output_size[0] * output_size[1], output_size[3], -1).transpose(-1, -2)\n",
    "\n",
    "matmul_input_buffer = torch.empty(\n",
    "    (output_size[0]*output_size[1], output_size[2], output_size[3]),\n",
    "     dtype=query_layer.dtype,\n",
    "     device=\"cuda\")\n",
    "\n",
    "hidden_size_per_attention_head = 5120 // output_size[1]\n",
    "norm_factor = math.sqrt(hf_q_rp.size(-1))\n",
    "matmul_result = torch.baddbmm(\n",
    "            matmul_input_buffer,\n",
    "            query_states,   # [b * np, sq, hn]\n",
    "            key_states,  # [b * np, hn, sk]\n",
    "            beta=0.0, alpha=(1.0/norm_factor))\n",
    "\n",
    "matmul_result = matmul_result.view(*output_size)\n",
    "print(matmul_result[0][0])\n",
    "\n",
    "hf2mlm_results = matmul_result[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "005873ca-3b6d-43c6-99bf-3f516924ecbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf  results:\n",
      "\ttensor([[-0.1736,  0.0000,  0.0000],\n",
      "        [ 1.6396, -2.5645,  0.0000],\n",
      "        [ 0.1628, -0.7998,  1.3223]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<TrilBackward0>)\n",
      "\n",
      "mlm results:\n",
      "\ttensor([[-0.1736,  0.0000,  0.0000],\n",
      "        [ 1.6396, -2.5645,  0.0000],\n",
      "        [ 0.1627, -0.7998,  1.3223]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<TrilBackward0>)\n",
      "\n",
      "hf2mlm results:\n",
      "\ttensor([[-0.1736,  0.0000,  0.0000],\n",
      "        [ 1.6396, -2.5645,  0.0000],\n",
      "        [ 0.1627, -0.7998,  1.3223]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<TrilBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"hf  results:\\n\\t{torch.tril(hf_results)}\\n\")\n",
    "print(f\"mlm results:\\n\\t{torch.tril(mlm_results)}\\n\")\n",
    "print(f\"hf2mlm results:\\n\\t{torch.tril(hf2mlm_results)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3fafea-a8d3-482f-8375-4fcbcaa662cd",
   "metadata": {},
   "source": [
    "#### 其他\n",
    "- hf的实现中加了debug选项，可以把中间计算结果落盘\n",
    "- mlm的实现中直接加debug代码会影响正常逻辑，因此将修改的代码也上传到oss上，以后如果再碰到对精度的问题可以快速开始\n",
    "- 对mlm的代码修改放在oss目录 oss://inf-alpha/home/megatron/tests/test_convert_ckpt/code/Megatron-LM/\n",
    "- 下面的代码主要是对中间结果进行比较，直接运行不了仅作存档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97f6a755-9989-43ca-9387-c4f545aae921",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "save_root = \"<path to tensors>\"\n",
    "idx = 0\n",
    "hf_tensor = torch.load(os.path.join(save_root, f\"hf.{idx}.pt\"))\n",
    "mlm_tensor = torch.load(os.path.join(save_root, f\"mlm.{idx}.pt\"))\n",
    "\n",
    "hf_tensors = []\n",
    "mlm_tensors = []\n",
    "for i in range(40):\n",
    "    if os.path.exists(os.path.join(save_root, f\"hf.{i}.pt\")) and \\\n",
    "       os.path.exists(os.path.join(save_root, f\"mlm.{i}.pt\")):\n",
    "        hf_tensors.append(torch.load(os.path.join(save_root, f\"hf.{i}.pt\")))\n",
    "        mlm_tensors.append(torch.load(os.path.join(save_root, f\"mlm.{i}.pt\")))\n",
    "\n",
    "hf_wte = torch.load(os.path.join(save_root, f\"hf.wte.pt\"))\n",
    "mlm_wte = torch.load(os.path.join(save_root, f\"mlm.wte.pt\"))\n",
    "\n",
    "hf_in = torch.load(os.path.join(save_root, \"hf_input_norm.pt\"))\n",
    "mlm_in = torch.load(os.path.join(save_root, \"mlm_input_norm.pt\"))\n",
    "\n",
    "hf_q = torch.load(os.path.join(save_root, \"hf_q.pt\"))\n",
    "mlm_q = torch.load(os.path.join(save_root, \"mlm_q.pt\"))\n",
    "hf_k = torch.load(os.path.join(save_root, \"hf_k.pt\"))\n",
    "mlm_k = torch.load(os.path.join(save_root, \"mlm_k.pt\"))\n",
    "hf_q_rp = torch.load(os.path.join(save_root, \"hf_q_rope.pt\"))\n",
    "mlm_q_rp = torch.load(os.path.join(save_root, \"mlm_q_rope.pt\"))\n",
    "hf_k_rp = torch.load(os.path.join(save_root, \"hf_k_rope.pt\"))\n",
    "mlm_k_rp = torch.load(os.path.join(save_root, \"mlm_k_rope.pt\"))\n",
    "\n",
    "hf_attn_wi = torch.load(os.path.join(save_root, \"hf_attn_wi.pt\"))\n",
    "mlm_attn_wi = torch.load(os.path.join(save_root, \"mlm_attn_wi.pt\"))\n",
    "\n",
    "hf_attn_w = torch.load(os.path.join(save_root, \"hf_attn_w.pt\"))\n",
    "mlm_attn_w = torch.load(os.path.join(save_root, \"mlm_attn_w.pt\"))\n",
    "hf_attn_sm = torch.load(os.path.join(save_root, \"hf_attn_sm.pt\"))\n",
    "mlm_attn_sm = torch.load(os.path.join(save_root, \"mlm_attn_sm.pt\"))\n",
    "\n",
    "hf_ao = torch.load(os.path.join(save_root, \"hf_attn_output.pt\"))\n",
    "mlm_ao = torch.load(os.path.join(save_root, \"mlm_attn_output.pt\"))\n",
    "\n",
    "hf_pn = torch.load(os.path.join(save_root, \"hf_post_norm.pt\"))\n",
    "mlm_pn = torch.load(os.path.join(save_root, \"mlm_post_norm.pt\"))\n",
    "\n",
    "hf_mo = torch.load(os.path.join(save_root, \"hf_mlp_out.pt\"))\n",
    "mlm_mo = torch.load(os.path.join(save_root, \"mlm_mlp_out.pt\"))\n",
    "\n",
    "hf_fn = torch.load(os.path.join(save_root, \"hf.fn.pt\"))\n",
    "mlm_fn = torch.load(os.path.join(save_root, \"mlm.fn.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d547ffd-9d89-4cc2-a000-12bfb50b5110",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0010,  0.0044,  0.0078,  ...,  0.0193,  0.0158,  0.0353],\n",
      "        [-0.0307, -0.0312,  0.0394,  ..., -0.0051, -0.0293, -0.0594],\n",
      "        [ 0.0998,  0.0155,  0.0452,  ...,  0.0493,  0.0040,  0.0309]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "tensor([[-0.0010,  0.0044,  0.0078,  ...,  0.0193,  0.0158,  0.0353],\n",
      "        [-0.0307, -0.0312,  0.0394,  ..., -0.0051, -0.0293, -0.0594],\n",
      "        [ 0.0998,  0.0155,  0.0452,  ...,  0.0493,  0.0040,  0.0309]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "layer-0[torch.Size([3, 5120])]: 0\n",
      "tensor([[-0.0136,  0.0066,  0.0076,  ...,  0.0031,  0.0281,  0.0287],\n",
      "        [-0.0829, -0.0149,  0.0231,  ..., -0.0282, -0.0173, -0.0608],\n",
      "        [ 0.0880,  0.0363,  0.0485,  ...,  0.0143,  0.0123,  0.0161]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "tensor([[-0.0136,  0.0066,  0.0076,  ...,  0.0031,  0.0281,  0.0287],\n",
      "        [-0.0829, -0.0149,  0.0231,  ..., -0.0282, -0.0173, -0.0608],\n",
      "        [ 0.0880,  0.0363,  0.0485,  ...,  0.0143,  0.0123,  0.0161]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "layer-1[torch.Size([3, 5120])]: 0\n",
      "tensor([[-0.0116,  0.0279,  0.0206,  ...,  0.0133,  0.0001,  0.0441],\n",
      "        [-0.0760, -0.0115,  0.0724,  ..., -0.0380, -0.0143, -0.0739],\n",
      "        [ 0.0735,  0.0697,  0.0503,  ...,  0.0310, -0.0193,  0.0465]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "tensor([[-0.0116,  0.0279,  0.0206,  ...,  0.0133,  0.0001,  0.0441],\n",
      "        [-0.0760, -0.0115,  0.0724,  ..., -0.0380, -0.0143, -0.0739],\n",
      "        [ 0.0735,  0.0697,  0.0503,  ...,  0.0310, -0.0193,  0.0465]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "layer-2[torch.Size([3, 5120])]: 0\n",
      "tensor([[ 0.2534,  0.1616,  0.1797,  ..., -0.0370,  0.4521, -0.2864],\n",
      "        [-0.0408, -0.0609,  0.1205,  ..., -0.0791, -0.0075, -0.0311],\n",
      "        [ 0.2949,  0.0739, -0.0535,  ..., -0.0373, -0.0328, -0.0049]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "tensor([[ 0.2534,  0.1616,  0.1797,  ..., -0.0370,  0.4521, -0.2864],\n",
      "        [-0.0408, -0.0609,  0.1205,  ..., -0.0791, -0.0075, -0.0311],\n",
      "        [ 0.2949,  0.0739, -0.0535,  ..., -0.0373, -0.0328, -0.0049]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "layer-3[torch.Size([3, 5120])]: 0\n",
      "tensor([[ 0.2477,  0.1405,  0.1838,  ..., -0.0384,  0.4890, -0.2898],\n",
      "        [-0.0036, -0.1068,  0.0654,  ..., -0.0767,  0.0751, -0.0099],\n",
      "        [ 0.3025, -0.0076, -0.0720,  ...,  0.0624, -0.0138, -0.0669]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "tensor([[ 0.2477,  0.1405,  0.1838,  ..., -0.0384,  0.4890, -0.2898],\n",
      "        [-0.0036, -0.1068,  0.0654,  ..., -0.0767,  0.0751, -0.0099],\n",
      "        [ 0.3025, -0.0076, -0.0720,  ...,  0.0624, -0.0138, -0.0669]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "layer-4[torch.Size([3, 5120])]: 0\n",
      "tensor([[ 0.2639,  0.1403,  0.2008,  ..., -0.0975,  0.5488, -0.3044],\n",
      "        [-0.0510, -0.1606,  0.0351,  ..., -0.1249,  0.1207,  0.0290],\n",
      "        [ 0.3186, -0.0526,  0.0225,  ..., -0.2761,  0.2123, -0.0452]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "tensor([[ 0.2639,  0.1403,  0.2008,  ..., -0.0975,  0.5488, -0.3044],\n",
      "        [-0.0510, -0.1606,  0.0351,  ..., -0.1249,  0.1207,  0.0290],\n",
      "        [ 0.3186, -0.0526,  0.0225,  ..., -0.2761,  0.2123, -0.0452]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "layer-5[torch.Size([3, 5120])]: 0\n",
      "tensor([[ 0.3188,  0.1255,  0.1394,  ..., -0.2202,  0.6211, -0.3174],\n",
      "        [-0.0192, -0.0486,  0.0311,  ..., -0.0365,  0.1648, -0.1088],\n",
      "        [ 0.4001, -0.1116,  0.1785,  ..., -0.5669,  0.1038, -0.1089]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "tensor([[ 0.3188,  0.1255,  0.1394,  ..., -0.2202,  0.6211, -0.3174],\n",
      "        [-0.0192, -0.0486,  0.0311,  ..., -0.0365,  0.1648, -0.1088],\n",
      "        [ 0.4001, -0.1116,  0.1785,  ..., -0.5669,  0.1038, -0.1089]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "layer-6[torch.Size([3, 5120])]: 0\n",
      "tensor([[ 0.1066, -0.4133,  0.1328,  ..., -0.0848,  0.4285, -0.3513],\n",
      "        [-0.1769, -0.1277,  0.0770,  ..., -0.0576,  0.0363,  0.1914],\n",
      "        [-0.0105, -0.7637, -0.0719,  ..., -0.3530, -0.2954, -0.2294]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "tensor([[ 0.1066, -0.4133,  0.1328,  ..., -0.0848,  0.4285, -0.3513],\n",
      "        [-0.1769, -0.1277,  0.0770,  ..., -0.0576,  0.0363,  0.1914],\n",
      "        [-0.0105, -0.7637, -0.0719,  ..., -0.3530, -0.2954, -0.2294]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "layer-7[torch.Size([3, 5120])]: 0\n",
      "tensor([[ 0.0664, -0.4138,  0.1230,  ..., -0.0760,  0.4775, -0.3628],\n",
      "        [-0.2339, -0.1570, -0.0809,  ..., -0.1921,  0.0121,  0.2578],\n",
      "        [-0.1479, -0.7124, -0.2452,  ..., -0.2474, -0.2959, -0.4189]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "tensor([[ 0.0664, -0.4138,  0.1230,  ..., -0.0760,  0.4775, -0.3628],\n",
      "        [-0.2339, -0.1570, -0.0809,  ..., -0.1921,  0.0121,  0.2578],\n",
      "        [-0.1479, -0.7124, -0.2452,  ..., -0.2474, -0.2959, -0.4189]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "layer-8[torch.Size([3, 5120])]: 0\n",
      "tensor([[ 6.3599e-02, -4.0405e-01,  1.0846e-01,  ..., -9.8389e-02,\n",
      "          5.2148e-01, -3.5449e-01],\n",
      "        [-2.4292e-01, -4.8828e-04, -1.8005e-01,  ..., -1.9507e-01,\n",
      "         -9.5764e-02,  3.1201e-01],\n",
      "        [-3.2654e-02, -6.1426e-01, -3.4497e-01,  ..., -6.2286e-02,\n",
      "          8.5388e-02, -2.5562e-01]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "tensor([[ 6.3599e-02, -4.0405e-01,  1.0846e-01,  ..., -9.8389e-02,\n",
      "          5.2148e-01, -3.5449e-01],\n",
      "        [-2.4292e-01, -4.8828e-04, -1.8005e-01,  ..., -1.9507e-01,\n",
      "         -9.5764e-02,  3.1201e-01],\n",
      "        [-3.2654e-02, -6.1426e-01, -3.4497e-01,  ..., -6.2286e-02,\n",
      "          8.5388e-02, -2.5562e-01]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "layer-9[torch.Size([3, 5120])]: 0\n",
      "tensor([[ 5.0690e-02, -3.7744e-01,  6.4209e-02,  ..., -4.6356e-02,\n",
      "          5.8496e-01, -3.9697e-01],\n",
      "        [-2.4377e-01, -1.1597e-01, -2.2900e-01,  ..., -1.7871e-01,\n",
      "         -2.3340e-01,  3.0444e-01],\n",
      "        [-2.6294e-01, -3.6157e-01, -2.8101e-01,  ..., -3.9673e-04,\n",
      "          4.5239e-01, -3.3276e-01]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "tensor([[ 5.0690e-02, -3.7744e-01,  6.4209e-02,  ..., -4.6356e-02,\n",
      "          5.8496e-01, -3.9697e-01],\n",
      "        [-2.4377e-01, -1.1597e-01, -2.2900e-01,  ..., -1.7871e-01,\n",
      "         -2.3340e-01,  3.0444e-01],\n",
      "        [-2.6294e-01, -3.6157e-01, -2.8101e-01,  ..., -3.9673e-04,\n",
      "          4.5239e-01, -3.3276e-01]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "layer-10[torch.Size([3, 5120])]: 0\n",
      "tensor([[ 0.0142, -0.4092,  0.0839,  ..., -0.0582,  0.6138, -0.4204],\n",
      "        [-0.1086, -0.2046, -0.2302,  ..., -0.0941, -0.2473,  0.1697],\n",
      "        [-0.2722, -0.2180, -0.5259,  ..., -0.3823,  0.5815, -0.2178]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "tensor([[ 0.0142, -0.4092,  0.0839,  ..., -0.0582,  0.6138, -0.4204],\n",
      "        [-0.1086, -0.2046, -0.2302,  ..., -0.0941, -0.2473,  0.1697],\n",
      "        [-0.2722, -0.2180, -0.5259,  ..., -0.3823,  0.5815, -0.2178]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "layer-11[torch.Size([3, 5120])]: 0\n",
      "tensor([[ 0.0294, -0.4114,  0.0811,  ..., -0.0363,  0.6255, -0.4238],\n",
      "        [-0.2034, -0.1405,  0.0734,  ...,  0.0496, -0.3079,  0.1244],\n",
      "        [-0.1799,  0.1182, -0.7720,  ..., -0.5435,  0.5469, -0.0988]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "tensor([[ 0.0294, -0.4114,  0.0811,  ..., -0.0363,  0.6255, -0.4238],\n",
      "        [-0.2034, -0.1405,  0.0734,  ...,  0.0496, -0.3079,  0.1244],\n",
      "        [-0.1799,  0.1182, -0.7720,  ..., -0.5435,  0.5469, -0.0988]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "layer-12[torch.Size([3, 5120])]: 0\n",
      "tensor([[ 0.0619, -0.4495,  0.0605,  ..., -0.0270,  0.5771, -0.4368],\n",
      "        [-0.0127, -0.0535, -0.0870,  ...,  0.0624, -0.2554, -0.0386],\n",
      "        [-0.3508,  0.4158, -0.9722,  ..., -0.6221,  0.6792, -0.2563]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "tensor([[ 0.0619, -0.4495,  0.0605,  ..., -0.0270,  0.5771, -0.4368],\n",
      "        [-0.0127, -0.0535, -0.0870,  ...,  0.0624, -0.2554, -0.0386],\n",
      "        [-0.3508,  0.4158, -0.9722,  ..., -0.6221,  0.6792, -0.2563]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "layer-13[torch.Size([3, 5120])]: 0\n",
      "tensor([[ 0.0624, -0.3984,  0.0092,  ..., -0.0271,  0.5273, -0.4509],\n",
      "        [-0.0920, -0.2310, -0.2190,  ...,  0.0261, -0.0159,  0.0461],\n",
      "        [-0.6704,  0.5669, -0.9961,  ..., -0.6670,  0.7148, -0.5396]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "tensor([[ 0.0624, -0.3984,  0.0092,  ..., -0.0271,  0.5273, -0.4509],\n",
      "        [-0.0920, -0.2310, -0.2190,  ...,  0.0261, -0.0159,  0.0461],\n",
      "        [-0.6704,  0.5669, -0.9961,  ..., -0.6670,  0.7148, -0.5396]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "layer-14[torch.Size([3, 5120])]: 0\n",
      "tensor([[-0.0149, -0.3506,  0.0228,  ..., -0.1061,  0.5850, -0.3687],\n",
      "        [-0.0372, -0.2499, -0.0960,  ...,  0.0429,  0.1338,  0.0785],\n",
      "        [-0.8071,  1.3809, -1.0830,  ..., -0.5303,  0.7559, -0.8154]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "tensor([[-0.0149, -0.3506,  0.0228,  ..., -0.1061,  0.5850, -0.3687],\n",
      "        [-0.0372, -0.2499, -0.0960,  ...,  0.0429,  0.1338,  0.0785],\n",
      "        [-0.8071,  1.3809, -1.0830,  ..., -0.5303,  0.7559, -0.8154]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "layer-15[torch.Size([3, 5120])]: 0\n",
      "tensor([[ 0.0184, -0.3455, -0.0515,  ..., -0.1219,  0.6021, -0.4456],\n",
      "        [-0.3611, -0.3145,  0.0278,  ..., -0.0123,  0.3550,  0.2158],\n",
      "        [-1.1152,  1.6572, -1.1230,  ..., -0.8740,  0.9751, -0.6953]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "tensor([[ 0.0184, -0.3455, -0.0515,  ..., -0.1219,  0.6021, -0.4456],\n",
      "        [-0.3611, -0.3145,  0.0278,  ..., -0.0123,  0.3550,  0.2158],\n",
      "        [-1.1152,  1.6572, -1.1230,  ..., -0.8740,  0.9751, -0.6953]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "layer-16[torch.Size([3, 5120])]: 0\n",
      "tensor([[ 0.1819, -0.2749, -0.2396,  ..., -0.1981,  0.4624, -0.4736],\n",
      "        [-0.2563, -0.0314, -0.2178,  ..., -0.1804,  0.2852, -0.0255],\n",
      "        [-0.8066,  1.9668, -1.3613,  ..., -1.2881,  1.0918, -1.0938]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "tensor([[ 0.1819, -0.2749, -0.2396,  ..., -0.1981,  0.4624, -0.4736],\n",
      "        [-0.2563, -0.0314, -0.2178,  ..., -0.1804,  0.2852, -0.0255],\n",
      "        [-0.8066,  1.9668, -1.3613,  ..., -1.2881,  1.0918, -1.0938]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "layer-17[torch.Size([3, 5120])]: 0\n",
      "tensor([[ 0.1141, -0.2996, -0.2925,  ..., -0.2046,  0.4580, -0.4668],\n",
      "        [-0.3965, -0.4592, -0.2181,  ..., -0.0049,  0.6763,  0.1460],\n",
      "        [-0.5923,  2.0742, -0.9146,  ..., -1.7188,  1.3994, -0.9888]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "tensor([[ 0.1141, -0.2996, -0.2925,  ..., -0.2046,  0.4580, -0.4668],\n",
      "        [-0.3965, -0.4592, -0.2181,  ..., -0.0049,  0.6763,  0.1460],\n",
      "        [-0.5923,  2.0742, -0.9146,  ..., -1.7188,  1.3994, -0.9888]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "layer-18[torch.Size([3, 5120])]: 0\n",
      "tensor([[ 0.0684, -0.2524, -0.4053,  ..., -0.2410,  0.4937, -0.4895],\n",
      "        [-0.2252, -0.1245,  0.0635,  ...,  0.0062,  1.0049,  0.5259],\n",
      "        [-0.9160,  1.9199, -0.9224,  ..., -2.1035,  1.8271, -0.8721]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "tensor([[ 0.0684, -0.2524, -0.4053,  ..., -0.2410,  0.4937, -0.4895],\n",
      "        [-0.2252, -0.1245,  0.0635,  ...,  0.0062,  1.0049,  0.5259],\n",
      "        [-0.9160,  1.9199, -0.9224,  ..., -2.1035,  1.8271, -0.8721]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "layer-19[torch.Size([3, 5120])]: 0\n",
      "tensor([[ 0.0736, -0.2054, -0.4771,  ..., -0.3389,  0.5469, -0.4431],\n",
      "        [-0.4648, -0.1765,  0.4443,  ..., -0.0768,  1.4111,  0.8311],\n",
      "        [-0.9995,  1.5010, -1.0986,  ..., -2.3242,  2.1328, -0.5435]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "tensor([[ 0.0736, -0.2054, -0.4771,  ..., -0.3389,  0.5469, -0.4431],\n",
      "        [-0.4648, -0.1765,  0.4443,  ..., -0.0768,  1.4111,  0.8311],\n",
      "        [-0.9995,  1.5010, -1.0986,  ..., -2.3242,  2.1328, -0.5435]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "layer-20[torch.Size([3, 5120])]: 0\n",
      "tensor([[ 0.1175, -0.0459, -0.4741,  ..., -0.2151,  0.3032, -0.4094],\n",
      "        [-0.6797, -0.4482,  0.4839,  ..., -0.1597,  1.1250,  1.2314],\n",
      "        [-1.1201,  1.8018, -1.1045,  ..., -2.7930,  1.8418,  0.0707]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "tensor([[ 0.1175, -0.0459, -0.4741,  ..., -0.2151,  0.3032, -0.4094],\n",
      "        [-0.6797, -0.4482,  0.4839,  ..., -0.1597,  1.1250,  1.2314],\n",
      "        [-1.1201,  1.8018, -1.1045,  ..., -2.7930,  1.8418,  0.0707]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "layer-21[torch.Size([3, 5120])]: 0\n",
      "tensor([[ 0.0823,  0.0414, -0.4421,  ..., -0.2411,  0.2302, -0.3923],\n",
      "        [-0.5195, -0.4253,  0.6084,  ..., -0.3120,  1.3105,  1.4287],\n",
      "        [-1.2305,  1.7168, -0.8887,  ..., -3.0898,  1.3945,  0.4644]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "tensor([[ 0.0823,  0.0414, -0.4421,  ..., -0.2411,  0.2302, -0.3923],\n",
      "        [-0.5195, -0.4253,  0.6084,  ..., -0.3120,  1.3105,  1.4287],\n",
      "        [-1.2305,  1.7168, -0.8887,  ..., -3.0898,  1.3945,  0.4644]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "layer-22[torch.Size([3, 5120])]: 0\n",
      "tensor([[ 0.1151,  0.0778, -0.4497,  ..., -0.2783,  0.2971, -0.3442],\n",
      "        [-0.4292,  0.0034,  0.6670,  ..., -0.5020,  1.6377,  1.2266],\n",
      "        [-1.4629,  1.9268, -0.9385,  ..., -3.2051,  1.3174,  0.6963]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "tensor([[ 0.1151,  0.0778, -0.4497,  ..., -0.2783,  0.2971, -0.3442],\n",
      "        [-0.4292,  0.0034,  0.6670,  ..., -0.5020,  1.6377,  1.2266],\n",
      "        [-1.4629,  1.9268, -0.9385,  ..., -3.2051,  1.3174,  0.6963]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "layer-23[torch.Size([3, 5120])]: 0\n",
      "tensor([[ 0.1748,  0.1095, -0.4663,  ..., -0.3071,  0.3789, -0.3657],\n",
      "        [-0.5513, -0.3201,  0.4602,  ..., -0.4832,  1.8926,  2.1328],\n",
      "        [-1.4150,  1.8799, -0.6260,  ..., -3.4121,  1.5361,  1.3486]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "tensor([[ 0.1748,  0.1095, -0.4663,  ..., -0.3071,  0.3789, -0.3657],\n",
      "        [-0.5513, -0.3201,  0.4602,  ..., -0.4832,  1.8926,  2.1328],\n",
      "        [-1.4150,  1.8799, -0.6260,  ..., -3.4121,  1.5361,  1.3486]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "layer-24[torch.Size([3, 5120])]: 0\n",
      "tensor([[-0.0509, -0.1650, -0.4453,  ..., -0.2268,  0.2583, -0.4648],\n",
      "        [-1.0918, -0.2128,  0.2898,  ..., -0.1045,  2.1133,  2.3379],\n",
      "        [-1.7529,  1.3711, -0.7378,  ..., -3.3848,  1.6455,  1.2236]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "tensor([[-0.0509, -0.1650, -0.4453,  ..., -0.2268,  0.2583, -0.4648],\n",
      "        [-1.0918, -0.2128,  0.2898,  ..., -0.1045,  2.1133,  2.3379],\n",
      "        [-1.7529,  1.3711, -0.7378,  ..., -3.3848,  1.6455,  1.2236]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "layer-25[torch.Size([3, 5120])]: 0\n",
      "tensor([[-0.0502, -0.1537, -0.4917,  ..., -0.2622,  0.2952, -0.4578],\n",
      "        [-1.0049, -0.3953,  0.3999,  ...,  0.2751,  2.5547,  2.0977],\n",
      "        [-1.8525,  0.9395, -0.4714,  ..., -2.8691,  2.3281,  1.5098]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "tensor([[-0.0502, -0.1537, -0.4917,  ..., -0.2622,  0.2952, -0.4578],\n",
      "        [-1.0049, -0.3953,  0.3999,  ...,  0.2751,  2.5547,  2.0977],\n",
      "        [-1.8525,  0.9395, -0.4714,  ..., -2.8691,  2.3281,  1.5098]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "layer-26[torch.Size([3, 5120])]: 0\n",
      "tensor([[-0.0959, -0.1697, -0.5332,  ..., -0.3018,  0.3096, -0.4653],\n",
      "        [-0.8569, -0.0979,  0.2681,  ...,  0.3145,  2.6934,  2.1270],\n",
      "        [-1.8711,  1.0176, -0.1290,  ..., -2.8672,  2.1953,  1.6025]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "tensor([[-0.0959, -0.1697, -0.5332,  ..., -0.3018,  0.3096, -0.4653],\n",
      "        [-0.8569, -0.0979,  0.2681,  ...,  0.3145,  2.6934,  2.1270],\n",
      "        [-1.8711,  1.0176, -0.1290,  ..., -2.8672,  2.1953,  1.6025]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "layer-27[torch.Size([3, 5120])]: 0\n",
      "tensor([[-0.1153, -0.1334, -0.5439,  ..., -0.2722,  0.3015, -0.4849],\n",
      "        [-0.1465,  0.1025,  0.0195,  ...,  0.4204,  2.7227,  1.9258],\n",
      "        [-2.0176,  1.0605, -0.2131,  ..., -3.0195,  1.8232,  1.4219]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "tensor([[-0.1153, -0.1334, -0.5439,  ..., -0.2722,  0.3015, -0.4849],\n",
      "        [-0.1465,  0.1025,  0.0195,  ...,  0.4204,  2.7227,  1.9258],\n",
      "        [-2.0176,  1.0605, -0.2131,  ..., -3.0195,  1.8232,  1.4219]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "layer-28[torch.Size([3, 5120])]: 0\n",
      "tensor([[-0.1116, -0.0799, -0.5732,  ..., -0.2632,  0.2878, -0.5498],\n",
      "        [-0.1201,  0.3965,  0.3835,  ...,  0.1066,  2.0039,  2.2266],\n",
      "        [-1.8994,  1.3066, -0.5957,  ..., -3.1934,  1.8291,  1.7031]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "tensor([[-0.1116, -0.0799, -0.5732,  ..., -0.2632,  0.2878, -0.5498],\n",
      "        [-0.1201,  0.3965,  0.3835,  ...,  0.1066,  2.0039,  2.2266],\n",
      "        [-1.8994,  1.3066, -0.5957,  ..., -3.1934,  1.8291,  1.7031]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "layer-29[torch.Size([3, 5120])]: 0\n",
      "tensor([[-0.1371, -0.0852, -0.6318,  ..., -0.2573,  0.3381, -0.6157],\n",
      "        [-0.1990,  0.2240,  0.3242,  ...,  0.1163,  2.0898,  2.8242],\n",
      "        [-1.6699,  0.9526, -0.7026,  ..., -2.5273,  2.1758,  1.3672]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "tensor([[-0.1371, -0.0852, -0.6318,  ..., -0.2573,  0.3381, -0.6157],\n",
      "        [-0.1990,  0.2240,  0.3242,  ...,  0.1163,  2.0898,  2.8242],\n",
      "        [-1.6699,  0.9526, -0.7026,  ..., -2.5273,  2.1758,  1.3672]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "layer-30[torch.Size([3, 5120])]: 0\n",
      "tensor([[-0.1638, -0.1472, -0.6982,  ..., -0.2284,  0.3721, -0.7432],\n",
      "        [-0.2346,  0.4619,  0.6079,  ..., -0.0195,  2.5020,  3.0000],\n",
      "        [-1.5869,  0.7568, -0.5610,  ..., -2.2520,  2.0840,  1.8730]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "tensor([[-0.1638, -0.1472, -0.6982,  ..., -0.2284,  0.3721, -0.7432],\n",
      "        [-0.2346,  0.4619,  0.6079,  ..., -0.0195,  2.5020,  3.0000],\n",
      "        [-1.5869,  0.7568, -0.5610,  ..., -2.2520,  2.0840,  1.8730]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "layer-31[torch.Size([3, 5120])]: 0\n",
      "tensor([[-0.2114, -0.2028, -0.7739,  ..., -0.2046,  0.3179, -0.8696],\n",
      "        [-0.8848,  0.6333,  0.7129,  ..., -0.0242,  2.4980,  2.8750],\n",
      "        [-1.7793,  0.6982, -0.6982,  ..., -2.0215,  1.8242,  1.7734]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "tensor([[-0.2114, -0.2028, -0.7739,  ..., -0.2046,  0.3179, -0.8696],\n",
      "        [-0.8848,  0.6333,  0.7129,  ..., -0.0242,  2.4980,  2.8750],\n",
      "        [-1.7793,  0.6982, -0.6982,  ..., -2.0215,  1.8242,  1.7734]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "layer-32[torch.Size([3, 5120])]: 0\n",
      "tensor([[-0.3513, -0.2222, -0.7358,  ..., -0.2311,  0.3816, -0.9214],\n",
      "        [-0.4709,  0.3037,  0.2932,  ...,  0.2118,  2.2871,  2.6602],\n",
      "        [-1.8936,  0.8530, -0.8486,  ..., -1.4492,  2.1504,  1.6299]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "tensor([[-0.3513, -0.2222, -0.7358,  ..., -0.2311,  0.3816, -0.9214],\n",
      "        [-0.4709,  0.3037,  0.2932,  ...,  0.2118,  2.2871,  2.6602],\n",
      "        [-1.8936,  0.8530, -0.8486,  ..., -1.4492,  2.1504,  1.6299]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "layer-33[torch.Size([3, 5120])]: 0\n",
      "tensor([[-0.4482, -0.3130, -0.8057,  ..., -0.1896,  0.4507, -0.9492],\n",
      "        [-0.9482,  0.4556,  0.7900,  ...,  0.7773,  2.2949,  2.6934],\n",
      "        [-2.5234,  0.1912, -0.6021,  ..., -1.0947,  2.0957,  1.6914]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "tensor([[-0.4482, -0.3130, -0.8057,  ..., -0.1896,  0.4507, -0.9492],\n",
      "        [-0.9482,  0.4556,  0.7900,  ...,  0.7773,  2.2949,  2.6934],\n",
      "        [-2.5234,  0.1912, -0.6021,  ..., -1.0947,  2.0957,  1.6914]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "layer-34[torch.Size([3, 5120])]: 0\n",
      "tensor([[-0.4270, -0.3157, -0.8589,  ..., -0.2764,  0.5322, -1.0078],\n",
      "        [-0.5161,  0.7627,  0.5571,  ...,  0.4678,  2.8418,  2.5195],\n",
      "        [-2.4941,  0.3374, -0.7393,  ..., -1.3555,  2.1250,  1.7314]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "tensor([[-0.4270, -0.3157, -0.8589,  ..., -0.2764,  0.5322, -1.0078],\n",
      "        [-0.5161,  0.7627,  0.5571,  ...,  0.4678,  2.8418,  2.5195],\n",
      "        [-2.4941,  0.3374, -0.7393,  ..., -1.3555,  2.1250,  1.7314]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "layer-35[torch.Size([3, 5120])]: 0\n",
      "tensor([[-0.5488, -0.3398, -0.8423,  ..., -0.3821,  0.8486, -1.0723],\n",
      "        [-0.5493,  0.3992,  0.6313,  ...,  0.7617,  2.3770,  2.2168],\n",
      "        [-2.1211,  0.0466, -0.2307,  ..., -0.8604,  2.3867,  1.3359]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "tensor([[-0.5488, -0.3398, -0.8423,  ..., -0.3821,  0.8486, -1.0723],\n",
      "        [-0.5493,  0.3992,  0.6313,  ...,  0.7617,  2.3770,  2.2168],\n",
      "        [-2.1211,  0.0466, -0.2307,  ..., -0.8604,  2.3867,  1.3359]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "layer-36[torch.Size([3, 5120])]: 0\n",
      "tensor([[-0.8486, -0.1125, -0.5967,  ..., -0.6250,  1.4668, -0.8789],\n",
      "        [-0.7451,  0.2397, -0.0811,  ...,  0.6680,  2.0801,  1.4219],\n",
      "        [-2.5605,  0.7505, -0.6582,  ..., -0.7939,  1.1758,  0.8740]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "tensor([[-0.8486, -0.1125, -0.5967,  ..., -0.6250,  1.4668, -0.8789],\n",
      "        [-0.7451,  0.2397, -0.0811,  ...,  0.6680,  2.0801,  1.4219],\n",
      "        [-2.5605,  0.7505, -0.6582,  ..., -0.7939,  1.1758,  0.8740]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "layer-37[torch.Size([3, 5120])]: 0\n",
      "tensor([[-0.7881, -0.2822,  0.1543,  ..., -0.3171,  1.1396,  0.0889],\n",
      "        [-0.9390,  0.5303,  0.6118,  ...,  0.8276,  1.8105,  1.6650],\n",
      "        [-3.2422,  1.9131,  0.9258,  ..., -1.6973,  2.1914,  1.7793]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "tensor([[-0.7881, -0.2822,  0.1543,  ..., -0.3171,  1.1396,  0.0889],\n",
      "        [-0.9390,  0.5303,  0.6118,  ...,  0.8276,  1.8105,  1.6650],\n",
      "        [-3.2422,  1.9131,  0.9258,  ..., -1.6973,  2.1914,  1.7793]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "layer-38[torch.Size([3, 5120])]: 0\n",
      "tensor([[ 4.1250,  0.9663,  1.9316,  ..., -0.9258, -0.7300, -0.1501],\n",
      "        [-2.2266, -0.0830,  0.1483,  ...,  0.3879,  0.9087,  1.4297],\n",
      "        [-2.8848,  1.8428,  0.2793,  ..., -1.3838,  1.3496,  1.6914]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "tensor([[ 4.1250,  0.9663,  1.9316,  ..., -0.9258, -0.7300, -0.1501],\n",
      "        [-2.2266, -0.0830,  0.1483,  ...,  0.3879,  0.9087,  1.4297],\n",
      "        [-2.8848,  1.8428,  0.2793,  ..., -1.3838,  1.3496,  1.6914]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<SqueezeBackward0>)\n",
      "layer-39[torch.Size([3, 5120])]: 0\n"
     ]
    }
   ],
   "source": [
    "for i, (_hf_tensor, _mlm_tensor) in enumerate(zip(hf_tensors, mlm_tensors)):\n",
    "    _hf_tensor = _hf_tensor.squeeze()\n",
    "    _mlm_tensor = _mlm_tensor.squeeze()\n",
    "    diff = (_hf_tensor != _mlm_tensor).sum().item()\n",
    "    print(_hf_tensor)\n",
    "    print(_mlm_tensor)\n",
    "    print(f\"layer-{i}[{_hf_tensor.shape}]: {diff}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "127aeafc-f9be-4dc8-88c9-a137b5189a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_layernorm: 0\n",
      "q: tensor(0, device='cuda:0')\n",
      "k: tensor(0, device='cuda:0')\n",
      "q_after_rope: tensor(0, device='cuda:0')\n",
      "k_after_rope: tensor(0, device='cuda:0')\n",
      "attn_wts (because difference mask format): 120\n",
      "attn_softmax: 0\n",
      "attn_outputs: 0\n",
      "self_attn_outputs: 0\n"
     ]
    }
   ],
   "source": [
    "src = hf_in.squeeze()\n",
    "dst = mlm_in.squeeze()\n",
    "print(\"input_layernorm:\", (src != dst).sum().item())\n",
    "\n",
    "src = mlm_q.squeeze()\n",
    "dst = hf_q.squeeze().transpose(0, 1)\n",
    "print(\"q:\", (src != dst).sum())\n",
    "\n",
    "src = mlm_k.squeeze()\n",
    "dst = hf_k.squeeze().transpose(0, 1)\n",
    "print(\"k:\", (src != dst).sum())\n",
    "\n",
    "src = mlm_q_rp.squeeze()\n",
    "dst = hf_q_rp.squeeze().transpose(0, 1)\n",
    "print(\"q_after_rope:\", (src != dst).sum())\n",
    "\n",
    "src = mlm_k_rp.squeeze()\n",
    "dst = hf_k_rp.squeeze().transpose(0, 1)\n",
    "print(\"k_after_rope:\", (src != dst).sum())\n",
    "\n",
    "src = hf_attn_wi.squeeze()\n",
    "dst = mlm_attn_wi.squeeze()\n",
    "# print(src[0], dst[0])\n",
    "print(\"attn_wts (because difference mask format):\", (src != dst).sum().item())\n",
    "\n",
    "src = hf_attn_w.squeeze()\n",
    "dst = mlm_attn_w.squeeze()\n",
    "print(\"attn_softmax:\", (src != dst).sum().item())\n",
    "\n",
    "src = hf_attn_sm.squeeze()\n",
    "dst = mlm_attn_sm.squeeze()\n",
    "print(\"attn_outputs:\", (src != dst).sum().item())\n",
    "\n",
    "src = hf_ao.squeeze()\n",
    "dst = mlm_ao.squeeze()\n",
    "print(\"self_attn_outputs:\", (src != dst).sum().item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
