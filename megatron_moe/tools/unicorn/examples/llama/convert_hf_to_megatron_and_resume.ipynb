{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f03ebfce-0530-49b4-9545-f0dca9deaf04",
   "metadata": {},
   "source": [
    "### Resume training from Huggingface model\n",
    "Use Llama2-13B HF model as an example, \n",
    "1. Convert Llama2-13B HF model to Megatron for expected TP and PP size.\n",
    "2. Resume training from converted Megatron checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5db3359-179a-4a04-aa8b-f8b3c9038932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import argparse\n",
    "\n",
    "MEGATRON_ROOT = \"/cpfs/29ccba8f16c61395/data/user/liushan/projects/Megatron-LM-master/\"\n",
    "sys.path.insert(0, MEGATRON_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79f254b3-7bd5-40cb-a0c9-3744f5eb18c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-18 05:38:06,689] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "# import unicorn\n",
    "sys.path.append(os.path.join(MEGATRON_ROOT, \"tools\", \"unicorn\"))\n",
    "import unicorn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05af7ae8-38a7-43c3-bb03-27ce2f1034c0",
   "metadata": {},
   "source": [
    "#### Convert Huggingface model to Megatron checkpoint\n",
    "* You can also use shell in `tools/unicorn/examples/llama/convert_hf_to_megatron.sh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32eeaa77-ff0d-48b4-9873-8dcc54c5ee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser = unicorn.add_checkpointing_args(parser)\n",
    "    parser = unicorn.add_transformers_checkpoint_args(parser)\n",
    "    parser = unicorn.add_megatron_checkpoint_args(parser)\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "sys.argv = ['script.py',\n",
    "            '--megatron-path', MEGATRON_ROOT,\n",
    "            '--load-path', os.path.join(MEGATRON_ROOT, \"models\", \"Llama-2-13b-hf\"),\n",
    "            '--save-path', os.path.join(MEGATRON_ROOT, \"models\", \"llama-megatron\"),\n",
    "            '--model-name', 'llama2-13b',\n",
    "            '--template-name', 'llama',\n",
    "            '--print-checkpoint-structure',\n",
    "            '--target_tensor_model_parallel_size', '2',\n",
    "            '--target_pipeline_model_parallel_size', '2',\n",
    "            '--target_params_dtype', 'fp16']\n",
    "\n",
    "args = parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f91ce50-9262-4e0c-a9ac-42e3ca8f2cb4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Loading /cpfs/29ccba8f16c61395/data/user/liushan/projects/Megatron-LM-master/models/Llama-2-13b-hf/model-00001-of-00003.safetensors ...\n",
      "=> Loading /cpfs/29ccba8f16c61395/data/user/liushan/projects/Megatron-LM-master/models/Llama-2-13b-hf/model-00003-of-00003.safetensors ...\n",
      "=> Loading /cpfs/29ccba8f16c61395/data/user/liushan/projects/Megatron-LM-master/models/Llama-2-13b-hf/model-00002-of-00003.safetensors ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cp: cannot stat '/cpfs/29ccba8f16c61395/data/user/liushan/projects/Megatron-LM-master/models/Llama-2-13b-hf/*.tiktoken': No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Converting ...\n",
      "=> converting embedding layer ...\n",
      "=> Converting transformer blocks ...\n",
      "Checkpoint structure of model state dict shard belonging to TP rank 0 and PP rank 0:\n",
      "# model                                           \n",
      "..# language_model                                \n",
      "....# embedding                                   \n",
      "......# word_embeddings                           \n",
      "........# weight                                   : torch.Size([16000, 5120])\n",
      "....# output_layer                                \n",
      "......# weight                                     : torch.Size([16000, 5120])\n",
      "....# encoder                                     \n",
      "......# layers.0.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.0.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.0.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.0.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.0.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.0.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.1.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.1.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.1.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.1.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.1.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.1.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.2.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.2.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.2.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.2.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.2.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.2.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.3.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.3.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.3.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.3.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.3.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.3.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.4.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.4.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.4.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.4.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.4.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.4.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.5.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.5.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.5.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.5.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.5.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.5.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.6.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.6.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.6.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.6.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.6.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.6.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.7.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.7.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.7.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.7.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.7.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.7.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.8.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.8.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.8.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.8.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.8.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.8.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.9.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.9.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.9.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.9.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.9.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.9.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.10.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.10.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.10.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.10.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.10.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.10.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.11.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.11.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.11.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.11.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.11.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.11.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.12.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.12.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.12.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.12.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.12.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.12.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.13.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.13.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.13.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.13.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.13.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.13.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.14.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.14.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.14.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.14.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.14.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.14.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.15.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.15.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.15.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.15.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.15.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.15.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.16.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.16.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.16.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.16.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.16.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.16.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.17.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.17.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.17.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.17.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.17.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.17.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.18.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.18.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.18.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.18.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.18.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.18.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.19.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.19.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.19.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.19.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.19.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.19.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "# checkpoint_version                               : 3.0\n",
      "# args                                             : namespace(orig_vocab_size=32000, hidden_size=5120, ffn_hidden_size=13824, num_layers=40, num_attention_heads=40, kv_channels=128, norm_epsilon=1e-05, init_method_std=0.02, seq_length=4096, untie_embeddings_and_output_weights=True, padded_vocab_size=32000, position_embedding_type='rope', normalization='RMSNorm', swiglu=True, max_position_embeddings=4096, tensor_model_parallel_size=2, pipeline_model_parallel_size=2, data_parallel_size=1, make_vocab_size_divisible_by=128, rank=0, tokenizer_type='NullTokenizer', float16=True, params_dtype=torch.float16)\n",
      "Checkpoint structure of model state dict shard belonging to TP rank 1 and PP rank 0:\n",
      "# model                                           \n",
      "..# language_model                                \n",
      "....# embedding                                   \n",
      "......# word_embeddings                           \n",
      "........# weight                                   : torch.Size([16000, 5120])\n",
      "....# output_layer                                \n",
      "......# weight                                     : torch.Size([16000, 5120])\n",
      "....# encoder                                     \n",
      "......# layers.0.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.0.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.0.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.0.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.0.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.0.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.1.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.1.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.1.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.1.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.1.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.1.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.2.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.2.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.2.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.2.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.2.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.2.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.3.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.3.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.3.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.3.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.3.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.3.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.4.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.4.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.4.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.4.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.4.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.4.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.5.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.5.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.5.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.5.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.5.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.5.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.6.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.6.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.6.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.6.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.6.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.6.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.7.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.7.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.7.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.7.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.7.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.7.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.8.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.8.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.8.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.8.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.8.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.8.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.9.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.9.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.9.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.9.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.9.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.9.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.10.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.10.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.10.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.10.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.10.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.10.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.11.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.11.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.11.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.11.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.11.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.11.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.12.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.12.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.12.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.12.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.12.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.12.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.13.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.13.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.13.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.13.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.13.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.13.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.14.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.14.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.14.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.14.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.14.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.14.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.15.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.15.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.15.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.15.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.15.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.15.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.16.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.16.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.16.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.16.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.16.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.16.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.17.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.17.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.17.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.17.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.17.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.17.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.18.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.18.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.18.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.18.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.18.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.18.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.19.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.19.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.19.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.19.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.19.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.19.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "# checkpoint_version                               : 3.0\n",
      "# args                                             : namespace(orig_vocab_size=32000, hidden_size=5120, ffn_hidden_size=13824, num_layers=40, num_attention_heads=40, kv_channels=128, norm_epsilon=1e-05, init_method_std=0.02, seq_length=4096, untie_embeddings_and_output_weights=True, padded_vocab_size=32000, position_embedding_type='rope', normalization='RMSNorm', swiglu=True, max_position_embeddings=4096, tensor_model_parallel_size=2, pipeline_model_parallel_size=2, data_parallel_size=1, make_vocab_size_divisible_by=128, rank=0, tokenizer_type='NullTokenizer', float16=True, params_dtype=torch.float16)\n",
      "Checkpoint structure of model state dict shard belonging to TP rank 0 and PP rank 1:\n",
      "# model                                           \n",
      "..# language_model                                \n",
      "....# encoder                                     \n",
      "......# layers.0.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.0.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.0.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.0.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.0.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.0.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.1.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.1.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.1.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.1.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.1.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.1.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.2.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.2.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.2.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.2.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.2.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.2.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.3.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.3.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.3.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.3.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.3.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.3.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.4.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.4.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.4.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.4.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.4.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.4.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.5.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.5.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.5.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.5.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.5.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.5.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.6.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.6.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.6.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.6.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.6.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.6.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.7.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.7.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.7.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.7.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.7.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.7.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.8.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.8.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.8.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.8.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.8.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.8.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.9.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.9.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.9.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.9.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.9.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.9.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.10.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.10.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.10.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.10.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.10.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.10.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.11.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.11.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.11.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.11.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.11.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.11.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.12.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.12.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.12.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.12.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.12.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.12.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.13.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.13.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.13.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.13.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.13.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.13.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.14.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.14.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.14.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.14.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.14.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.14.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.15.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.15.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.15.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.15.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.15.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.15.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.16.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.16.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.16.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.16.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.16.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.16.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.17.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.17.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.17.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.17.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.17.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.17.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.18.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.18.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.18.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.18.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.18.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.18.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.19.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.19.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.19.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.19.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.19.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.19.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# final_norm.weight                          : torch.Size([5120])\n",
      "....# output_layer                                \n",
      "......# weight                                     : torch.Size([16000, 5120])\n",
      "..# word_embeddings_for_head                      \n",
      "....# weight                                       : torch.Size([16000, 5120])\n",
      "# checkpoint_version                               : 3.0\n",
      "# args                                             : namespace(orig_vocab_size=32000, hidden_size=5120, ffn_hidden_size=13824, num_layers=40, num_attention_heads=40, kv_channels=128, norm_epsilon=1e-05, init_method_std=0.02, seq_length=4096, untie_embeddings_and_output_weights=True, padded_vocab_size=32000, position_embedding_type='rope', normalization='RMSNorm', swiglu=True, max_position_embeddings=4096, tensor_model_parallel_size=2, pipeline_model_parallel_size=2, data_parallel_size=1, make_vocab_size_divisible_by=128, rank=0, tokenizer_type='NullTokenizer', float16=True, params_dtype=torch.float16)\n",
      "Checkpoint structure of model state dict shard belonging to TP rank 1 and PP rank 1:\n",
      "# model                                           \n",
      "..# language_model                                \n",
      "....# encoder                                     \n",
      "......# layers.0.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.0.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.0.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.0.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.0.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.0.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.1.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.1.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.1.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.1.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.1.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.1.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.2.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.2.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.2.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.2.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.2.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.2.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.3.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.3.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.3.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.3.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.3.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.3.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.4.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.4.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.4.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.4.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.4.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.4.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.5.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.5.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.5.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.5.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.5.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.5.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.6.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.6.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.6.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.6.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.6.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.6.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.7.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.7.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.7.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.7.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.7.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.7.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.8.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.8.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.8.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.8.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.8.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.8.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.9.input_norm.weight                 : torch.Size([5120])\n",
      "......# layers.9.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.9.self_attention.dense.weight       : torch.Size([5120, 2560])\n",
      "......# layers.9.post_attention_norm.weight        : torch.Size([5120])\n",
      "......# layers.9.mlp.dense_h_to_4h.weight          : torch.Size([13824, 5120])\n",
      "......# layers.9.mlp.dense_4h_to_h.weight          : torch.Size([5120, 6912])\n",
      "......# layers.10.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.10.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.10.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.10.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.10.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.10.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.11.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.11.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.11.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.11.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.11.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.11.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.12.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.12.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.12.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.12.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.12.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.12.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.13.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.13.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.13.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.13.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.13.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.13.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.14.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.14.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.14.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.14.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.14.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.14.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.15.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.15.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.15.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.15.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.15.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.15.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.16.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.16.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.16.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.16.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.16.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.16.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.17.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.17.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.17.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.17.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.17.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.17.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.18.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.18.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.18.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.18.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.18.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.18.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# layers.19.input_norm.weight                : torch.Size([5120])\n",
      "......# layers.19.self_attention.query_key_value.weight : torch.Size([7680, 5120])\n",
      "......# layers.19.self_attention.dense.weight      : torch.Size([5120, 2560])\n",
      "......# layers.19.post_attention_norm.weight       : torch.Size([5120])\n",
      "......# layers.19.mlp.dense_h_to_4h.weight         : torch.Size([13824, 5120])\n",
      "......# layers.19.mlp.dense_4h_to_h.weight         : torch.Size([5120, 6912])\n",
      "......# final_norm.weight                          : torch.Size([5120])\n",
      "....# output_layer                                \n",
      "......# weight                                     : torch.Size([16000, 5120])\n",
      "..# word_embeddings_for_head                      \n",
      "....# weight                                       : torch.Size([16000, 5120])\n",
      "# checkpoint_version                               : 3.0\n",
      "# args                                             : namespace(orig_vocab_size=32000, hidden_size=5120, ffn_hidden_size=13824, num_layers=40, num_attention_heads=40, kv_channels=128, norm_epsilon=1e-05, init_method_std=0.02, seq_length=4096, untie_embeddings_and_output_weights=True, padded_vocab_size=32000, position_embedding_type='rope', normalization='RMSNorm', swiglu=True, max_position_embeddings=4096, tensor_model_parallel_size=2, pipeline_model_parallel_size=2, data_parallel_size=1, make_vocab_size_divisible_by=128, rank=0, tokenizer_type='NullTokenizer', float16=True, params_dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "unicorn.convert_checkpoint_from_transformers_to_megatron(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83d93bb5-e965-4144-823c-78bbc0e7da54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json\t\t\t   special_tokens_map.json\n",
      "latest_checkpointed_iteration.txt  tokenizer.json\n",
      "model.safetensors.index.json\t   tokenizer_config.json\n",
      "release\n"
     ]
    }
   ],
   "source": [
    "target_path = os.path.join(MEGATRON_ROOT, \"models\", \"llama-megatron\")\n",
    "!ls {target_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9485a3-01ae-4913-832f-f7ab33631b37",
   "metadata": {},
   "source": [
    "#### Launch a distributed task loading the Megatron checkpoint\n",
    "* You can also check the following shells\n",
    "    * `tools/unicorn/examples/llama/prepare_data.sh`\n",
    "    * `tools/unicorn/examples/llama/run_examples.sh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bfcc7ad-cd7a-4ef1-b454-6db9a0a39056",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "+ PYTHONPATH=/cpfs/29ccba8f16c61395/data/user/liushan/projects/Megatron-LM-master/ python /cpfs/29ccba8f16c61395/data/user/liushan/projects/Megatron-LM-master//tools/preprocess_data.py --input /cpfs/29ccba8f16c61395/data/user/liushan/projects/Megatron-LM-master//tests/unicorn/data/sample.jsonl --json-keys text --tokenizer-type PretrainedFromHF --tokenizer-name-or-path /cpfs/29ccba8f16c61395/data/user/liushan/projects/Megatron-LM-master/models/Llama-2-13b-hf --append-eod --output-prefix /cpfs/29ccba8f16c61395/data/user/liushan/projects/Megatron-LM-master//tests/unicorn/data/sample_llama --workers 4\n",
      "Zarr-based strategies will not be registered because of missing packages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening /cpfs/29ccba8f16c61395/data/user/liushan/projects/Megatron-LM-master//tests/unicorn/data/sample.jsonl\n",
      "Time to startup: 0.24666452407836914\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare data\n",
    "shell = \\\n",
    "\"\"\"set -x\n",
    "\n",
    "PYTHONPATH={0} python {0}/tools/preprocess_data.py \\\n",
    "  --input {0}/tests/unicorn/data/sample.jsonl \\\n",
    "  --json-keys text \\\n",
    "  --tokenizer-type PretrainedFromHF \\\n",
    "  --tokenizer-name-or-path {1} \\\n",
    "  --append-eod \\\n",
    "  --output-prefix {0}/tests/unicorn/data/sample_llama \\\n",
    "  --workers 4\n",
    "\n",
    "\"\"\".format(MEGATRON_ROOT, os.path.join(MEGATRON_ROOT, \"models\", \"Llama-2-13b-hf\"))\n",
    "# print(shell)\n",
    "os.system(shell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bc57f5a-f738-43d2-8291-316f8076765b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python -m torch.distributed.launch --nproc_per_node 8 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 12345 /cpfs/29ccba8f16c61395/data/user/liushan/projects/Megatron-LM-master//pretrain_gpt.py --save /cpfs/29ccba8f16c61395/data/user/liushan/projects/Megatron-LM-master//test/checkpoint/ --load /cpfs/29ccba8f16c61395/data/user/liushan/projects/Megatron-LM-master/models/llama-megatron --split 99.5,0.5,0 --data-path /cpfs/29ccba8f16c61395/data/user/liushan/projects/Megatron-LM-master//tests/unicorn/data//sample_llama_text_document --lr 3e-4 --min-lr 3e-5 --lr-decay-style cosine --adam-beta1 0.9 --adam-beta2 0.95 --adam-eps 1e-5 --weight-decay 0.1 --clip-grad 1.0 --lr-decay-iters 50 --lr-warmup-iters 10 --train-iters 50 --micro-batch-size 1 --global-batch-size 128 --num-layers 40 --hidden-size 5120 --num-attention-heads 40 --seq-length 1024 --max-position-embeddings 1024 --attention-dropout 0.0 --hidden-dropout 0.0 --log-interval 1 --eval-interval 1000 --eval-iters 50 --save-interval 1000 --tensor-model-parallel-size 2 --pipeline-model-parallel-size 2 --num-workers 8 --seed 888 --tokenizer-type NullTokenizer --vocab-size 31999 --disable-bias-linear --swiglu --untie-embeddings-and-output-weights --swiglu-make-ffn-hidden-size-divisible-by 256 --position-embedding-type rope --normalization RMSNorm --norm-epsilon 1e-5 --init-method-std 0.02 --disable-scaled-init-method --use-distributed-optimizer --fp16 --initial-loss-scale 65536 --use-flash-attn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use-env is set by default in torchrun.\n",
      "If your script expects `--local-rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  warnings.warn(\n",
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "Zarr-based strategies will not be registered because of missing packages\n",
      "Zarr-based strategies will not be registered because of missing packages\n",
      "Zarr-based strategies will not be registered because of missing packages\n",
      "Zarr-based strategies will not be registered because of missing packages\n",
      "Zarr-based strategies will not be registered because of missing packages\n",
      "Zarr-based strategies will not be registered because of missing packages\n",
      "Zarr-based strategies will not be registered because of missing packages\n",
      "Zarr-based strategies will not be registered because of missing packages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using world size: 8, data-parallel-size: 2, tensor-model-parallel size: 2, pipeline-model-parallel size: 2 \n",
      "WARNING: overriding default arguments for tokenizer_type:GPT2BPETokenizer                        with tokenizer_type:NullTokenizer\n",
      "using torch.float16 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.95\n",
      "  adam_eps ........................................ 1e-05\n",
      "  add_bias_linear ................................. False\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_layernorm_1p .............................. False\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  async_tensor_model_parallel_allreduce ........... True\n",
      "  attention_dropout ............................... 0.0\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  barrier_with_L1_time ............................ True\n",
      "  bert_binary_head ................................ True\n",
      "  bert_embedder_type .............................. megatron\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_attn_linear ................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ False\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  check_for_nan_in_loss_and_grad .................. True\n",
      "  classes_fraction ................................ 1.0\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_cache_path ................................. None\n",
      "  data_parallel_random_init ....................... False\n",
      "  data_parallel_size .............................. 2\n",
      "  data_path ....................................... ['/cpfs/29ccba8f16c61395/data/user/liushan/projects/Megatron-LM-master//tests/unicorn/data//sample_llama_text_document']\n",
      "  data_per_class_fraction ......................... 1.0\n",
      "  data_sharding ................................... True\n",
      "  dataloader_type ................................. single\n",
      "  decoder_num_layers .............................. None\n",
      "  decoder_seq_length .............................. None\n",
      "  delay_grad_reduce ............................... True\n",
      "  dino_bottleneck_size ............................ 256\n",
      "  dino_freeze_last_layer .......................... 1\n",
      "  dino_head_hidden_size ........................... 2048\n",
      "  dino_local_crops_number ......................... 10\n",
      "  dino_local_img_size ............................. 96\n",
      "  dino_norm_last_layer ............................ False\n",
      "  dino_teacher_temp ............................... 0.07\n",
      "  dino_warmup_teacher_temp ........................ 0.04\n",
      "  dino_warmup_teacher_temp_epochs ................. 30\n",
      "  disable_scaled_init_method ...................... True\n",
      "  distribute_saved_activations .................... False\n",
      "  distributed_backend ............................. nccl\n",
      "  distributed_timeout_minutes ..................... 10\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  encoder_num_layers .............................. 40\n",
      "  encoder_seq_length .............................. 1024\n",
      "  end_weight_decay ................................ 0.1\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 50\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... None\n",
      "  exit_on_missing_checkpoint ...................... False\n",
      "  exit_signal_handler ............................. False\n",
      "  expert_model_parallel_size ...................... 1\n",
      "  expert_parallel ................................. False\n",
      "  ffn_hidden_size ................................. 13824\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ True\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  fp8 ............................................. None\n",
      "  fp8_amax_compute_algo ........................... most_recent\n",
      "  fp8_amax_history_len ............................ 1\n",
      "  fp8_interval .................................... 1\n",
      "  fp8_margin ...................................... 0\n",
      "  fp8_wgrad ....................................... True\n",
      "  global_batch_size ............................... 128\n",
      "  gradient_accumulation_fusion .................... True\n",
      "  group_query_attention ........................... False\n",
      "  head_lr_mult .................................... 1.0\n",
      "  hidden_dropout .................................. 0.0\n",
      "  hidden_size ..................................... 5120\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_h ........................................... 224\n",
      "  img_w ........................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  inference_batch_times_seqlen_threshold .......... 512\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 65536.0\n",
      "  iter_per_epoch .................................. 1250\n",
      "  kv_channels ..................................... 128\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ /cpfs/29ccba8f16c61395/data/user/liushan/projects/Megatron-LM-master/models/llama-megatron\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 1\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  log_world_size_to_tensorboard ................... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.0003\n",
      "  lr_decay_iters .................................. 50\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. None\n",
      "  lr_warmup_init .................................. 0.0\n",
      "  lr_warmup_iters ................................. 10\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_factor ..................................... 1.0\n",
      "  mask_prob ....................................... 0.15\n",
      "  mask_type ....................................... random\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 1024\n",
      "  max_tokens_to_oom ............................... 12000\n",
      "  merge_file ...................................... None\n",
      "  micro_batch_size ................................ 1\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 3e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  model_spec ...................................... None\n",
      "  no_load_lr_scheduler ............................ None\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_persist_layer_norm ........................... False\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  no_shuffle_dataset .............................. None\n",
      "  norm_epsilon .................................... 1e-05\n",
      "  normalization ................................... RMSNorm\n",
      "  num_attention_heads ............................. 40\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_experts ..................................... None\n",
      "  num_layers ...................................... 40\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_query_groups ................................ 1\n",
      "  num_workers ..................................... 8\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  output_bert_embeddings .......................... False\n",
      "  overlap_grad_reduce ............................. False\n",
      "  overlap_p2p_comm ................................ False\n",
      "  override_opt_param_scheduler .................... False\n",
      "  params_dtype .................................... torch.float16\n",
      "  patch_dim ....................................... 16\n",
      "  perform_initialization .......................... True\n",
      "  pipeline_model_parallel_size .................... 2\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  position_embedding_type ......................... rope\n",
      "  profile ......................................... False\n",
      "  profile_ranks ................................... [0]\n",
      "  profile_step_end ................................ 12\n",
      "  profile_step_start .............................. 10\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  recompute_granularity ........................... None\n",
      "  recompute_method ................................ None\n",
      "  recompute_num_layers ............................ None\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  reset_sample_and_iteration_stat ................. None\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  retro_add_retriever ............................. False\n",
      "  retro_cyclic_train_iters ........................ None\n",
      "  retro_encoder_attention_dropout ................. 0.1\n",
      "  retro_encoder_hidden_dropout .................... 0.1\n",
      "  retro_encoder_layers ............................ 2\n",
      "  retro_num_neighbors ............................. 2\n",
      "  retro_num_retrieved_chunks ...................... 2\n",
      "  retro_return_doc_ids ............................ False\n",
      "  retro_workdir ................................... None\n",
      "  rotary_percent .................................. 1.0\n",
      "  rotary_seq_len_interpolation_factor ............. None\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ /cpfs/29ccba8f16c61395/data/user/liushan/projects/Megatron-LM-master//test/checkpoint/\n",
      "  save_interval ................................... 1000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 888\n",
      "  seq_length ...................................... 1024\n",
      "  sequence_parallel ............................... False\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  skip_train ...................................... False\n",
      "  split ........................................... 99.5,0.5,0\n",
      "  squared_relu .................................... False\n",
      "  standalone_embedding_stage ...................... False\n",
      "  start_weight_decay .............................. 0.1\n",
      "  swiglu .......................................... True\n",
      "  swiglu_make_ffn_hidden_size_divisible_by ........ 256\n",
      "  swin_backbone_type .............................. tiny\n",
      "  tensor_model_parallel_size ...................... 2\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  test_data_path .................................. None\n",
      "  timing_log_level ................................ 0\n",
      "  timing_log_option ............................... minmax\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_model ................................. None\n",
      "  tokenizer_name_or_path .......................... None\n",
      "  tokenizer_type .................................. NullTokenizer\n",
      "  train_data_path ................................. None\n",
      "  train_iters ..................................... 50\n",
      "  train_samples ................................... None\n",
      "  transformer_impl ................................ local\n",
      "  transformer_pipeline_model_parallel_size ........ 2\n",
      "  untie_embeddings_and_output_weights ............. True\n",
      "  use_checkpoint_args ............................. False\n",
      "  use_checkpoint_opt_param_scheduler .............. False\n",
      "  use_cpu_initialization .......................... None\n",
      "  use_distributed_optimizer ....................... True\n",
      "  use_flash_attn .................................. True\n",
      "  use_one_sent_docs ............................... False\n",
      "  use_ring_exchange_p2p ........................... False\n",
      "  valid_data_path ................................. None\n",
      "  variable_seq_lengths ............................ False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vision_backbone_type ............................ vit\n",
      "  vision_pretraining .............................. False\n",
      "  vision_pretraining_type ......................... classify\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... None\n",
      "  vocab_size ...................................... 31999\n",
      "  weight_decay .................................... 0.1\n",
      "  weight_decay_incr_style ......................... constant\n",
      "  world_size ...................................... 8\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 64\n",
      "> building NullTokenizer tokenizer ...\n",
      " > padded vocab (size: 32000) with 0 dummy tokens (new size: 32000)\n",
      "> initializing torch distributed ...\n",
      "> initialized tensor model parallel with size 2\n",
      "> initialized pipeline model parallel with size 2\n",
      "> setting random seeds to 888 ...\n",
      "> compiling dataset index builder ...\n",
      "make: Entering directory '/cpfs/29ccba8f16c61395/data/user/liushan/projects/Megatron-LM-master/megatron/data'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/cpfs/29ccba8f16c61395/data/user/liushan/projects/Megatron-LM-master/megatron/data'\n",
      ">>> done with dataset index builder. Compilation time: 0.027 seconds\n",
      "> compiling and loading fused kernels ...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 6.344 seconds\n",
      "time to initialize megatron (seconds): 10.551\n",
      "[after megatron is initialized] datetime: 2023-10-18 05:44:15 \n",
      "building GPT model ...\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 3254067200\n",
      "> buckets for gradient all-reduce / reduce-scatter:\n",
      "    params for bucket 1\n",
      "      module.language_model.encoder.layers.16.input_norm.weight\n",
      "      module.language_model.encoder.layers.10.self_attention.query_key_value.weight\n",
      "      module.language_model.encoder.layers.4.mlp.dense_h_to_4h.weight\n",
      "      module.language_model.encoder.layers.18.self_attention.query_key_value.weight\n",
      "      module.language_model.encoder.layers.12.mlp.dense_4h_to_h.weight\n",
      "      module.language_model.encoder.layers.8.input_norm.weight\n",
      "      module.language_model.encoder.layers.2.post_attention_norm.weight\n",
      "      module.language_model.encoder.layers.17.self_attention.dense.weight\n",
      "      module.language_model.encoder.layers.12.post_attention_norm.weight\n",
      "      module.language_model.encoder.layers.6.mlp.dense_h_to_4h.weight\n",
      "      module.language_model.encoder.layers.1.mlp.dense_4h_to_h.weight\n",
      "      module.language_model.encoder.layers.14.mlp.dense_h_to_4h.weight\n",
      "      module.language_model.encoder.layers.9.self_attention.dense.weight\n",
      "      module.language_model.encoder.layers.5.input_norm.weight\n",
      "      module.language_model.encoder.layers.19.mlp.dense_4h_to_h.weight\n",
      "      module.language_model.encoder.layers.15.input_norm.weight\n",
      "      module.language_model.encoder.layers.9.self_attention.query_key_value.weight\n",
      "      module.language_model.encoder.layers.3.mlp.dense_h_to_4h.weight\n",
      "      module.language_model.encoder.layers.0.self_attention.query_key_value.weight\n",
      "      module.language_model.encoder.layers.17.self_attention.query_key_value.weight\n",
      "      module.language_model.encoder.layers.11.mlp.dense_4h_to_h.weight\n",
      "      module.language_model.encoder.layers.6.self_attention.dense.weight\n",
      "      module.language_model.encoder.layers.1.post_attention_norm.weight\n",
      "      module.language_model.encoder.layers.16.self_attention.dense.weight\n",
      "      module.language_model.encoder.layers.11.post_attention_norm.weight\n",
      "      module.language_model.encoder.layers.6.self_attention.query_key_value.weight\n",
      "      module.language_model.encoder.layers.0.self_attention.dense.weight\n",
      "      module.language_model.encoder.layers.19.post_attention_norm.weight\n",
      "      module.language_model.encoder.layers.13.mlp.dense_h_to_4h.weight\n",
      "      module.language_model.encoder.layers.8.self_attention.dense.weight\n",
      "      module.language_model.encoder.layers.4.input_norm.weight\n",
      "      module.language_model.encoder.layers.18.mlp.dense_4h_to_h.weight\n",
      "      module.language_model.encoder.layers.14.input_norm.weight\n",
      "      module.language_model.encoder.layers.8.self_attention.query_key_value.weight\n",
      "      module.language_model.encoder.layers.2.mlp.dense_h_to_4h.weight\n",
      "      module.language_model.encoder.layers.16.self_attention.query_key_value.weight\n",
      "      module.language_model.encoder.layers.10.mlp.dense_4h_to_h.weight\n",
      "      module.language_model.encoder.layers.5.self_attention.dense.weight\n",
      "      module.language_model.encoder.layers.15.self_attention.dense.weight\n",
      "      module.language_model.encoder.layers.10.post_attention_norm.weight\n",
      "      module.language_model.encoder.layers.5.self_attention.query_key_value.weight\n",
      "      module.language_model.encoder.layers.18.post_attention_norm.weight\n",
      "      module.language_model.encoder.layers.12.mlp.dense_h_to_4h.weight\n",
      "      module.language_model.encoder.layers.7.self_attention.dense.weight\n",
      "      module.language_model.encoder.layers.3.input_norm.weight\n",
      "      module.language_model.encoder.layers.17.mlp.dense_4h_to_h.weight\n",
      "      module.language_model.encoder.layers.13.input_norm.weight\n",
      "      module.language_model.encoder.layers.7.self_attention.query_key_value.weight\n",
      "      module.language_model.encoder.layers.1.mlp.dense_h_to_4h.weight\n",
      "      module.language_model.encoder.layers.15.self_attention.query_key_value.weight\n",
      "      module.language_model.encoder.layers.9.mlp.dense_4h_to_h.weight\n",
      "      module.language_model.encoder.layers.4.self_attention.dense.weight\n",
      "      module.language_model.encoder.layers.19.mlp.dense_h_to_4h.weight\n",
      "      module.language_model.encoder.layers.14.self_attention.dense.weight\n",
      "      module.language_model.encoder.layers.9.post_attention_norm.weight\n",
      "      module.language_model.encoder.layers.4.self_attention.query_key_value.weight\n",
      "      module.language_model.encoder.layers.17.post_attention_norm.weight\n",
      "      module.language_model.encoder.layers.11.mlp.dense_h_to_4h.weight\n",
      "      module.language_model.encoder.layers.6.mlp.dense_4h_to_h.weight\n",
      "      module.language_model.encoder.layers.2.input_norm.weight\n",
      "      module.language_model.encoder.layers.0.post_attention_norm.weight\n",
      "      module.language_model.encoder.layers.16.mlp.dense_4h_to_h.weight\n",
      "      module.language_model.encoder.layers.12.input_norm.weight\n",
      "      module.language_model.encoder.layers.6.post_attention_norm.weight\n",
      "      module.language_model.encoder.layers.1.input_norm.weight\n",
      "      module.language_model.encoder.layers.14.self_attention.query_key_value.weight\n",
      "      module.language_model.encoder.layers.8.mlp.dense_4h_to_h.weight\n",
      "      module.language_model.encoder.layers.3.self_attention.dense.weight\n",
      "      module.language_model.encoder.layers.18.mlp.dense_h_to_4h.weight\n",
      "      module.language_model.encoder.layers.13.self_attention.dense.weight\n",
      "      module.language_model.encoder.layers.8.post_attention_norm.weight\n",
      "      module.language_model.encoder.layers.3.self_attention.query_key_value.weight\n",
      "      module.language_model.encoder.layers.16.post_attention_norm.weight\n",
      "      module.language_model.encoder.layers.10.mlp.dense_h_to_4h.weight\n",
      "      module.language_model.encoder.layers.5.mlp.dense_4h_to_h.weight\n",
      "      module.language_model.encoder.layers.0.mlp.dense_4h_to_h.weight\n",
      "      module.language_model.encoder.layers.15.mlp.dense_4h_to_h.weight\n",
      "      module.language_model.encoder.layers.11.input_norm.weight\n",
      "      module.language_model.encoder.layers.5.post_attention_norm.weight\n",
      "      module.language_model.encoder.layers.19.input_norm.weight\n",
      "      module.language_model.encoder.layers.13.self_attention.query_key_value.weight\n",
      "      module.language_model.encoder.layers.7.mlp.dense_4h_to_h.weight\n",
      "      module.language_model.encoder.layers.2.self_attention.dense.weight\n",
      "      module.language_model.encoder.layers.17.mlp.dense_h_to_4h.weight\n",
      "      module.language_model.encoder.layers.12.self_attention.dense.weight\n",
      "      module.language_model.encoder.layers.7.post_attention_norm.weight\n",
      "      module.language_model.encoder.layers.2.self_attention.query_key_value.weight\n",
      "      module.language_model.encoder.layers.15.post_attention_norm.weight\n",
      "      module.language_model.encoder.layers.9.mlp.dense_h_to_4h.weight\n",
      "      module.language_model.encoder.layers.4.mlp.dense_4h_to_h.weight\n",
      "      module.language_model.encoder.layers.14.mlp.dense_4h_to_h.weight\n",
      "      module.language_model.encoder.layers.10.input_norm.weight\n",
      "      module.language_model.encoder.layers.4.post_attention_norm.weight\n",
      "      module.language_model.encoder.layers.18.input_norm.weight\n",
      "      module.language_model.encoder.layers.12.self_attention.query_key_value.weight\n",
      "      module.language_model.encoder.layers.1.self_attention.dense.weight\n",
      "      module.language_model.encoder.layers.0.input_norm.weight\n",
      "      module.language_model.encoder.layers.16.mlp.dense_h_to_4h.weight\n",
      "      module.language_model.encoder.layers.11.self_attention.dense.weight\n",
      "      module.language_model.encoder.layers.7.input_norm.weight\n",
      "      module.language_model.encoder.layers.1.self_attention.query_key_value.weight\n",
      "      module.language_model.encoder.layers.19.self_attention.dense.weight\n",
      "      module.language_model.encoder.layers.14.post_attention_norm.weight\n",
      "      module.language_model.encoder.layers.8.mlp.dense_h_to_4h.weight\n",
      "      module.language_model.encoder.layers.3.mlp.dense_4h_to_h.weight\n",
      "      module.language_model.encoder.layers.19.self_attention.query_key_value.weight\n",
      "      module.language_model.encoder.layers.13.mlp.dense_4h_to_h.weight\n",
      "      module.language_model.encoder.layers.9.input_norm.weight\n",
      "      module.language_model.encoder.layers.3.post_attention_norm.weight\n",
      "      module.language_model.embedding.word_embeddings.weight\n",
      "      module.language_model.encoder.layers.17.input_norm.weight\n",
      "      module.language_model.encoder.layers.11.self_attention.query_key_value.weight\n",
      "      module.language_model.encoder.layers.5.mlp.dense_h_to_4h.weight\n",
      "      module.language_model.encoder.layers.0.mlp.dense_h_to_4h.weight\n",
      "      module.language_model.encoder.layers.15.mlp.dense_h_to_4h.weight\n",
      "      module.language_model.encoder.layers.10.self_attention.dense.weight\n",
      "      module.language_model.encoder.layers.6.input_norm.weight\n",
      "      module.language_model.encoder.layers.18.self_attention.dense.weight\n",
      "      module.language_model.encoder.layers.13.post_attention_norm.weight\n",
      "      module.language_model.encoder.layers.7.mlp.dense_h_to_4h.weight\n",
      "      module.language_model.encoder.layers.2.mlp.dense_4h_to_h.weight\n",
      "     total number of elements: 3254067200\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 3254067200\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 3254072320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/apex/optimizers/fused_adam.py:112: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:82.)\n",
      "  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n",
      "/usr/local/lib/python3.10/dist-packages/apex/optimizers/fused_adam.py:112: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:82.)\n",
      "  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n",
      "/usr/local/lib/python3.10/dist-packages/apex/optimizers/fused_adam.py:112: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:82.)\n",
      "  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n",
      "/usr/local/lib/python3.10/dist-packages/apex/optimizers/fused_adam.py:112: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:82.)\n",
      "  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n",
      "/usr/local/lib/python3.10/dist-packages/apex/optimizers/fused_adam.py:112: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:82.)\n",
      "  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n",
      "/usr/local/lib/python3.10/dist-packages/apex/optimizers/fused_adam.py:112: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:82.)\n",
      "  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n",
      "/usr/local/lib/python3.10/dist-packages/apex/optimizers/fused_adam.py:112: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:82.)\n",
      "  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n",
      "/usr/local/lib/python3.10/dist-packages/apex/optimizers/fused_adam.py:112: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:82.)\n",
      "  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 3254072320\n",
      "> learning rate decay style: cosine\n",
      " loading release checkpoint from /cpfs/29ccba8f16c61395/data/user/liushan/projects/Megatron-LM-master/models/llama-megatron\n",
      " checkpoint version 3.0\n",
      "  successfully loaded checkpoint from /cpfs/29ccba8f16c61395/data/user/liushan/projects/Megatron-LM-master/models/llama-megatron at iteration 0\n",
      "(min, max) time across ranks (ms):\n",
      "    load-checkpoint ................................: (3936.19, 3936.60)\n",
      "[after model, optimizer, and learning rate scheduler are built] datetime: 2023-10-18 05:44:19 \n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      6400\n",
      "    validation: 6400\n",
      "    test:       6400\n",
      "> building train, validation, and test datasets for GPT ...\n",
      "Single data path provided for train, valid & test\n",
      " > building dataset index ...\n",
      "    reading sequence lengths...\n",
      "    reading sequence pointers...\n",
      "    reading document indices...\n",
      "    creating np buffer of mmap...\n",
      "    creating memory view of np buffer...\n",
      " > finished creating indexed dataset in 0.001544 seconds\n",
      "    number of documents: 100\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 100) total of 100 documents\n",
      "    validation:\n",
      "     document indices in [100, 100) total of 0 documents\n",
      "    test:\n",
      "     document indices in [100, 100) total of 0 documents\n",
      " > WARNING: could not find index map files, building the indices on rank 0 ...\n",
      " > last epoch number of samples (53) is larger than 80% of number of samples per epoch (60), setting separate_last_epoch to False\n",
      " > elasped time to build and save doc-idx mapping (seconds): 0.001406\n",
      "    using:\n",
      "     number of documents:       100\n",
      "     number of epochs:          106\n",
      "     sequence length:           1024\n",
      "     total number of samples:   6407\n",
      " > elasped time to build and save sample-idx mapping (seconds): 0.002361\n",
      " > building shuffle index with split [0, 6407) and [6407, 6407) ...\n",
      " > elasped time to build and save shuffle-idx mapping (seconds): 0.002156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:2788: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:2788: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:2788: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:2788: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:2788: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:2788: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:2788: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:2788: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > loading doc-idx mapping from /cpfs/29ccba8f16c61395/data/user/liushan/projects/Megatron-LM-master//tests/unicorn/data/index-cache/5bd83c009d154c0bac3bd39ccfa8247b_doc_idx.npy\n",
      " > loading sample-idx mapping from /cpfs/29ccba8f16c61395/data/user/liushan/projects/Megatron-LM-master//tests/unicorn/data/index-cache/5bd83c009d154c0bac3bd39ccfa8247b_sample_idx.npy\n",
      " > loading shuffle-idx mapping from /cpfs/29ccba8f16c61395/data/user/liushan/projects/Megatron-LM-master//tests/unicorn/data/index-cache/5bd83c009d154c0bac3bd39ccfa8247b_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 6408\n",
      "    total number of epochs: 106\n",
      "> finished creating GPT datasets ...\n",
      "[after dataloaders are built] datetime: 2023-10-18 05:44:22 \n",
      "done with setup ...\n",
      "(min, max) time across ranks (ms):\n",
      "    model-and-optimizer-setup ......................: (4497.73, 4512.70)\n",
      "    train/valid/test-data-iterators-setup ..........: (2628.27, 2878.86)\n",
      "training ...\n",
      "[before the start of training step] datetime: 2023-10-18 05:44:22 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W ProcessGroupNCCL.cpp:1626] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())\n",
      "[W ProcessGroupNCCL.cpp:1626] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())\n",
      "[W ProcessGroupNCCL.cpp:1626] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())\n",
      "[W ProcessGroupNCCL.cpp:1626] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())\n",
      "[W ProcessGroupNCCL.cpp:1626] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())\n",
      "[W ProcessGroupNCCL.cpp:1626] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())\n",
      "[W ProcessGroupNCCL.cpp:1626] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())\n",
      "[W ProcessGroupNCCL.cpp:1626] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())\n",
      "[W ProcessGroupNCCL.cpp:1626] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())\n",
      "[W ProcessGroupNCCL.cpp:1626] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())\n",
      "[W ProcessGroupNCCL.cpp:1626] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())\n",
      "[W ProcessGroupNCCL.cpp:1626] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())\n",
      "[W ProcessGroupNCCL.cpp:1626] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())\n",
      "[W ProcessGroupNCCL.cpp:1626] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())\n",
      "[W ProcessGroupNCCL.cpp:1626] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())\n",
      "[W ProcessGroupNCCL.cpp:1626] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())\n",
      "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:3256: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:3256: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:3256: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:3256: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:3256: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:3256: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:3256: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:3256: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iteration        1/      50 | consumed samples:          128 | elapsed time per iteration (ms): 8491.2 | learning rate: 3.000E-05 | global batch size:   128 | lm loss: 1.533932E+00 | loss scale: 65536.0 | grad norm: 1.595 | number of skipped iterations:   0 | number of nan iterations:   0 | TFLOPs: 152.43 |\n",
      "[Rank 1] (after 1 iterations) memory (MB) | allocated: 37408.5986328125 | max allocated: 37408.59912109375 | reserved: 39424.0 | max reserved: 39424.0\n",
      "[Rank 4] (after 1 iterations) memory (MB) | allocated: 37424.1484375 | max allocated: 37424.1796875 | reserved: 37694.0 | max reserved: 37694.0\n",
      "[Rank 5] (after 1 iterations) memory (MB) | allocated: 37424.1484375 | max allocated: 37424.1796875 | reserved: 37694.0 | max reserved: 37694.0\n",
      "[Rank 0] (after 1 iterations) memory (MB) | allocated: 37407.5986328125 | max allocated: 37407.59912109375 | reserved: 39708.0 | max reserved: 39708.0\n",
      " iteration        2/      50 | consumed samples:          256 | elapsed time per iteration (ms): 4824.9 | learning rate: 6.000E-05 | global batch size:   128 | lm loss: 1.559090E+00 | loss scale: 65536.0 | grad norm: 1.669 | number of skipped iterations:   0 | number of nan iterations:   0 | TFLOPs: 268.25 |\n",
      " iteration        3/      50 | consumed samples:          384 | elapsed time per iteration (ms): 4863.0 | learning rate: 9.000E-05 | global batch size:   128 | lm loss: 1.371131E+00 | loss scale: 65536.0 | grad norm: 1.338 | number of skipped iterations:   0 | number of nan iterations:   0 | TFLOPs: 266.15 |\n",
      " iteration        4/      50 | consumed samples:          512 | elapsed time per iteration (ms): 5053.1 | learning rate: 1.200E-04 | global batch size:   128 | lm loss: 1.198595E+00 | loss scale: 65536.0 | grad norm: 1.098 | number of skipped iterations:   0 | number of nan iterations:   0 | TFLOPs: 256.14 |\n",
      " iteration        5/      50 | consumed samples:          640 | elapsed time per iteration (ms): 5004.3 | learning rate: 1.500E-04 | global batch size:   128 | lm loss: 8.786811E-01 | loss scale: 65536.0 | grad norm: 1.247 | number of skipped iterations:   0 | number of nan iterations:   0 | TFLOPs: 258.64 |\n",
      " iteration        6/      50 | consumed samples:          768 | elapsed time per iteration (ms): 5094.1 | learning rate: 1.800E-04 | global batch size:   128 | lm loss: 6.300437E-01 | loss scale: 65536.0 | grad norm: 1.059 | number of skipped iterations:   0 | number of nan iterations:   0 | TFLOPs: 254.07 |\n",
      " iteration        7/      50 | consumed samples:          896 | elapsed time per iteration (ms): 4819.6 | learning rate: 2.100E-04 | global batch size:   128 | lm loss: 4.475701E-01 | loss scale: 65536.0 | grad norm: 1.839 | number of skipped iterations:   0 | number of nan iterations:   0 | TFLOPs: 268.55 |\n",
      " iteration        8/      50 | consumed samples:         1024 | elapsed time per iteration (ms): 4847.3 | learning rate: 2.400E-04 | global batch size:   128 | lm loss: 2.955773E-01 | loss scale: 65536.0 | grad norm: 0.972 | number of skipped iterations:   0 | number of nan iterations:   0 | TFLOPs: 267.01 |\n",
      " iteration        9/      50 | consumed samples:         1152 | elapsed time per iteration (ms): 4792.8 | learning rate: 2.700E-04 | global batch size:   128 | lm loss: 2.148202E-01 | loss scale: 65536.0 | grad norm: 1.015 | number of skipped iterations:   0 | number of nan iterations:   0 | TFLOPs: 270.05 |\n",
      " iteration       10/      50 | consumed samples:         1280 | elapsed time per iteration (ms): 4762.3 | learning rate: 3.000E-04 | global batch size:   128 | lm loss: 1.387264E-01 | loss scale: 65536.0 | grad norm: 0.670 | number of skipped iterations:   0 | number of nan iterations:   0 | TFLOPs: 271.78 |\n",
      " iteration       11/      50 | consumed samples:         1408 | elapsed time per iteration (ms): 4832.3 | learning rate: 2.996E-04 | global batch size:   128 | lm loss: 1.119415E-01 | loss scale: 65536.0 | grad norm: 0.688 | number of skipped iterations:   0 | number of nan iterations:   0 | TFLOPs: 267.84 |\n",
      " iteration       12/      50 | consumed samples:         1536 | elapsed time per iteration (ms): 4893.9 | learning rate: 2.983E-04 | global batch size:   128 | lm loss: 9.793559E-02 | loss scale: 65536.0 | grad norm: 0.807 | number of skipped iterations:   0 | number of nan iterations:   0 | TFLOPs: 264.47 |\n",
      " iteration       13/      50 | consumed samples:         1664 | elapsed time per iteration (ms): 4819.8 | learning rate: 2.963E-04 | global batch size:   128 | lm loss: 6.744651E-02 | loss scale: 65536.0 | grad norm: 0.397 | number of skipped iterations:   0 | number of nan iterations:   0 | TFLOPs: 268.54 |\n",
      " iteration       14/      50 | consumed samples:         1792 | elapsed time per iteration (ms): 4760.3 | learning rate: 2.934E-04 | global batch size:   128 | lm loss: 6.012047E-02 | loss scale: 65536.0 | grad norm: 0.363 | number of skipped iterations:   0 | number of nan iterations:   0 | TFLOPs: 271.89 |\n",
      " iteration       15/      50 | consumed samples:         1920 | elapsed time per iteration (ms): 4764.8 | learning rate: 2.897E-04 | global batch size:   128 | lm loss: 5.217484E-02 | loss scale: 65536.0 | grad norm: 0.238 | number of skipped iterations:   0 | number of nan iterations:   0 | TFLOPs: 271.63 |\n",
      " iteration       16/      50 | consumed samples:         2048 | elapsed time per iteration (ms): 4740.6 | learning rate: 2.853E-04 | global batch size:   128 | lm loss: 4.978499E-02 | loss scale: 65536.0 | grad norm: 0.339 | number of skipped iterations:   0 | number of nan iterations:   0 | TFLOPs: 273.02 |\n",
      " iteration       17/      50 | consumed samples:         2176 | elapsed time per iteration (ms): 4761.4 | learning rate: 2.801E-04 | global batch size:   128 | lm loss: 4.517830E-02 | loss scale: 65536.0 | grad norm: 0.209 | number of skipped iterations:   0 | number of nan iterations:   0 | TFLOPs: 271.83 |\n",
      " iteration       18/      50 | consumed samples:         2304 | elapsed time per iteration (ms): 4785.6 | learning rate: 2.742E-04 | global batch size:   128 | lm loss: 4.222200E-02 | loss scale: 65536.0 | grad norm: 0.232 | number of skipped iterations:   0 | number of nan iterations:   0 | TFLOPs: 270.45 |\n",
      " iteration       19/      50 | consumed samples:         2432 | elapsed time per iteration (ms): 4907.1 | learning rate: 2.677E-04 | global batch size:   128 | lm loss: 4.023964E-02 | loss scale: 65536.0 | grad norm: 0.242 | number of skipped iterations:   0 | number of nan iterations:   0 | TFLOPs: 263.76 |\n",
      " iteration       20/      50 | consumed samples:         2560 | elapsed time per iteration (ms): 4794.7 | learning rate: 2.605E-04 | global batch size:   128 | lm loss: 3.899191E-02 | loss scale: 65536.0 | grad norm: 0.215 | number of skipped iterations:   0 | number of nan iterations:   0 | TFLOPs: 269.94 |\n",
      " iteration       21/      50 | consumed samples:         2688 | elapsed time per iteration (ms): 4775.4 | learning rate: 2.527E-04 | global batch size:   128 | lm loss: 3.550610E-02 | loss scale: 65536.0 | grad norm: 0.165 | number of skipped iterations:   0 | number of nan iterations:   0 | TFLOPs: 271.03 |\n",
      " iteration       22/      50 | consumed samples:         2816 | elapsed time per iteration (ms): 5014.5 | learning rate: 2.444E-04 | global batch size:   128 | lm loss: 3.744963E-02 | loss scale: 65536.0 | grad norm: 1.216 | number of skipped iterations:   0 | number of nan iterations:   0 | TFLOPs: 258.11 |\n",
      " iteration       23/      50 | consumed samples:         2944 | elapsed time per iteration (ms): 4662.2 | learning rate: 2.444E-04 | global batch size:   128 | loss scale: 65536.0 | number of skipped iterations:   1 | number of nan iterations:   0 | TFLOPs: 277.61 |\n",
      " iteration       24/      50 | consumed samples:         3072 | elapsed time per iteration (ms): 4689.9 | learning rate: 2.444E-04 | global batch size:   128 | loss scale: 32768.0 | number of skipped iterations:   1 | number of nan iterations:   0 | TFLOPs: 275.97 |\n",
      " iteration       25/      50 | consumed samples:         3200 | elapsed time per iteration (ms): 4748.4 | learning rate: 2.444E-04 | global batch size:   128 | loss scale: 16384.0 | number of skipped iterations:   1 | number of nan iterations:   0 | TFLOPs: 272.57 |\n",
      " iteration       26/      50 | consumed samples:         3328 | elapsed time per iteration (ms): 4654.3 | learning rate: 2.444E-04 | global batch size:   128 | loss scale: 8192.0 | number of skipped iterations:   1 | number of nan iterations:   0 | TFLOPs: 278.09 |\n",
      " iteration       27/      50 | consumed samples:         3456 | elapsed time per iteration (ms): 4791.7 | learning rate: 2.355E-04 | global batch size:   128 | lm loss: 5.424372E-02 | loss scale: 8192.0 | grad norm: 10.838 | number of skipped iterations:   0 | number of nan iterations:   0 | TFLOPs: 270.11 |\n",
      " iteration       28/      50 | consumed samples:         3584 | elapsed time per iteration (ms): 4786.8 | learning rate: 2.263E-04 | global batch size:   128 | lm loss: 3.649360E-02 | loss scale: 8192.0 | grad norm: 0.205 | number of skipped iterations:   0 | number of nan iterations:   0 | TFLOPs: 270.39 |\n",
      " iteration       29/      50 | consumed samples:         3712 | elapsed time per iteration (ms): 4688.7 | learning rate: 2.167E-04 | global batch size:   128 | lm loss: 3.484631E-02 | loss scale: 8192.0 | grad norm: 0.404 | number of skipped iterations:   0 | number of nan iterations:   0 | TFLOPs: 276.04 |\n",
      " iteration       30/      50 | consumed samples:         3840 | elapsed time per iteration (ms): 4714.9 | learning rate: 2.067E-04 | global batch size:   128 | lm loss: 3.543791E-02 | loss scale: 8192.0 | grad norm: 0.670 | number of skipped iterations:   0 | number of nan iterations:   0 | TFLOPs: 274.51 |\n",
      " iteration       31/      50 | consumed samples:         3968 | elapsed time per iteration (ms): 4711.0 | learning rate: 1.965E-04 | global batch size:   128 | lm loss: 3.416618E-02 | loss scale: 8192.0 | grad norm: 0.775 | number of skipped iterations:   0 | number of nan iterations:   0 | TFLOPs: 274.73 |\n",
      " iteration       32/      50 | consumed samples:         4096 | elapsed time per iteration (ms): 4794.3 | learning rate: 1.861E-04 | global batch size:   128 | lm loss: 5.885243E-02 | loss scale: 8192.0 | grad norm: 2.997 | number of skipped iterations:   0 | number of nan iterations:   0 | TFLOPs: 269.96 |\n",
      " iteration       33/      50 | consumed samples:         4224 | elapsed time per iteration (ms): 4733.0 | learning rate: 1.756E-04 | global batch size:   128 | lm loss: 3.118202E-02 | loss scale: 8192.0 | grad norm: 0.173 | number of skipped iterations:   0 | number of nan iterations:   0 | TFLOPs: 273.46 |\n",
      " iteration       34/      50 | consumed samples:         4352 | elapsed time per iteration (ms): 4800.9 | learning rate: 1.650E-04 | global batch size:   128 | lm loss: 3.280135E-02 | loss scale: 8192.0 | grad norm: 0.160 | number of skipped iterations:   0 | number of nan iterations:   0 | TFLOPs: 269.59 |\n",
      " iteration       35/      50 | consumed samples:         4480 | elapsed time per iteration (ms): 4714.1 | learning rate: 1.544E-04 | global batch size:   128 | lm loss: 3.100550E-02 | loss scale: 8192.0 | grad norm: 0.203 | number of skipped iterations:   0 | number of nan iterations:   0 | TFLOPs: 274.56 |\n",
      " iteration       36/      50 | consumed samples:         4608 | elapsed time per iteration (ms): 4730.3 | learning rate: 1.439E-04 | global batch size:   128 | lm loss: 3.253379E-02 | loss scale: 8192.0 | grad norm: 0.616 | number of skipped iterations:   0 | number of nan iterations:   0 | TFLOPs: 273.62 |\n",
      " iteration       37/      50 | consumed samples:         4736 | elapsed time per iteration (ms): 4949.0 | learning rate: 1.335E-04 | global batch size:   128 | lm loss: 3.116639E-02 | loss scale: 8192.0 | grad norm: 0.130 | number of skipped iterations:   0 | number of nan iterations:   0 | TFLOPs: 261.53 |\n",
      " iteration       38/      50 | consumed samples:         4864 | elapsed time per iteration (ms): 4719.2 | learning rate: 1.233E-04 | global batch size:   128 | lm loss: 3.088586E-02 | loss scale: 8192.0 | grad norm: 0.136 | number of skipped iterations:   0 | number of nan iterations:   0 | TFLOPs: 274.26 |\n",
      " iteration       39/      50 | consumed samples:         4992 | elapsed time per iteration (ms): 4722.6 | learning rate: 1.133E-04 | global batch size:   128 | lm loss: 3.236873E-02 | loss scale: 8192.0 | grad norm: 0.142 | number of skipped iterations:   0 | number of nan iterations:   0 | TFLOPs: 274.06 |\n",
      " iteration       40/      50 | consumed samples:         5120 | elapsed time per iteration (ms): 4728.1 | learning rate: 1.037E-04 | global batch size:   128 | lm loss: 2.883647E-02 | loss scale: 8192.0 | grad norm: 0.119 | number of skipped iterations:   0 | number of nan iterations:   0 | TFLOPs: 273.74 |\n",
      " iteration       41/      50 | consumed samples:         5248 | elapsed time per iteration (ms): 4718.4 | learning rate: 9.446E-05 | global batch size:   128 | lm loss: 2.886766E-02 | loss scale: 8192.0 | grad norm: 0.184 | number of skipped iterations:   0 | number of nan iterations:   0 | TFLOPs: 274.31 |\n",
      " iteration       42/      50 | consumed samples:         5376 | elapsed time per iteration (ms): 4790.1 | learning rate: 8.565E-05 | global batch size:   128 | lm loss: 2.953870E-02 | loss scale: 8192.0 | grad norm: 0.122 | number of skipped iterations:   0 | number of nan iterations:   0 | TFLOPs: 270.20 |\n",
      " iteration       43/      50 | consumed samples:         5504 | elapsed time per iteration (ms): 4725.0 | learning rate: 7.732E-05 | global batch size:   128 | lm loss: 2.779700E-02 | loss scale: 8192.0 | grad norm: 0.196 | number of skipped iterations:   0 | number of nan iterations:   0 | TFLOPs: 273.92 |\n",
      " iteration       44/      50 | consumed samples:         5632 | elapsed time per iteration (ms): 4715.0 | learning rate: 6.954E-05 | global batch size:   128 | lm loss: 2.666926E-02 | loss scale: 8192.0 | grad norm: 0.137 | number of skipped iterations:   0 | number of nan iterations:   0 | TFLOPs: 274.51 |\n",
      " iteration       45/      50 | consumed samples:         5760 | elapsed time per iteration (ms): 4804.4 | learning rate: 6.235E-05 | global batch size:   128 | lm loss: 2.773422E-02 | loss scale: 8192.0 | grad norm: 0.104 | number of skipped iterations:   0 | number of nan iterations:   0 | TFLOPs: 269.40 |\n",
      " iteration       46/      50 | consumed samples:         5888 | elapsed time per iteration (ms): 4802.7 | learning rate: 5.578E-05 | global batch size:   128 | lm loss: 2.830485E-02 | loss scale: 8192.0 | grad norm: 0.106 | number of skipped iterations:   0 | number of nan iterations:   0 | TFLOPs: 269.49 |\n",
      " iteration       47/      50 | consumed samples:         6016 | elapsed time per iteration (ms): 4730.7 | learning rate: 4.989E-05 | global batch size:   128 | lm loss: 2.661580E-02 | loss scale: 8192.0 | grad norm: 0.132 | number of skipped iterations:   0 | number of nan iterations:   0 | TFLOPs: 273.59 |\n",
      " iteration       48/      50 | consumed samples:         6144 | elapsed time per iteration (ms): 4727.1 | learning rate: 4.471E-05 | global batch size:   128 | lm loss: 2.723376E-02 | loss scale: 8192.0 | grad norm: 0.118 | number of skipped iterations:   0 | number of nan iterations:   0 | TFLOPs: 273.80 |\n",
      " iteration       49/      50 | consumed samples:         6272 | elapsed time per iteration (ms): 4745.0 | learning rate: 4.028E-05 | global batch size:   128 | lm loss: 2.684665E-02 | loss scale: 8192.0 | grad norm: 0.104 | number of skipped iterations:   0 | number of nan iterations:   0 | TFLOPs: 272.77 |\n",
      " iteration       50/      50 | consumed samples:         6400 | elapsed time per iteration (ms): 4789.4 | learning rate: 3.661E-05 | global batch size:   128 | lm loss: 2.712388E-02 | loss scale: 8192.0 | grad norm: 0.109 | number of skipped iterations:   0 | number of nan iterations:   0 | TFLOPs: 270.24 |\n",
      "[after training is done] datetime: 2023-10-18 05:48:25 \n",
      "saving checkpoint at iteration      50 to /cpfs/29ccba8f16c61395/data/user/liushan/projects/Megatron-LM-master//test/checkpoint/\n",
      "  successfully saved checkpoint at iteration      50 to /cpfs/29ccba8f16c61395/data/user/liushan/projects/Megatron-LM-master//test/checkpoint/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Launch distributed training and loading megatron checkpoint built from HF model.\n",
    "# Check the loss value at the beginning is nearly ~1.5, it should be reasonable.\n",
    "launch_task = \\\n",
    "\"\"\"\n",
    "MEGATRON_PATH={0}\n",
    "CODE_ROOT={0}\n",
    "export PYTHONPATH={0}:$PYTHONPATH\n",
    "export CUDA_DEVICE_MAX_CONNECTIONS=1\n",
    "\n",
    "NNODES=1\n",
    "NODE_RANK=0\n",
    "GPUS_PER_NODE=8\n",
    "MASTER_ADDR=localhost\n",
    "MASTER_PORT=12345\n",
    "\n",
    "DISTRIBUTED_ARGS=\"--nproc_per_node $GPUS_PER_NODE \\\n",
    "                  --nnodes $NNODES \\\n",
    "                  --node_rank $NODE_RANK \\\n",
    "                  --master_addr $MASTER_ADDR \\\n",
    "                  --master_port $MASTER_PORT\"\n",
    "\n",
    "custom_options=\"--disable-bias-linear \\\n",
    "                --swiglu \\\n",
    "                --untie-embeddings-and-output-weights \\\n",
    "                --swiglu-make-ffn-hidden-size-divisible-by 256 \\\n",
    "                --position-embedding-type rope \\\n",
    "                --normalization RMSNorm \\\n",
    "                --norm-epsilon 1e-5 \\\n",
    "                --init-method-std 0.02 \\\n",
    "                --disable-scaled-init-method \\\n",
    "                \"\n",
    "\n",
    "# Llama tokenizers and use NullTokenizer\n",
    "VOCAB_SIZE=$(( 32000 - 1 ))\n",
    "TP=2\n",
    "PP=2\n",
    "\n",
    "DATA_ROOT=\"{0}/tests/unicorn/data/\"\n",
    "TOKENIZER_NAME_OR_PATH=\"/path/to/tokenizer\"\n",
    "DATASET_PATH=\" \\\n",
    "    $DATA_ROOT/sample_llama_text_document \\\n",
    "    \"\n",
    "\n",
    "OUTPUT_BASEPATH=\"{0}/test\"\n",
    "mkdir -p \"$OUTPUT_BASEPATH/tensorboard/\"\n",
    "mkdir -p \"$OUTPUT_BASEPATH/checkpoint/\"\n",
    "mkdir -p \"$OUTPUT_BASEPATH/log/\"\n",
    "TENSORBOARD_DIR=\"$OUTPUT_BASEPATH/tensorboard/\"\n",
    "mkdir -p $TENSORBOARD_DIR\n",
    "\n",
    "SAVED_PRETRAIN_CHECKPOINT_PATH=\"$OUTPUT_BASEPATH/checkpoint/$NAME\"\n",
    "LOAD_PATH={1}\n",
    "\n",
    "megatron_options=\"  \\\n",
    "        --save $SAVED_PRETRAIN_CHECKPOINT_PATH \\\n",
    "        --load $LOAD_PATH \\\n",
    "        --split 99.5,0.5,0 \\\n",
    "        --data-path $DATASET_PATH \\\n",
    "        --lr 3e-4 \\\n",
    "        --min-lr 3e-5 \\\n",
    "        --lr-decay-style cosine \\\n",
    "        --adam-beta1 0.9 \\\n",
    "        --adam-beta2 0.95 \\\n",
    "        --adam-eps 1e-5 \\\n",
    "        --weight-decay 0.1 \\\n",
    "        --clip-grad 1.0 \\\n",
    "        --lr-decay-iters 50 \\\n",
    "        --lr-warmup-iters 10 \\\n",
    "        --train-iters 50 \\\n",
    "        --micro-batch-size 1 \\\n",
    "        --global-batch-size 128 \\\n",
    "        --num-layers 40 \\\n",
    "        --hidden-size 5120 \\\n",
    "        --num-attention-heads 40 \\\n",
    "        --seq-length 1024 \\\n",
    "        --max-position-embeddings 1024 \\\n",
    "        --attention-dropout 0.0 \\\n",
    "        --hidden-dropout 0.0 \\\n",
    "        --log-interval 1 \\\n",
    "        --eval-interval 1000 \\\n",
    "        --eval-iters 50 \\\n",
    "        --save-interval 1000 \\\n",
    "        --tensor-model-parallel-size $TP \\\n",
    "        --pipeline-model-parallel-size $PP \\\n",
    "        --num-workers 8 \\\n",
    "        --seed 888 \\\n",
    "        --tokenizer-type NullTokenizer \\\n",
    "        --vocab-size $VOCAB_SIZE \\\n",
    "        \"\n",
    "\n",
    "cd $MEGATRON_PATH\n",
    "\n",
    "run_cmd=\"python -m torch.distributed.launch $DISTRIBUTED_ARGS $CODE_ROOT/pretrain_gpt.py \\\n",
    "         $megatron_options \\\n",
    "         $custom_options \\\n",
    "         --use-distributed-optimizer \\\n",
    "         --fp16 \\\n",
    "         --initial-loss-scale 65536 \\\n",
    "         --use-flash-attn \\\n",
    "         \"\n",
    "\n",
    "echo $run_cmd\n",
    "eval $run_cmd\n",
    "\n",
    "\"\"\".format(MEGATRON_ROOT, os.path.join(MEGATRON_ROOT, \"models\", \"llama-megatron\"))\n",
    "\n",
    "# print(launch_task)\n",
    "os.system(launch_task)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
